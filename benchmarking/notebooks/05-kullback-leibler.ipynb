{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change working directory to parent so we can import as we would from main.py\n",
    "%cd ..\n",
    "\n",
    "# import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log2\n",
    "# from statistics import mean\n",
    "# from scipy.special import kl_div\n",
    "# from scipy.stats import norm\n",
    "# from scipy import stats, special\n",
    "from scipy.stats import fit, gamma, norm, exponnorm, laplace, gaussian_kde\n",
    "# from statistics import NormalDist\n",
    "# from statistics import NormalDist\n",
    "\n",
    "import configuration as config\n",
    "# import functions.data_manipulation as data_funcs\n",
    "# import functions.plotting as plot_funcs\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity ratio score: Kullbackâ€“Leibler divergence\n",
    "\n",
    "Plan here is to take our sampling distributions of perplexity ratio scores for human and synthetic text and use them to generate a function that takes a perplexity ratio score and converts it into a Kullback-Leibler score. See the figure below from the [Wikipedia article on KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url = 'https://raw.githubusercontent.com/gperdrizet/llm_detector/benchmarking/benchmarking/notebooks/images/KL-Gauss-Example.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will load up the perplexity score distributions and fit them. Then, calculate the KL divergence between the fitted distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_df = pd.read_json(f'{config.HANS_DATA_PATH}/falcon-7b_scores.json')\n",
    "\n",
    "# Replace and remove string 'OOM' and 'NAN' values and negative or positive\n",
    "# infinity, if found.\n",
    "data_df.replace('NAN', np.nan, inplace = True)\n",
    "data_df.replace('nan', np.nan, inplace = True)\n",
    "data_df.replace('OOM', np.nan, inplace = True)\n",
    "data_df.replace('oom', np.nan, inplace = True)\n",
    "data_df.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "data_df.dropna(inplace = True)\n",
    "\n",
    "# Fix some d-types\n",
    "data_df = data_df.astype({\n",
    "    'Fragment length (tokens)': int, \n",
    "    'Perplexity': float,\n",
    "    'Cross-perplexity': float,\n",
    "    'Perplexity ratio score': float\n",
    "})\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_non_numeric = pd.to_numeric(data_df['Perplexity ratio score'], errors='coerce').isnull()\n",
    "data_df[is_non_numeric]['Perplexity ratio score'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract perplexity score ratios as lists, clipping outliers higher than 1.6 and \n",
    "# removing anything not finite\n",
    "scores = data_df['Perplexity ratio score'][data_df['Perplexity ratio score'] < 1.6]\n",
    "\n",
    "# Split to human and synthetic for easy handling\n",
    "human_scores = data_df['Perplexity ratio score'][data_df['Source'] == 'human']\n",
    "synthetic_scores = data_df['Perplexity ratio score'][data_df['Source'] == 'synthetic']\n",
    "\n",
    "# Set up bins for the combined dataset\n",
    "counts, bins = np.histogram(scores, bins = 100, density = True)\n",
    "\n",
    "# Get bin centers\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "# Calculate histograms for the human and synthetic data\n",
    "human_density, human_bins = np.histogram(human_scores, bins = bins, density = True)\n",
    "synthetic_density, synthetic_bins = np.histogram(synthetic_scores, bins = bins, density = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot as scatter\n",
    "plt.scatter(bin_centers, human_density, label = 'human')\n",
    "plt.scatter(bin_centers, synthetic_density, label = 'synthetic')\n",
    "\n",
    "plt.title('Perplexity ratio score distribution')\n",
    "plt.xlabel('Perplexity ratio score')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try fitting a couple of different continuous distributions to see what looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian distribution\n",
    "bounds = [[0.6,1.2],[0,10]]\n",
    "\n",
    "human_norm = fit(norm, human_scores, bounds = bounds)\n",
    "human_norm_fit = norm(human_norm.params.loc, human_norm.params.scale).pdf(bin_centers)\n",
    "\n",
    "synthetic_norm = fit(norm, synthetic_scores, bounds = bounds)\n",
    "synthetic_norm_fit = norm(synthetic_norm.params.loc, synthetic_norm.params.scale).pdf(bin_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential Gaussian\n",
    "bounds = [[0,10],[0.6,1.2],[0,10]]\n",
    "\n",
    "human_exponnorm = fit(exponnorm, human_scores, bounds = bounds)\n",
    "human_exponnorm_fit = exponnorm(human_exponnorm.params.K, human_exponnorm.params.loc, human_exponnorm.params.scale).pdf(bin_centers)\n",
    "\n",
    "synthetic_exponnorm = fit(exponnorm, synthetic_scores, bounds = bounds)\n",
    "synthetic_exponnorm_fit = exponnorm(synthetic_exponnorm.params.K, synthetic_exponnorm.params.loc, synthetic_exponnorm.params.scale).pdf(bin_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gamma distribution\n",
    "bounds = [[1,100],[0,1],[0,1]]\n",
    "\n",
    "human_gamma = fit(gamma, human_scores, bounds = bounds)\n",
    "human_gamma_fit = gamma(human_gamma.params.a, human_gamma.params.loc, human_gamma.params.scale).pdf(bin_centers)\n",
    "\n",
    "synthetic_gamma = fit(gamma, synthetic_scores, bounds = bounds)\n",
    "synthetic_gamma_fit = gamma(synthetic_gamma.params.a, synthetic_gamma.params.loc, synthetic_gamma.params.scale).pdf(bin_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laplace\n",
    "bounds = [[0,10],[0,10]]\n",
    "\n",
    "human_laplace = fit(laplace, human_scores, bounds = bounds)\n",
    "human_laplace_fit = laplace(human_laplace.params.loc, human_laplace.params.scale).pdf(bin_centers)\n",
    "\n",
    "synthetic_laplace = fit(laplace, synthetic_scores, bounds = bounds)\n",
    "synthetic_laplace_fit = laplace(synthetic_laplace.params.loc, synthetic_laplace.params.scale).pdf(bin_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs=plt.subplots(\n",
    "    1,\n",
    "    2,\n",
    "    figsize=(6, 3),\n",
    "    sharex='col',\n",
    "    sharey='row',\n",
    "    tight_layout=True,\n",
    "    gridspec_kw = {'wspace':0, 'hspace':0}\n",
    ")\n",
    "\n",
    "axs[0].set_title('Synthetic text')\n",
    "axs[0].scatter(bin_centers, synthetic_density, color = 'black', label = 'Data')\n",
    "axs[0].plot(bin_centers, synthetic_norm_fit, label = 'Gaussian')\n",
    "axs[0].plot(bin_centers, synthetic_gamma_fit, label = 'Gamma')\n",
    "axs[0].plot(bin_centers, synthetic_exponnorm_fit, label = 'ExGaussian')\n",
    "axs[0].plot(bin_centers, synthetic_laplace_fit, label = 'Laplace')\n",
    "\n",
    "axs[1].set_title('Human text')\n",
    "axs[1].scatter(bin_centers, human_density, color = 'black', label = 'Data')\n",
    "axs[1].plot(bin_centers, human_norm_fit, label = 'Gaussian')\n",
    "axs[1].plot(bin_centers, human_gamma_fit, label = 'Gamma')\n",
    "axs[1].plot(bin_centers, human_exponnorm_fit, label = 'ExGaussian')\n",
    "axs[1].plot(bin_centers, human_laplace_fit, label = 'Laplace')\n",
    "\n",
    "axs[1].legend(\n",
    "    title = 'Distribution',\n",
    "    loc = 'upper left',\n",
    "    fontsize = 'small'\n",
    ")\n",
    "\n",
    "# Set figure title\n",
    "fig.text(0.5, 1, 'Perplexity ratio score distributions', ha='center', fontsize='x-large')\n",
    "\n",
    "# Set single label for shared x\n",
    "fig.text(0.5, 0.01, 'Perplexity ratio score', ha='center')\n",
    "\n",
    "# Set single label for shared y\n",
    "fig.text(0.01, 0.5, 'Density', va='center', ha='center', rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laplassian is too sharp, Gaussian too blunt, especialy on the synthetic text distribution, think the exponential Gaussian has it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs=plt.subplots(\n",
    "    1,\n",
    "    2,\n",
    "    figsize=(8, 4),\n",
    "    sharex='col',\n",
    "    sharey='row',\n",
    "    tight_layout=True,\n",
    "    gridspec_kw = {'wspace':0, 'hspace':0}\n",
    ")\n",
    "\n",
    "axs[0].set_title('Synthetic text')\n",
    "axs[0].scatter(bin_centers, (synthetic_density - synthetic_norm_fit), label = 'Gaussian')\n",
    "axs[0].scatter(bin_centers, (synthetic_density - synthetic_gamma_fit), label = 'Gamma')\n",
    "axs[0].scatter(bin_centers, (synthetic_density - synthetic_exponnorm_fit), label = 'ExGaussian')\n",
    "axs[0].scatter(bin_centers, (synthetic_density - synthetic_laplace_fit), label = 'Laplace')\n",
    "\n",
    "axs[1].set_title('Human text')\n",
    "axs[1].scatter(bin_centers, (human_density - human_norm_fit), label = 'Gaussian')\n",
    "axs[1].scatter(bin_centers, (human_density - human_gamma_fit), label = 'Gamma')\n",
    "axs[1].scatter(bin_centers, (human_density - human_exponnorm_fit), label = 'ExGaussian')\n",
    "axs[1].scatter(bin_centers, (human_density - human_laplace_fit), label = 'Laplace')\n",
    "\n",
    "axs[1].legend(\n",
    "    title = 'Distribution',\n",
    "    loc = 'upper left',\n",
    "    fontsize = 'small'\n",
    ")\n",
    "\n",
    "# Set figure title\n",
    "fig.text(0.5, 1, 'Perplexity distribution fit residuals', ha='center', fontsize='x-large')\n",
    "\n",
    "# Set single label for shared x\n",
    "fig.text(0.5, 0.01, 'Perplexity ratio score', ha='center')\n",
    "\n",
    "# Set single label for shared y\n",
    "fig.text(0.01, 0.5, 'Density', va='center', ha='center', rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs=plt.subplots(\n",
    "    1,\n",
    "    2,\n",
    "    figsize=(8, 4),\n",
    "    sharex='col',\n",
    "    sharey='row',\n",
    "    tight_layout=True,\n",
    "    gridspec_kw = {'wspace':0, 'hspace':0}\n",
    ")\n",
    "\n",
    "axs[0].set_title('Synthetic text')\n",
    "axs[0].scatter(synthetic_density, synthetic_norm_fit, label = 'Gaussian')\n",
    "axs[0].scatter(synthetic_density, synthetic_gamma_fit, label = 'Gamma')\n",
    "axs[0].scatter(synthetic_density, synthetic_exponnorm_fit, label = 'ExGaussian')\n",
    "axs[0].scatter(synthetic_density, synthetic_laplace_fit, label = 'Laplace')\n",
    "\n",
    "axs[1].set_title('Human text')\n",
    "axs[1].scatter(human_density, human_norm_fit, label = 'Gaussian')\n",
    "axs[1].scatter(human_density, human_gamma_fit, label = 'Gamma')\n",
    "axs[1].scatter(human_density, human_exponnorm_fit, label = 'ExGaussian')\n",
    "axs[1].scatter(human_density, human_laplace_fit, label = 'Laplace')\n",
    "\n",
    "axs[1].legend(\n",
    "    title = 'Distribution',\n",
    "    loc = 'upper left',\n",
    "    fontsize = 'small'\n",
    ")\n",
    "\n",
    "# Set figure title\n",
    "fig.text(0.5, 1, 'Perplexity ratio score actual vs fitted values', ha='center', fontsize='x-large')\n",
    "\n",
    "# Set single label for shared x\n",
    "fig.text(0.5, 0.01, 'True score', ha='center')\n",
    "\n",
    "# Set single label for shared y\n",
    "fig.text(0.01, 0.5, 'Fitted score', va='center', ha='center', rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p, q):\n",
    "    '''Takes two lists, calcualtes KD divergence'''\n",
    "    return [p[i] * log2(p[i]/q[i]) for i in range(len(p))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs=plt.subplots(\n",
    "    2,\n",
    "    2,\n",
    "    figsize=(5, 5),\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    "    tight_layout=True,\n",
    "    gridspec_kw = {'wspace':0, 'hspace':0}\n",
    ")\n",
    "\n",
    "axs[0,0].set_title('Gaussian', y=1.0, pad=-18)\n",
    "axs[0,0].plot(bin_centers, human_norm_fit, label = 'human')\n",
    "axs[0,0].plot(bin_centers, synthetic_norm_fit, label = 'synthetic')\n",
    "axs[0,0].plot(bin_centers, kl_divergence(synthetic_norm_fit, human_norm_fit), label = 'KL divergence')\n",
    "\n",
    "axs[0,1].set_title('Exponential Gaussian', y=1.0, pad=-18)\n",
    "axs[0,1].plot(bin_centers, human_exponnorm_fit, label = 'human')\n",
    "axs[0,1].plot(bin_centers, synthetic_exponnorm_fit, label = 'synthetic')\n",
    "axs[0,1].plot(bin_centers, kl_divergence(synthetic_exponnorm_fit, human_exponnorm_fit), label = 'KL divergence')\n",
    "\n",
    "axs[1,0].set_title('Gamma', y=1.0, pad=-18)\n",
    "axs[1,0].plot(bin_centers, human_gamma_fit, label = 'human')\n",
    "axs[1,0].plot(bin_centers, synthetic_gamma_fit, label = 'synthetic')\n",
    "axs[1,0].plot(bin_centers, kl_divergence(synthetic_gamma_fit, human_gamma_fit), label = 'KL divergence')\n",
    "\n",
    "axs[1,1].set_title('Laplace', y=1.0, pad=-18)\n",
    "axs[1,1].plot(bin_centers, human_laplace_fit, label = 'human')\n",
    "axs[1,1].plot(bin_centers, synthetic_laplace_fit, label = 'synthetic')\n",
    "axs[1,1].plot(bin_centers, kl_divergence(synthetic_laplace_fit, human_laplace_fit), label = 'KL divergence')\n",
    "\n",
    "\n",
    "axs[0,1].legend(\n",
    "    title = 'Distribution',\n",
    "    loc = 'center right',\n",
    "    fontsize = 'small'\n",
    ")\n",
    "\n",
    "# Set figure title\n",
    "fig.text(0.5, 1, 'Kullback-Leibler divergence', ha='center', fontsize='x-large')\n",
    "\n",
    "# Set single label for shared x\n",
    "fig.text(0.5, 0.01, 'Perplexity ratio score', ha='center')\n",
    "\n",
    "# Set single label for shared y\n",
    "fig.text(0.01, 0.5, 'Density', va='center', ha='center', rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the exponential gaussian looks best, I think. Proof will be in the pudding, but before we move on, I want to try this one more way. Let's see if we can use kernel density extimates of the perplexity ratio score distributions to get the KL divirgence. That way we don't even have to worry about fitting the data or justifying the choice of distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get kernel density estimates for the score distributions\n",
    "human_kde = gaussian_kde(human_scores)\n",
    "synthetic_kde = gaussian_kde(synthetic_scores)\n",
    "\n",
    "# Calculate the KL divergence\n",
    "kl = kl_divergence(synthetic_kde.pdf(bin_centers), human_kde.pdf(bin_centers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Kernel density estimates')\n",
    "plt.plot(bin_centers, human_kde.pdf(bin_centers), label = 'human')\n",
    "plt.plot(bin_centers, synthetic_kde.pdf(bin_centers), label = 'synthetic')\n",
    "plt.plot(bin_centers, kl, label = 'KL divergence')\n",
    "plt.xlabel('Perplexity ratio score')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(\n",
    "    title = 'Distribution',\n",
    "    loc = 'upper right'\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still think the Gaussian or the exponential gaussian looks better, but It's nice to have a non-parametric way to do this. I think we should declare victory and move on. Next thing to work on is building the training data for XGBoost."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
