# VSCode & Google Cloud remote development set up

Notes for set up of Visual Studio Code for remote development on Google Cloud compute instance VM. Host OS is Arch Linux.

## 1. VSCode

### 1.1. Installation

VSCode insiders installed via AUR repository [visual-studio-code-insiders-bin](https://aur.archlinux.org/packages/visual-studio-code-insiders-bin).

```bash
git clone https://aur.archlinux.org/packages/visual-studio-code-insiders-bin
cd visual-studio-code-insiders-bin
makepkg -si
```

Current version is 1.97 on 2024-12-28. Update via git with:

```bash
git restore .
git pull
makepkg -si
```

### 1.2. Customizations

#### 1.2.1. Hide search bar

Top search bar is 'command center' hide via setting 'Window: command center'. Also need to set the following:

- 'Custom Title Bar Visibility' to 'never'
- 'Menu bar visibility' to 'toggle'
- 'Window: title bar style' to 'native'

## 2. Google Cloud Compute Engine

### 2.1. Install Google Cloud CLI

Installation from AUR repository [google-cloud-cli](https://aur.archlinux.org/packages/google-cloud-cli)

```bash
git clone https://aur.archlinux.org/google-cloud-cli.git
cd google-cloud-cli
makepkg -si
```

Current version is 1.97 on 2024-12-28. Update via git with:

```bash
git restore .
git pull
makepkg -si
```

Add the following to `~/.bashrc` to enable command line completion of Google Cloud CLI commands:

```bash
source /home/siderealyear/AUR/google-cloud-cli/pkg/google-cloud-cli/opt/google-cloud-cli/completion.bash.inc
source /home/siderealyear/AUR/google-cloud-cli/pkg/google-cloud-cli/opt/google-cloud-cli/path.bash.inc
```

Reboot and `gcloud` should be accessible.

```bash
$ gcloud --version

Google Cloud SDK 504.0.1
alpha 2024.12.19
beta 2024.12.19
bq 2.1.11
bundled-python3-unix 3.11.9
core 2024.12.19
gcloud-crc32c 1.0.0
gsutil 5.33
```

Then configure the project via `gcloud init`. Set default region to `us-central-1a`.

### 2.3. Compute Engine remote access

#### 2.3.1. SSH

Connect via command line with `gcloud`:

```bash
gcloud compute ssh --zone "us-central1-a" "nostalgia-for-infinity" --project "ask-agatha"
```

The first time this command is run, `gcloud` will set up a key pair and then log in. Exit the VM instance and run the following on the host machine:

```bash
gcloud compute config-ssh
```

This will add an alias to `.ssh/config`:

```text
# Google Compute Engine Section
#
# The following has been auto-generated by "gcloud compute config-ssh"
# to make accessing your Google Compute Engine virtual machines easier.
#
# To remove this blob, run:
#
#   gcloud compute config-ssh --remove
#
# You can also manually remove this blob by deleting everything from
# here until the comment that contains the string "End of Google Compute
# Engine Section".
#
# You should not hand-edit this section, unless you are deleting it.
#
Host nostalgia-for-infinity.us-central1-a.ask-agatha
    HostName 34.31.191.133
    IdentityFile /home/siderealyear/.ssh/google_compute_engine
    UserKnownHostsFile=/home/siderealyear/.ssh/google_compute_known_hosts
    HostKeyAlias=compute.2908299054195589923
    IdentitiesOnly=yes
    CheckHostIP=no

# End of Google Compute Engine Section
```

Though the boilerplate says not to edit the alias stanza, changing the hostname seems to work fine.

#### 2.3.2. VSCode

Copy the Google Cloud Compute Engine alias from `~/.ssh/config` to `~/.ssh/vscode_config`:

```text
Host nostalgia-for-infinity
    HostName 34.31.191.133
    IdentityFile /home/siderealyear/.ssh/google_compute_engine
    UserKnownHostsFile=/home/siderealyear/.ssh/google_compute_known_hosts
    HostKeyAlias=compute.2908299054195589923
    IdentitiesOnly=yes
    CheckHostIP=no
```

Then, reload the VSCode window. Go to 'remotes' in the activity bar. The GC VM should be listed, click on the '→' symbol next to it to connect.

#### 2.3.3. Prevent automatic Conda environment activation

By default, Conda activates the base environment on login, disable this by running:

```bash
conda config --set auto_activate_base false
```

### 2.4. Persistent disk

[Create a new Persistent Disk volume](https://cloud.google.com/compute/docs/disks/add-persistent-disk)

#### 2.4.1. Create and attach disk

Create a disk via Google Cloud Console:

- Go to Compute Engine, Storage, Disks
- Click 'Create Disk', choose options

Attach the disk to the VM:

```bash
gcloud compute instances attach-disk nostalgia-for-infinity \
  --disk nostalgia-for-infinity-data --device-name=data
```

#### 2.4.2. Format and mount disk

From the VM, find the disk in `/dev`:

```bash
$  ls -l /dev/disk/by-id/google-*
lrwxrwxrwx 1 root root  9 Dec 28 18:40 /dev/disk/by-id/google-data -> ../../sdb
lrwxrwxrwx 1 root root  9 Dec 25 18:53 /dev/disk/by-id/google-instance-20241124-191956 -> ../../sda
lrwxrwxrwx 1 root root 10 Dec 25 18:53 /dev/disk/by-id/google-instance-20241124-191956-part1 -> ../../sda1
lrwxrwxrwx 1 root root 11 Dec 25 18:53 /dev/disk/by-id/google-instance-20241124-191956-part14 -> ../../sda14
lrwxrwxrwx 1 root root 11 Dec 25 18:53 /dev/disk/by-id/google-instance-20241124-191956-part15 -> ../../sda15
```

Our new disk is sdb - we can tell by:

1. The name - we called the device 'data', so it shows up as `google-data`
2. The fact that it does not have any partition symlinks yet

Format the disk as ext4, using the recommended options:

```bash
sudo mkfs.ext4 -m 0 -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/sdb
```

Set up the mount point. In this case we are mounting it to 'project' in the home directory.

```bash
mkdir -p ~/projects
sudo mount -o discard,defaults /dev/sdb /home/siderealyear/projects
sudo chown -r siderealyear:siderealyear /home/siderealyear/projects
```

Then add the mount to `/etc/fstab` using the disk's UUID to make the mount persistent across re-boot. First find the disk UUID:

```bash
$ sudo blkid /dev/sdb

/dev/sdb: UUID="04cc4fcf-f8f9-474c-acbd-43e085f0f3c7" BLOCK_SIZE="4096" TYPE="ext4"
```

Then, add it to `/etc/fstab`:

```text
UUID="04cc4fcf-f8f9-474c-acbd-43e085f0f3c7" /home/siderealyear/projects ext4 discard,defaults,nofail 0 2
```

#### 2.4.3. Resize disk

Persistent disks can be resized at any time. Two steps, set the new size in Google Cloud Console. Then, in the running vm, resize the partition.

Check the device ID:

```bash
$ lsblk

NAME    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda       8:0    0   50G  0 disk 
├─sda1    8:1    0 49.9G  0 part /
├─sda14   8:14   0    3M  0 part 
└─sda15   8:15   0  124M  0 part /boot/efi
sdb       8:16   0  256G  0 disk /home/siderealyear/projects
```

The persistent disk is sdb and shows the new size 256 GB. But, checking the file system with `df`, shows the old size:

```bash
$ df -h

Filesystem      Size  Used Avail Use% Mounted on
udev             59G     0   59G   0% /dev
tmpfs            12G  600K   12G   1% /run
/dev/sda1        49G   30G   18G  63% /
tmpfs            59G     0   59G   0% /dev/shm
tmpfs           5.0M     0  5.0M   0% /run/lock
/dev/sda15      124M   11M  114M   9% /boot/efi
/dev/sdb         98G   74G   25G  75% /home/siderealyear/projects
tmpfs            12G     0   12G   0% /run/user/1001
```

Here we see the persistent disk mounted as before, but the old size is listed. Use `parted` to resize the file system.

1. Run `sudo parted /dev/sdb` on the running VM.
2. At the prompt submit, `resizepart` and enter partition number 1.
3. Answer yes when prompted if you are sure.
4. When prompted for an 'End' value enter 100%.
5. Done, exit with the command: `quit`.
6. Once back at the system prompt, read the new partition table: `sudo partprobe /dev/sdb`.
7. Extend the file system: `sudo resize2fs /dev/sdb`

Check the result with:

```bash
$ df -h /dev/sdb

Filesystem      Size  Used Avail Use% Mounted on
/dev/sdb        252G   74G  179G  30% /home/siderealyear/projects
```

### 2.5. Install/configure ops agent

[Installing the Ops Agent on individual VMs](https://cloud.google.com/stackdriver/docs/solutions/agents/ops-agent/installation)

Install the ops agent on the VM:

```bash
curl -sSO https://dl.google.com/cloudagents/add-google-cloud-ops-agent-repo.sh
sudo bash ./add-google-cloud-ops-agent-repo.sh --also-install
```

Next, make sure the VM's service account has the `Monitoring Metric Writer` and `Logs Writer` roles.

- Find the service account on the VM's details page
- Go to IAM to check permissions - note: have not been able to find the 'default' compute service account listed in the VM details on the IAM page. The IAM page shows a default compute engine service account, but it's different. Instead, I use a service account I made for this purpose.

**Note**: VM must be stopped to change the service account. Verify the ops agent is running with:

```bash
sudo systemctl status google-cloud-ops-agent"*"
```

### 2.6. Move zones

[Move a VM instance between zones or regions](https://cloud.google.com/compute/docs/instances/moving-instance-across-zones)

To move a VM to a new zone, you need to make a machine image, preserve the persistent disks and then start a new machine based on the image in the new zone. Note the VM must be running at first for this to work.

#### 2.6.1. Protect disks from deletion

First, identify the disk(s):

```bash
$ gcloud compute instances describe nostalgia-for-infinity --format="list(name,status,disks)"

 - nostalgia-for-infinity
   TERMINATED
   [{'autoDelete': True, 'boot': True, 'deviceName': 'instance-20241124-191956', 'diskSizeGb': '50', 'guestOsFeatures': [{'type': 'VIRTIO_SCSI_MULTIQUEUE'}, {'type': 'UEFI_COMPATIBLE'}, {'type': 'GVNIC'}], 'index': 0, 'interface': 'SCSI', 'kind': 'compute#attachedDisk', 'licenses': ['https://www.googleapis.com/compute/v1/projects/click-to-deploy-images/global/licenses/c2d-tensorflow', 'https://www.googleapis.com/compute/v1/projects/click-to-deploy-images/global/licenses/c2d-dl-platform-gvnic', 'https://www.googleapis.com/compute/v1/projects/click-to-deploy-images/global/licenses/c2d-dl-platform-gpu-common-cu118', 'https://www.googleapis.com/compute/v1/projects/click-to-deploy-images/global/licenses/c2d-dl-platform-debian-11', 'https://www.googleapis.com/compute/v1/projects/click-to-deploy-images/global/licenses/c2d-dl-platform-ml-images', 'https://www.googleapis.com/compute/v1/projects/click-to-deploy-images/global/licenses/c2d-dl-platform-dlvm'], 'mode': 'READ_WRITE', 'source': 'https://www.googleapis.com/compute/v1/projects/ask-agatha/zones/us-central1-a/disks/instance-20241124-191956', 'type': 'PERSISTENT'}, {'autoDelete': False, 'boot': False, 'deviceName': 'data', 'diskSizeGb': '256', 'index': 1, 'interface': 'SCSI', 'kind': 'compute#attachedDisk', 'mode': 'READ_WRITE', 'source': 'https://www.googleapis.com/compute/v1/projects/ask-agatha/zones/us-central1-a/disks/nostalgia-for-infinity-data', 'type': 'PERSISTENT'}]
```

In the above we see two disks called `instance-20241124-191956` (the boot disk) and `data`. Update both disks with autodelete false, so they are not deleted when we delete the machine in the old zone. Note - the set-disk command seems to need the Name while the describe command returns the device name....

```bash
$ gcloud compute instances set-disk-auto-delete nostalgia-for-infinity --zone us-central1-a \
    --disk instance-20241124-191956 --no-auto-delete

Updated [https://www.googleapis.com/compute/v1/projects/ask-agatha/zones/us-central1-a/instances/nostalgia-for-infinity].
```

```bash
$ gcloud compute instances set-disk-auto-delete nostalgia-for-infinity --zone us-central1-a \
    --disk nostalgia-for-infinity-data --no-auto-delete

No change requested; skipping update for [nostalgia-for-infinity].
```

Good - double check that all disks are set to `'autoDelete': False` before proceeding. Then download the machine metadata if desired.

```bash
$ gcloud compute instances describe nostalgia-for-infinity --zone us-central1-a

  provisioningModel: STANDARD
selfLink: https://www.googleapis.com/compute/v1/projects/ask-agatha/zones/us-central1-a/instances/nostalgia-for-infinity
serviceAccounts:
- email: ask-agatha-service@ask-agatha.iam.gserviceaccount.com
  scopes:
  - https://www.googleapis.com/auth/cloud-platform
shieldedInstanceConfig:
  enableIntegrityMonitoring: true
  enableSecureBoot: false
  enableVtpm: true
shieldedInstanceIntegrityPolicy:
  updateAutoLearnPolicy: true
startRestricted: false
status: RUNNING
tags:
  fingerprint: 42WmSpB8rSM=
zone: https://www.googleapis.com/compute/v1/projects/ask-agatha/zones/us-central1-a
```

Then back up your data, just in case:

```bash
$ gcloud compute disks snapshot instance-20241124-191956 nostalgia-for-infinity-data \
    --snapshot-names backup-nostalgia-for-infinity-boot,backup-nostalgia-for-infinity-data \
    --zone us-central1-a

Creating snapshot(s) backup-nostalgia-for-infinity-boot, backup-nostalgia-for-infinity-data...done.
```

#### 2.6.2. Shut down and delete old VM

```bash
$ gcloud compute instances delete nostalgia-for-infinity --zone us-central1-a

The following instances will be deleted. Any attached disks configured to be auto-deleted will 
be deleted unless they are attached to any other instances or the `--keep-disks` flag is 
given and specifies them for keeping. Deleting a disk is irreversible and any data on the disk 
will be lost.
 - [nostalgia-for-infinity] in [us-central1-a]

Do you want to continue (Y/n)? Y

Deleted [https://www.googleapis.com/compute/v1/projects/ask-agatha/zones/us-central1-a/instances/nostalgia-for-infinity].
```

#### 2.6.3. Create new persistent disks

Now, create new snapshots of the persistent disks for the new machine:

```bash
$ gcloud compute disks snapshot instance-20241124-191956 nostalgia-for-infinity-data \
    --snapshot-names nostalgia-for-infinity-boot,nostalgia-for-infinity-data \
    --zone us-central1-a

Creating snapshot(s) nostalgia-for-infinity-boot, nostalgia-for-infinity-data...done. 
```

Then use the snapshots to create new persistent disks in the new region/zone:

```bash
$ gcloud compute disks create nostalgia-for-infinity-boot --source-snapshot nostalgia-for-infinity-boot \
    --zone us-central1-c

Created [https://www.googleapis.com/compute/v1/projects/ask-agatha/zones/us-central1-c/disks/nostalgia-for-infinity-boot].
NAME                         ZONE           SIZE_GB  TYPE         STATUS
nostalgia-for-infinity-boot  us-central1-c  50       pd-standard  READY
```

```bash
gcloud compute disks create nostalgia-for-infinity-data --source-snapshot nostalgia-for-infinity-data \
    --zone us-central1-c

Created [https://www.googleapis.com/compute/v1/projects/ask-agatha/zones/us-central1-c/disks/nostalgia-for-infinity-data].
NAME                         ZONE           SIZE_GB  TYPE         STATUS
nostalgia-for-infinity-data  us-central1-c  256      pd-standard  READY
```

#### 2.6.4. Create new VM

Now, use Google Cloud Console to create a new VM in the new region/zone and attach the disks. Then be sure to clean up old persistent disks and snapshots once everything is running.

A N1 machine with 4 cores and 16 GB RAM costs a few hundred dollars a month with 4 T4 GPUs attached. To get 4 P100s costs a few thousand dollars a month and needs a quota increase request.

## 3. Transfer files to/from a VM

Sounds like the best solution for this is a shared Google Cloud Storage bucket. The bucket is mounted on the Compute Engine VM and wherever else file access is needed. The following instructions should work on the VM and any other Debian based Linux machine.

### 3.1. Create a Cloud Storage bucket

[Create buckets](https://cloud.google.com/storage/docs/creating-buckets)

From Google Cloud Console:

- Navigation menu ⇾ Cloud Storage ⇾ Buckets ⇾ 'CREATE BUCKET'
- Set the name and pick single region
- Set autoclass and enable Coldline and Archive classes
- Set the region to match the VM's
- Use uniform, bucket level permissions
- Use default security settings
- 'CREATE', make sure 'prevent public access' is selects and continue

### 3.2. Mount the bucket

- [Mount a Cloud Storage bucket using Cloud Storage FUSE](https://cloud.google.com/storage/docs/cloud-storage-fuse/quickstart-mount-bucket)

#### 3.2.1. Install Cloud Storage FUSE

Set up the Google Cloud Storage APT repository:

```bash
export GCSFUSE_REPO=gcsfuse-`lsb_release -c -s`
echo "deb https://packages.cloud.google.com/apt $GCSFUSE_REPO main" | sudo tee /etc/apt/sources.list.d/gcsfuse.list
curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
```

Install Cloud Storage FUSE

```bash
sudo apt update
sudo apt upgrade
sudo apt-get install fuse gcsfuse
```

Check the installation:

```bash
gcsfuse -v
$ gcsfuse version 2.7.0 (Go version go1.23.4)
```

### 3.2.2. Mount the bucket

Generate credentials:

```bash
gcloud auth application-default login
```

Make a mount directory:

```bash
mkdir /path/to/mount
```

Depending on where you make the directory, you may need chown it with `username:username`. Then you can mount the bucket:

```bash
gcsfuse BUCKET_NAME "/path/to/mount"
```

Done!