{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw data length binning\n",
    "\n",
    "The purpose of this notebook is to design a strategy to load perplexity ratio score data generated by the scoring algorithm, bin it by text fragment length and get it ready for input into feature engineering. The plan is to handle data as pandas dataframes and store it using hdf5. The text fragments will be put into overlapping length bins so that feature engineering and classifier training can be conducted separately for different length regimes. This general strategy is based on early observations of classifier performance on short fragments, long fragments and un-binned fragments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The bins we are planning on using\n",
    "bins = {\n",
    "    'bin_100': [1, 100],\n",
    "    'bin_150': [51, 150],\n",
    "    'bin_200': [101, 200],\n",
    "    'bin_250': [151, 250],\n",
    "    'bin_300': [201, 300],\n",
    "    'bin_350': [251, 350],\n",
    "    'bin_400': [301, 400],\n",
    "    'bin_450': [351, 450],\n",
    "    'bin_500': [401, 500],\n",
    "    'bin_550': [451, 550],\n",
    "    'bin_600': [501, 600]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/arkk/llm_detector/classifier\n"
     ]
    }
   ],
   "source": [
    "# Change working directory to parent so we can import as we would from __main__.py\n",
    "%cd ..\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import configuration as config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fragment ID</th>\n",
       "      <th>Source record num</th>\n",
       "      <th>Fragment length (words)</th>\n",
       "      <th>Fragment length (tokens)</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Source</th>\n",
       "      <th>Generator</th>\n",
       "      <th>String</th>\n",
       "      <th>Perplexity</th>\n",
       "      <th>Cross-perplexity</th>\n",
       "      <th>Perplexity ratio score</th>\n",
       "      <th>Reader time (seconds)</th>\n",
       "      <th>Writer time (seconds)</th>\n",
       "      <th>Reader peak memory (GB)</th>\n",
       "      <th>Writer peak memory (GB)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8161</td>\n",
       "      <td>187</td>\n",
       "      <td>258</td>\n",
       "      <td>cnn</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>llama2-13b</td>\n",
       "      <td>a natural ally. The BBC is where I’d go afterw...</td>\n",
       "      <td>2.482</td>\n",
       "      <td>2.611328</td>\n",
       "      <td>0.950636</td>\n",
       "      <td>3.734034</td>\n",
       "      <td>4.157798</td>\n",
       "      <td>6.948370</td>\n",
       "      <td>6.889287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5356</td>\n",
       "      <td>239</td>\n",
       "      <td>359</td>\n",
       "      <td>cc_news</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>llama2-13b</td>\n",
       "      <td>Boston. (AP Photo/Michael Dwyer)\\nBOSTON (AP) ...</td>\n",
       "      <td>1.712</td>\n",
       "      <td>1.957031</td>\n",
       "      <td>0.874750</td>\n",
       "      <td>4.433797</td>\n",
       "      <td>5.023460</td>\n",
       "      <td>8.617490</td>\n",
       "      <td>8.522192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1414</td>\n",
       "      <td>85</td>\n",
       "      <td>106</td>\n",
       "      <td>cnn</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>falcon7</td>\n",
       "      <td>and 'Viennese whirls'.\\nThe Lib Dems were also...</td>\n",
       "      <td>2.797</td>\n",
       "      <td>3.398438</td>\n",
       "      <td>0.822989</td>\n",
       "      <td>1.953291</td>\n",
       "      <td>2.140416</td>\n",
       "      <td>5.556048</td>\n",
       "      <td>5.527256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7883</td>\n",
       "      <td>373</td>\n",
       "      <td>480</td>\n",
       "      <td>cnn</td>\n",
       "      <td>human</td>\n",
       "      <td>human</td>\n",
       "      <td>it in the face of hostility. While waiting for...</td>\n",
       "      <td>3.178</td>\n",
       "      <td>3.060547</td>\n",
       "      <td>1.038290</td>\n",
       "      <td>5.638005</td>\n",
       "      <td>6.512831</td>\n",
       "      <td>7.472024</td>\n",
       "      <td>7.467131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9617</td>\n",
       "      <td>175</td>\n",
       "      <td>223</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>human</td>\n",
       "      <td>human</td>\n",
       "      <td>of prostate cancer .\\nrecent studies indicate ...</td>\n",
       "      <td>2.605</td>\n",
       "      <td>2.720703</td>\n",
       "      <td>0.957645</td>\n",
       "      <td>3.181693</td>\n",
       "      <td>3.511006</td>\n",
       "      <td>7.577057</td>\n",
       "      <td>7.466524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fragment ID  Source record num  Fragment length (words)  \\\n",
       "0            0               8161                      187   \n",
       "1            1               5356                      239   \n",
       "2            2               1414                       85   \n",
       "3            3               7883                      373   \n",
       "4            4               9617                      175   \n",
       "\n",
       "   Fragment length (tokens)  Dataset     Source   Generator  \\\n",
       "0                       258      cnn  synthetic  llama2-13b   \n",
       "1                       359  cc_news  synthetic  llama2-13b   \n",
       "2                       106      cnn  synthetic     falcon7   \n",
       "3                       480      cnn      human       human   \n",
       "4                       223   pubmed      human       human   \n",
       "\n",
       "                                              String  Perplexity  \\\n",
       "0  a natural ally. The BBC is where I’d go afterw...       2.482   \n",
       "1  Boston. (AP Photo/Michael Dwyer)\\nBOSTON (AP) ...       1.712   \n",
       "2  and 'Viennese whirls'.\\nThe Lib Dems were also...       2.797   \n",
       "3  it in the face of hostility. While waiting for...       3.178   \n",
       "4  of prostate cancer .\\nrecent studies indicate ...       2.605   \n",
       "\n",
       "   Cross-perplexity  Perplexity ratio score  Reader time (seconds)  \\\n",
       "0          2.611328                0.950636               3.734034   \n",
       "1          1.957031                0.874750               4.433797   \n",
       "2          3.398438                0.822989               1.953291   \n",
       "3          3.060547                1.038290               5.638005   \n",
       "4          2.720703                0.957645               3.181693   \n",
       "\n",
       "   Writer time (seconds)  Reader peak memory (GB)  Writer peak memory (GB)  \n",
       "0               4.157798                 6.948370                 6.889287  \n",
       "1               5.023460                 8.617490                 8.522192  \n",
       "2               2.140416                 5.556048                 5.527256  \n",
       "3               6.512831                 7.472024                 7.467131  \n",
       "4               3.511006                 7.577057                 7.466524  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "data_df = pd.read_json(config.RAW_INPUT_DATA)\n",
    "\n",
    "# Replace and remove string 'OOM' and 'NAN' values\n",
    "data_df.replace('NAN', np.nan, inplace = True)\n",
    "data_df.replace('OOM', np.nan, inplace = True)\n",
    "data_df.dropna(inplace = True)\n",
    "\n",
    "# Shuffle the deck, resetting the index\n",
    "data_df = data_df.sample(frac = 1).reset_index(drop = True)\n",
    "\n",
    "# Use the index to add a unique fragment id\n",
    "data_df.reset_index(inplace = True)\n",
    "data_df.rename({'index': 'Fragment ID'}, axis = 1, inplace = True)\n",
    "\n",
    "# Enforce dtypes\n",
    "data_df = data_df.astype({\n",
    "    'Fragment ID': np.int32,\n",
    "    'Source record num': np.int32,\n",
    "    'Fragment length (words)': np.int32,\n",
    "    'Fragment length (tokens)': np.int32,\n",
    "    'Dataset': object, #pd.StringDtype(),\n",
    "    'Source': object, #pd.StringDtype(),\n",
    "    'Generator': object, #pd.StringDtype(),\n",
    "    'String': object, #pd.StringDtype(),\n",
    "    'Perplexity': np.float32,\n",
    "    'Cross-perplexity': np.float32,\n",
    "    'Perplexity ratio score': np.float32,\n",
    "    'Reader time (seconds)': np.float32,\n",
    "    'Writer time (seconds)': np.float32,\n",
    "    'Reader peak memory (GB)': np.float32,\n",
    "    'Writer peak memory (GB)': np.float32\n",
    "})\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34196 entries, 0 to 34195\n",
      "Data columns (total 15 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Fragment ID               34196 non-null  int32  \n",
      " 1   Source record num         34196 non-null  int32  \n",
      " 2   Fragment length (words)   34196 non-null  int32  \n",
      " 3   Fragment length (tokens)  34196 non-null  int32  \n",
      " 4   Dataset                   34196 non-null  object \n",
      " 5   Source                    34196 non-null  object \n",
      " 6   Generator                 34196 non-null  object \n",
      " 7   String                    34196 non-null  object \n",
      " 8   Perplexity                34196 non-null  float32\n",
      " 9   Cross-perplexity          34196 non-null  float32\n",
      " 10  Perplexity ratio score    34196 non-null  float32\n",
      " 11  Reader time (seconds)     34196 non-null  float32\n",
      " 12  Writer time (seconds)     34196 non-null  float32\n",
      " 13  Reader peak memory (GB)   34196 non-null  float32\n",
      " 14  Writer peak memory (GB)   34196 non-null  float32\n",
      "dtypes: float32(7), int32(4), object(4)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's start structuring the dataset. We want two top level groups, one for training data and one for reserved testing data. Inside those groups will live datasets for the bins. We will also use attributes to save some metadata etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output is type: <class 'h5py._hl.files.File'>\n",
      "Top level groups: ['testing', 'training']\n",
      "\n",
      "testing contains:\n",
      " bin_100\n",
      " bin_150\n",
      " bin_200\n",
      " bin_250\n",
      " bin_300\n",
      " bin_350\n",
      " bin_400\n",
      " bin_450\n",
      " bin_500\n",
      " bin_550\n",
      " bin_600\n",
      " combined\n",
      "\n",
      "training contains:\n",
      " bin_100\n",
      " bin_150\n",
      " bin_200\n",
      " bin_250\n",
      " bin_300\n",
      " bin_350\n",
      " bin_400\n",
      " bin_450\n",
      " bin_500\n",
      " bin_550\n",
      " bin_600\n",
      " combined\n"
     ]
    }
   ],
   "source": [
    "# Prepare the hdf5 output - create or open for read/write\n",
    "output = h5py.File(config.LENGTH_BINNED_DATASET, 'a')\n",
    "\n",
    "# Create the top-level groups\n",
    "_ = output.require_group('training')\n",
    "_ = output.require_group('testing')\n",
    "\n",
    "print(f'Output is type: {type(output)}')\n",
    "print(f'Top level groups: {(list(output.keys()))}')\n",
    "\n",
    "# Next, we need to add a group for each fragment length bin,\n",
    "# and one for the un-binned data\n",
    "for group in output.keys():\n",
    "\n",
    "    # Add the un-binned data group\n",
    "    _ = output.require_group(f'{group}/combined')\n",
    "\n",
    "    # Loop on the bins and add a group for each\n",
    "    for bin in bins.keys():\n",
    "        _ = output.require_group(f'{group}/{bin}')\n",
    "\n",
    "# Print the result\n",
    "for group in output.keys():\n",
    "    print(f'\\n{group} contains:')\n",
    "\n",
    "    for subgroup in output[group]:\n",
    "        print(f' {subgroup}')\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, the basic data structure is ready to go. Next thing to do is add data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features_labels(training_df, testing_df):\n",
    "    '''Takes training and testing dataframes, separates features\n",
    "    from labels, encodes labels and returns'''\n",
    "\n",
    "    # Split the data into features and labels\n",
    "    training_labels = training_df['Source']\n",
    "    training_features = training_df.drop('Source', axis = 1)\n",
    "\n",
    "    testing_labels = testing_df['Source']\n",
    "    testing_features = testing_df.drop('Source', axis = 1)\n",
    "\n",
    "    # Encode string class values as integers\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder = label_encoder.fit(training_labels)\n",
    "    training_labels = pd.Series(label_encoder.transform(training_labels)).astype(np.int32)\n",
    "    testing_labels = pd.Series(label_encoder.transform(testing_labels)).astype(np.int32)\n",
    "\n",
    "    return training_labels, testing_labels, training_features, testing_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reopen out hdf5 file with pandas so we can work with dataframes\n",
    "data_lake = pd.HDFStore(config.LENGTH_BINNED_DATASET)\n",
    "\n",
    "# Add the raw data set at the top level as 'master' incase we want it later.\n",
    "data_lake['master'] = data_df\n",
    "\n",
    "# Next, get rid of un-trainable/unnecessary features\n",
    "feature_drops = [\n",
    "    'Fragment ID',\n",
    "    'Source record num',\n",
    "    'Dataset',\n",
    "    'Generator',\n",
    "    'String',\n",
    "    'Reader time (seconds)',\n",
    "    'Writer time (seconds)',\n",
    "    'Reader peak memory (GB)',\n",
    "    'Writer peak memory (GB)'\n",
    "]\n",
    "\n",
    "data_df.drop(feature_drops, axis = 1, inplace = True)\n",
    "\n",
    "# Split the data into training and testing\n",
    "training_df = data_df.sample(frac = 0.7, random_state = 42)\n",
    "testing_df = data_df.drop(training_df.index)\n",
    "\n",
    "# Fix the index\n",
    "training_df.reset_index(inplace = True, drop = True)\n",
    "testing_df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# Split un-binned data into features and labels\n",
    "training_labels, testing_labels, training_features, testing_features = make_features_labels(training_df, testing_df)\n",
    "\n",
    "# Add the un-binned data to the data lake\n",
    "data_lake.put(f'training/combined/features', training_features)\n",
    "data_lake.put(f'training/combined/labels', training_labels)\n",
    "data_lake.put(f'testing/combined/features', training_features)\n",
    "data_lake.put(f'testing/combined/labels', training_labels)\n",
    "\n",
    "# Now do the same for the bins\n",
    "for bin_id, bin_range in bins.items():\n",
    "\n",
    "    # Pull the fragments for this bin\n",
    "    bin_training_df = training_df[(training_df['Fragment length (words)'] >= bin_range[0]) & (training_df['Fragment length (words)'] <= bin_range[1])]\n",
    "    bin_testing_df = testing_df[(testing_df['Fragment length (words)'] >= bin_range[0]) & (testing_df['Fragment length (words)'] <= bin_range[1])]\n",
    "\n",
    "    # Fix the index\n",
    "    bin_training_df.reset_index(inplace = True, drop = True)\n",
    "    bin_testing_df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "    # Split un-binned data into features and labels\n",
    "    bin_training_labels, bin_testing_labels, bin_training_features, bin_testing_features = make_features_labels(bin_training_df, bin_testing_df)\n",
    "\n",
    "    # Add the data to the data lake\n",
    "    data_lake.put(f'training/{bin_id}/features', bin_training_features)\n",
    "    data_lake.put(f'training/{bin_id}/labels', bin_training_labels)\n",
    "    data_lake.put(f'testing/{bin_id}/features', bin_training_features)\n",
    "    data_lake.put(f'testing/{bin_id}/labels', bin_training_labels)\n",
    "\n",
    "data_lake.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "master contains:\n",
      " axis0\n",
      " axis1\n",
      " block0_items\n",
      " block0_values\n",
      " block1_items\n",
      " block1_values\n",
      " block2_items\n",
      " block2_values\n",
      "\n",
      "testing contains:\n",
      "  bin_100 contains:\n",
      "    features\n",
      "    labels\n",
      "  bin_150 contains:\n",
      "    features\n",
      "    labels\n",
      "  bin_200 contains:\n",
      "    features\n",
      "    labels\n",
      "  bin_250 contains:\n",
      "    features\n",
      "    labels\n",
      "  bin_300 contains:\n",
      "    features\n",
      "    labels\n",
      "  bin_350 contains:\n",
      "    features\n",
      "    labels\n",
      "  bin_400 contains:\n",
      "    features\n",
      "    labels\n",
      "  bin_450 contains:\n",
      "    features\n",
      "    labels\n",
      "  bin_500 contains:\n",
      "    features\n",
      "    labels\n",
      "  bin_550 contains:\n",
      "    features\n",
      "    labels\n",
      "  bin_600 contains:\n",
      "    features\n",
      "    labels\n",
      "  combined contains:\n",
      "    features\n",
      "    labels\n",
      "\n",
      "training contains:\n",
      "  bin_100 contains:\n",
      "    features\n",
      "    labels\n",
      "  bin_150 contains:\n",
      "    features\n",
      "    labels\n",
      "  bin_200 contains:\n",
      "    features\n",
      "    labels\n",
      "  bin_250 contains:\n",
      "    features\n",
      "    labels\n",
      "  bin_300 contains:\n",
      "    features\n",
      "    labels\n",
      "  bin_350 contains:\n",
      "    features\n",
      "    labels\n",
      "  bin_400 contains:\n",
      "    features\n",
      "    labels\n",
      "  bin_450 contains:\n",
      "    features\n",
      "    labels\n",
      "  bin_500 contains:\n",
      "    features\n",
      "    labels\n",
      "  bin_550 contains:\n",
      "    features\n",
      "    labels\n",
      "  bin_600 contains:\n",
      "    features\n",
      "    labels\n",
      "  combined contains:\n",
      "    features\n",
      "    labels\n"
     ]
    }
   ],
   "source": [
    "# Open the data lake to check out the result\n",
    "data_lake = h5py.File(config.LENGTH_BINNED_DATASET, 'a')\n",
    "\n",
    "# Print the result\n",
    "for group in data_lake.keys():\n",
    "    print(f'\\n{group} contains:')\n",
    "\n",
    "    for subgroup in data_lake[group]:\n",
    "\n",
    "        if 'bin' in subgroup:\n",
    "            print(f'  {subgroup} contains:')\n",
    "\n",
    "            for subsubgroup in data_lake[group][subgroup].keys():\n",
    "                print(f'    {subsubgroup}')\n",
    "\n",
    "        else:\n",
    "            print(f' {subgroup}')\n",
    "\n",
    "data_lake.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined training features:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 23937 entries, 0 to 23936\n",
      "Data columns (total 5 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Fragment length (words)   23937 non-null  int32  \n",
      " 1   Fragment length (tokens)  23937 non-null  int32  \n",
      " 2   Perplexity                23937 non-null  float32\n",
      " 3   Cross-perplexity          23937 non-null  float32\n",
      " 4   Perplexity ratio score    23937 non-null  float32\n",
      "dtypes: float32(3), int32(2)\n",
      "memory usage: 654.5 KB\n",
      "None\n",
      "\n",
      "Combined training labels:\n",
      "\n",
      "<class 'pandas.core.series.Series'>\n",
      "Index: 23937 entries, 0 to 23936\n",
      "Series name: None\n",
      "Non-Null Count  Dtype\n",
      "--------------  -----\n",
      "23937 non-null  int32\n",
      "dtypes: int32(1)\n",
      "memory usage: 280.5 KB\n",
      "None\n",
      "\n",
      "Bin 100 training features:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 8356 entries, 0 to 8355\n",
      "Data columns (total 5 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Fragment length (words)   8356 non-null   int32  \n",
      " 1   Fragment length (tokens)  8356 non-null   int32  \n",
      " 2   Perplexity                8356 non-null   float32\n",
      " 3   Cross-perplexity          8356 non-null   float32\n",
      " 4   Perplexity ratio score    8356 non-null   float32\n",
      "dtypes: float32(3), int32(2)\n",
      "memory usage: 228.5 KB\n",
      "None\n",
      "\n",
      "Bin 100 training labels:\n",
      "\n",
      "<class 'pandas.core.series.Series'>\n",
      "Index: 8356 entries, 0 to 8355\n",
      "Series name: None\n",
      "Non-Null Count  Dtype\n",
      "--------------  -----\n",
      "8356 non-null   int32\n",
      "dtypes: int32(1)\n",
      "memory usage: 97.9 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Reopen out hdf5 file with pandas so we can work with dataframes\n",
    "data_lake = pd.HDFStore(config.LENGTH_BINNED_DATASET)\n",
    "\n",
    "print('Combined training features:\\n')\n",
    "print(data_lake['training/combined/features'].info())\n",
    "print('\\nCombined training labels:\\n')\n",
    "print(data_lake['training/combined/labels'].info())\n",
    "print('\\nBin 100 training features:\\n')\n",
    "print(data_lake['training/bin_100/features'].info())\n",
    "print('\\nBin 100 training labels:\\n')\n",
    "print(data_lake['training/bin_100/labels'].info())\n",
    "\n",
    "data_lake.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
