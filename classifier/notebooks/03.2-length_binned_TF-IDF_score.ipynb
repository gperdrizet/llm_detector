{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF score: Kullback–Leibler divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/arkk/llm_detector/classifier\n"
     ]
    }
   ],
   "source": [
    "# Change working directory to parent so we can import as we would from main.py\n",
    "%cd ..\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import gc\n",
    "import re\n",
    "import nltk\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "\n",
    "from IPython.display import Image\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import configuration as config\n",
    "import functions.kullback_leibler_divergence as kld_funcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan here is to take our sampling distributions of text frequency, inverse document frequency (TF-IDF) scores for human and synthetic text and use them to generate a function that takes a TF-IDF score and converts it into a Kullback-Leibler divergence (KLD) score. See the figure below from the [Wikipedia article on KLD](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence).\n",
    "\n",
    "Workflow is as follows:\n",
    "1. Get kernel density estimate of TF-IDF score distribution for human and synthetic text fragments in training data.\n",
    "2. Calculated KLD between the human and synthetic TF-IDF score distributions.\n",
    "3. Get get kernel density estimate of KLD.\n",
    "4. Use probability density function of KLD kernel density estimate to calculate KLD score for each text fragment in the training and testing data.\n",
    "5. Add the KLD score as a new feature.\n",
    "\n",
    "The above will be done individually for each fragment length bin and the combined data. This way the KLD score feature in each bin will capture the TF-IDF score distribution for text fragments in that specific length regime, rather that for the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/gperdrizet/llm_detector/benchmarking/benchmarking/notebooks/images/KL-Gauss-Example.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url = 'https://raw.githubusercontent.com/gperdrizet/llm_detector/benchmarking/benchmarking/notebooks/images/KL-Gauss-Example.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TF-IDF score\n",
    "Before calculating a Kullback-Leibler divergence score for the TF-IDF score, we need to calculate the TF-IDF score itself for each fragment.\n",
    "\n",
    "The TF-IDF score created for this project involves scoring each text fragment with TF-IDF term frequencies derived from the human and synthetic text fragments in the training data. The TF-IDF score is a product normalized difference calculated as:\n",
    "\n",
    "$$ (human - synthetic)(human + synthetic) $$\n",
    "\n",
    "Where human and synthetic refer to average TF-IDF by term for a given text fragment where the term TF-IDF values were derived from the human or synthetic text in the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Get human and synthetic text strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and set up stop words and an instance of the Word Net\n",
    "# Lemmatizer for use in cleaning text for vectorization\n",
    "nltk.download('stopwords', quiet = True)\n",
    "nltk.download('wordnet', quiet = True)\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "sw = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "def clean_text(text: str = None) -> str:\n",
    "    '''Takes a text string and cleans it for vectorization.\n",
    "    Returns cleaned text as string.'''\n",
    "    \n",
    "    # Lowercase everything\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\n",
    "\n",
    "    # Remove URLs \n",
    "    text = re.sub(r\"http\\S+\", \"\",text)\n",
    "    \n",
    "    # Remove html tags\n",
    "    html = re.compile(r'<.*?>') \n",
    "    text = html.sub(r'',text)\n",
    "    \n",
    "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_'\n",
    "\n",
    "    # Remove punctuations\n",
    "    for p in punctuations:\n",
    "        text = text.replace(p,'')\n",
    "        \n",
    "    # Remove stopwords\n",
    "    text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    # Remove emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "    \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(bin_training_data_df: pd.DataFrame) ->tuple[pd.Series, pd.Series]:\n",
    "    '''Gets and cleans human and synthetic text from training data.'''\n",
    "\n",
    "    # If we have more than 10,000 text fragments, take a random sample of 10,000\n",
    "    # to keep memory utilization under control during vectorization, then get the\n",
    "    # text strings for the human and synthetic text fragments in the sample\n",
    "    if len(bin_training_data_df) > 10000:\n",
    "    \n",
    "        training_data_df_sample = bin_training_data_df.sample(n = 10000, random_state = 42)\n",
    "        training_data_df_sample.reset_index(inplace = True, drop = True)\n",
    "\n",
    "        human_texts = training_data_df_sample['String'][training_data_df_sample['Source'] == 'human']\n",
    "        synthetic_texts = training_data_df_sample['String'][training_data_df_sample['Source'] == 'synthetic']\n",
    "\n",
    "    # If the dataset has 10,000 or less text fragments, directly pull all of the\n",
    "    # text fragment strings for human and synthetic fragments from the data\n",
    "    else:\n",
    "        \n",
    "        human_texts = bin_training_data_df['String'][bin_training_data_df['Source'] == 'human']\n",
    "        synthetic_texts = bin_training_data_df['String'][bin_training_data_df['Source'] == 'synthetic']\n",
    "\n",
    "    # Clean text for vectorization\n",
    "    human_texts = human_texts.apply(lambda x: clean_text(x))\n",
    "    synthetic_texts = synthetic_texts.apply(lambda x: clean_text(x))\n",
    "\n",
    "    return human_texts, synthetic_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Get term TF-IDF values for human and synthetic text from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_tf_idf(human_texts: pd.Series, synthetic_texts: pd.Series) -> dict:\n",
    "    '''Takes cleaned human and synthetic text as Pandas series, gets term TF-IDF values\n",
    "    for each and returns as dictionary of look-up tables where key is term feature and\n",
    "    value is term TF-IDF.'''\n",
    "\n",
    "    # Dictionary to hold TF-IDF look-up tables\n",
    "    tfidf_luts = {}\n",
    "\n",
    "    # Loop twice to process the human and synthetic texts the same way\n",
    "    for text_source, texts in zip(['human', 'synthetic'], [human_texts, synthetic_texts]):\n",
    "\n",
    "        # Fit the TF-IDF vectorizer\n",
    "        tfidf_vectorizer = TfidfVectorizer()\n",
    "        tfidf_vectors = tfidf_vectorizer.fit_transform(texts)\n",
    "\n",
    "        # Convert the vectors to numpy and replace zeros with NAN\n",
    "        tfidf = tfidf_vectors.toarray()\n",
    "        tfidf[tfidf == 0] = np.nan\n",
    "\n",
    "        # Take the log2 and average the columns (i.e. get average TF-IDF per word)\n",
    "        log_tfidf = np.log2(tfidf)\n",
    "        log_tfidf_mean = np.nanmean(log_tfidf, axis = 0)\n",
    "\n",
    "        # Get the words\n",
    "        features = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "        # Release some memory\n",
    "        del tfidf_vectorizer\n",
    "        del tfidf_vectors\n",
    "        _ = gc.collect()\n",
    "\n",
    "        # Add result to look-up table\n",
    "        tfidf_luts[text_source] = dict(zip(features, log_tfidf_mean))\n",
    "\n",
    "    return tfidf_luts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Score each text fragment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_score_text_fragments(data_df: pd.DataFrame, tfidf_luts: dict = None) -> dict:\n",
    "    '''Takes features dataframe and dictionary containing term TF-IDF look-up tables.\n",
    "    scores text fragments from dataframe with product normalized difference in log2 TF-IDF mean.\n",
    "    Adds TF-IDF score and log2 TF-IDF mean'''\n",
    "\n",
    "    # Holders for new features\n",
    "    tfidf_scores = []\n",
    "    human_tfidf = []\n",
    "    synthetic_tfidf = []\n",
    "\n",
    "    # Get the text fragments\n",
    "    texts = data_df['String']\n",
    "\n",
    "    # Loop on dataframe rows\n",
    "    for text in texts:\n",
    "\n",
    "        # Clean the text\n",
    "        text = clean_text(text)\n",
    "\n",
    "        # Split the text into words\n",
    "        words = text.split(' ')\n",
    "\n",
    "        # Score the words using the human and synthetic luts only scoring words for which we have\n",
    "        # a human and a synthetic TF-IDF value\n",
    "        scored_word_count = 0\n",
    "        human_tfidf_sum = 0\n",
    "        synthetic_tfidf_sum = 0\n",
    "\n",
    "        for word in words:\n",
    "\n",
    "            if word in tfidf_luts['human'].keys() and word in tfidf_luts['synthetic'].keys():\n",
    "                human_tfidf_sum += tfidf_luts['human'][word]\n",
    "                synthetic_tfidf_sum += tfidf_luts['synthetic'][word]\n",
    "\n",
    "                scored_word_count += 1\n",
    "\n",
    "        # Get the means, protecting from division by zero\n",
    "        if scored_word_count == 0:\n",
    "\n",
    "            human_tfidf_mean = 0\n",
    "            synthetic_tfidf_mean = 0\n",
    "\n",
    "        elif scored_word_count != 0:\n",
    "            \n",
    "            human_tfidf_mean = human_tfidf_sum / scored_word_count\n",
    "            synthetic_tfidf_mean = synthetic_tfidf_sum / scored_word_count\n",
    "\n",
    "        # Get the product normalized TF-IDF score\n",
    "        dmean_tfidf = human_tfidf_mean - synthetic_tfidf_mean\n",
    "        product_normalized_dmean_tfidf = dmean_tfidf * (human_tfidf_mean + synthetic_tfidf_mean)\n",
    "\n",
    "        # Add to results\n",
    "        human_tfidf.append(human_tfidf_mean)\n",
    "        synthetic_tfidf.append(synthetic_tfidf_mean)\n",
    "        tfidf_scores.append(product_normalized_dmean_tfidf)\n",
    "\n",
    "    # Add new feature back to dataframe\n",
    "    data_df['Human TF-IDF'] = human_tfidf\n",
    "    data_df['Synthetic TF-IDF'] = synthetic_tfidf\n",
    "    data_df['TF-IDF score'] = tfidf_scores\n",
    "\n",
    "    return data_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Add TF-IDF score to data in length bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tf_idf_score(\n",
    "        bin_training_data_df: pd.DataFrame, \n",
    "        bin_testing_data_df: pd.DataFrame,\n",
    "        worker_num: str,\n",
    "        bin_id: int\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \n",
    "    '''Takes training and testing datasets in dataframes. Uses training\n",
    "    data to calculate term TF-IDF for human and synthetic data. Uses those\n",
    "    term TF-IDF values to calculate product normalized TF-IDF score for\n",
    "    each text fragment in the training and testing data. Adds TF-IDF score\n",
    "    to dataframes as new features and return the updated dataframes.'''\n",
    "\n",
    "    try:\n",
    "        human_texts, synthetic_texts = get_text(bin_training_data_df)\n",
    "\n",
    "    except Exception as err_string:\n",
    "        print(f'\\nWorker {worker_num} - get_text() error: {err_string}', end = '')\n",
    "\n",
    "    try:\n",
    "        tfidf_luts = get_term_tf_idf(human_texts, synthetic_texts)\n",
    "\n",
    "    except Exception as err_string:\n",
    "        print(f'\\nWorker {worker_num} - get_term_tf_idf() error: {err_string}', end = '')\n",
    "\n",
    "    try:\n",
    "        bin_training_data_df = tf_idf_score_text_fragments(bin_training_data_df, tfidf_luts)\n",
    "        bin_testing_data_df = tf_idf_score_text_fragments(bin_testing_data_df, tfidf_luts)\n",
    "\n",
    "    except Exception as err_string:\n",
    "        print(f'\\nWorker {worker_num} - tf_idf_score_text_fragments() error: {err_string}', end = '')\n",
    "\n",
    "    return bin_id, bin_training_data_df, bin_testing_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Bring it all together\n",
    "Now, let's parallelize the calculation over the length bins in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_score(\n",
    "        hdf5_file: str,\n",
    "        score_sample: bool = False,\n",
    ") -> None:\n",
    "\n",
    "    '''Main function to parallelize computation of TF-IDF score\n",
    "    over length bins.'''\n",
    "\n",
    "    # Get the bins from the hdf5 file's metadata\n",
    "    data_lake = h5py.File(hdf5_file, 'r')\n",
    "    bins = dict(data_lake.attrs.items())\n",
    "    data_lake.close()\n",
    "\n",
    "    # Calculate worker number whichever is less, the number of avalible\n",
    "    # CPU or the humber of bins\n",
    "    n_workers = min(20, len(list(bins.keys())))\n",
    "\n",
    "    # Instantiate worker pool\n",
    "    pool = mp.Pool(\n",
    "        processes = n_workers,\n",
    "        maxtasksperchild = 1\n",
    "    )\n",
    "\n",
    "    # Holder for returns from workers\n",
    "    async_results = []\n",
    "\n",
    "    # Open a connection to the hdf5 dataset via PyTables with Pandas\n",
    "    data_lake = pd.HDFStore(hdf5_file)\n",
    "\n",
    "    # Loop on the bins\n",
    "    for worker_num, bin_id in enumerate(bins.keys()):\n",
    "\n",
    "        # Pull the training features for this bin\n",
    "        bin_training_features_df = data_lake[f'training/{bin_id}/features']\n",
    "        print(f'\\nWorker {worker_num} - {len(bin_training_features_df)} fragments in {bin_id}', end = '')\n",
    "\n",
    "        # Pull the testing features for this bin\n",
    "        bin_testing_features_df = data_lake[f'testing/{bin_id}/features']\n",
    "\n",
    "        # Take sample if desired\n",
    "        if score_sample is True:\n",
    "            bin_training_features_df = bin_training_features_df.sample(frac = 0.1)\n",
    "            bin_testing_features_df = bin_testing_features_df.sample(frac = 0.1)\n",
    "\n",
    "        async_results.append(\n",
    "            pool.apply_async(add_tf_idf_score,\n",
    "                args = (\n",
    "                    bin_training_features_df,\n",
    "                    bin_testing_features_df,\n",
    "                    worker_num,\n",
    "                    bin_id\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Clean up\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    ##### Collect and save the results #########################################\n",
    "\n",
    "    # Get the results\n",
    "    new_results = [async_result.get() for async_result in async_results]\n",
    "\n",
    "    # Add the new results\n",
    "    for new_result in new_results:\n",
    "\n",
    "        # Parse the result\n",
    "        bin_id = new_result[0]\n",
    "        training_features_df = new_result[1]\n",
    "        testing_features_df = new_result[2]\n",
    "\n",
    "        # Print info for sanity check\n",
    "        print(f'\\n\\n{bin_id} training features:\\n')\n",
    "        training_features_df.info()\n",
    "\n",
    "        # Put data back into hdf5\n",
    "        data_lake.put(f'training/{bin_id}/features', training_features_df)\n",
    "        data_lake.put(f'testing/{bin_id}/features', testing_features_df)\n",
    "\n",
    "    data_lake.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Worker 0 - 8532 fragments in bin_100\n",
      "Worker 1 - 8081 fragments in bin_150\n",
      "Worker 2 - 6995 fragments in bin_200\n",
      "Worker 3 - 5999 fragments in bin_250\n",
      "Worker 4 - 5193 fragments in bin_300\n",
      "Worker 5 - 4169 fragments in bin_350\n",
      "Worker 6 - 2571 fragments in bin_400\n",
      "Worker 7 - 1146 fragments in bin_450\n",
      "Worker 8 - 435 fragments in bin_500\n",
      "Worker 9 - 325 fragments in bin_600\n",
      "Cleaned text is: <class 'pandas.core.series.Series'>\n",
      "Worker 10 - 23937 fragments in combined\n",
      "Cleaned text is: <class 'pandas.core.series.Series'>\n",
      "TF-IDF look-up table is: <class 'dict'>\n",
      "Cleaned text is: <class 'pandas.core.series.Series'>\n",
      "TF-IDF look-up table is: <class 'dict'>\n",
      "Cleaned text is: <class 'pandas.core.series.Series'>\n",
      "TF-IDF look-up table is: <class 'dict'>\n",
      "Cleaned text is: <class 'pandas.core.series.Series'>\n",
      "Cleaned text is: <class 'pandas.core.series.Series'>\n",
      "Cleaned text is: <class 'pandas.core.series.Series'>\n",
      "Cleaned text is: <class 'pandas.core.series.Series'>\n",
      "TF-IDF look-up table is: <class 'dict'>\n",
      "TF-IDF look-up table is: <class 'dict'>\n",
      "TF-IDF look-up table is: <class 'dict'>\n",
      "TF-IDF look-up table is: <class 'dict'>\n",
      "TF-IDF look-up table is: <class 'dict'>\n",
      "Cleaned text is: <class 'pandas.core.series.Series'>\n",
      "Cleaned text is: <class 'pandas.core.series.Series'>\n",
      "TF-IDF look-up table is: <class 'dict'>\n",
      "TF-IDF look-up table is: <class 'dict'>\n",
      "Cleaned text is: <class 'pandas.core.series.Series'>\n",
      "TF-IDF look-up table is: <class 'dict'>\n",
      "\n",
      "bin_100 training features:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 853 entries, 6223 to 2451\n",
      "Data columns (total 10 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Fragment length (words)   853 non-null    int64  \n",
      " 1   Fragment length (tokens)  853 non-null    int64  \n",
      " 2   Source                    853 non-null    object \n",
      " 3   String                    853 non-null    object \n",
      " 4   Perplexity                853 non-null    float64\n",
      " 5   Cross-perplexity          853 non-null    float64\n",
      " 6   Perplexity ratio score    853 non-null    float64\n",
      " 7   Human TF-IDF              853 non-null    float64\n",
      " 8   Synthetic TF-IDF          853 non-null    float64\n",
      " 9   TF-IDF score              853 non-null    float64\n",
      "dtypes: float64(6), int64(2), object(2)\n",
      "memory usage: 73.3+ KB\n",
      "\n",
      "\n",
      "bin_150 training features:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 808 entries, 5034 to 7109\n",
      "Data columns (total 10 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Fragment length (words)   808 non-null    int64  \n",
      " 1   Fragment length (tokens)  808 non-null    int64  \n",
      " 2   Source                    808 non-null    object \n",
      " 3   String                    808 non-null    object \n",
      " 4   Perplexity                808 non-null    float64\n",
      " 5   Cross-perplexity          808 non-null    float64\n",
      " 6   Perplexity ratio score    808 non-null    float64\n",
      " 7   Human TF-IDF              808 non-null    float64\n",
      " 8   Synthetic TF-IDF          808 non-null    float64\n",
      " 9   TF-IDF score              808 non-null    float64\n",
      "dtypes: float64(6), int64(2), object(2)\n",
      "memory usage: 69.4+ KB\n",
      "\n",
      "\n",
      "bin_200 training features:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 700 entries, 6568 to 3921\n",
      "Data columns (total 10 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Fragment length (words)   700 non-null    int64  \n",
      " 1   Fragment length (tokens)  700 non-null    int64  \n",
      " 2   Source                    700 non-null    object \n",
      " 3   String                    700 non-null    object \n",
      " 4   Perplexity                700 non-null    float64\n",
      " 5   Cross-perplexity          700 non-null    float64\n",
      " 6   Perplexity ratio score    700 non-null    float64\n",
      " 7   Human TF-IDF              700 non-null    float64\n",
      " 8   Synthetic TF-IDF          700 non-null    float64\n",
      " 9   TF-IDF score              700 non-null    float64\n",
      "dtypes: float64(6), int64(2), object(2)\n",
      "memory usage: 60.2+ KB\n",
      "\n",
      "\n",
      "bin_250 training features:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 600 entries, 4090 to 3864\n",
      "Data columns (total 10 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Fragment length (words)   600 non-null    int64  \n",
      " 1   Fragment length (tokens)  600 non-null    int64  \n",
      " 2   Source                    600 non-null    object \n",
      " 3   String                    600 non-null    object \n",
      " 4   Perplexity                600 non-null    float64\n",
      " 5   Cross-perplexity          600 non-null    float64\n",
      " 6   Perplexity ratio score    600 non-null    float64\n",
      " 7   Human TF-IDF              600 non-null    float64\n",
      " 8   Synthetic TF-IDF          600 non-null    float64\n",
      " 9   TF-IDF score              600 non-null    float64\n",
      "dtypes: float64(6), int64(2), object(2)\n",
      "memory usage: 51.6+ KB\n",
      "\n",
      "\n",
      "bin_300 training features:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 519 entries, 1454 to 2169\n",
      "Data columns (total 10 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Fragment length (words)   519 non-null    int64  \n",
      " 1   Fragment length (tokens)  519 non-null    int64  \n",
      " 2   Source                    519 non-null    object \n",
      " 3   String                    519 non-null    object \n",
      " 4   Perplexity                519 non-null    float64\n",
      " 5   Cross-perplexity          519 non-null    float64\n",
      " 6   Perplexity ratio score    519 non-null    float64\n",
      " 7   Human TF-IDF              519 non-null    float64\n",
      " 8   Synthetic TF-IDF          519 non-null    float64\n",
      " 9   TF-IDF score              519 non-null    float64\n",
      "dtypes: float64(6), int64(2), object(2)\n",
      "memory usage: 44.6+ KB\n",
      "\n",
      "\n",
      "bin_350 training features:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 417 entries, 1317 to 1824\n",
      "Data columns (total 10 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Fragment length (words)   417 non-null    int64  \n",
      " 1   Fragment length (tokens)  417 non-null    int64  \n",
      " 2   Source                    417 non-null    object \n",
      " 3   String                    417 non-null    object \n",
      " 4   Perplexity                417 non-null    float64\n",
      " 5   Cross-perplexity          417 non-null    float64\n",
      " 6   Perplexity ratio score    417 non-null    float64\n",
      " 7   Human TF-IDF              417 non-null    float64\n",
      " 8   Synthetic TF-IDF          417 non-null    float64\n",
      " 9   TF-IDF score              417 non-null    float64\n",
      "dtypes: float64(6), int64(2), object(2)\n",
      "memory usage: 35.8+ KB\n",
      "\n",
      "\n",
      "bin_400 training features:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 257 entries, 1523 to 505\n",
      "Data columns (total 10 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Fragment length (words)   257 non-null    int64  \n",
      " 1   Fragment length (tokens)  257 non-null    int64  \n",
      " 2   Source                    257 non-null    object \n",
      " 3   String                    257 non-null    object \n",
      " 4   Perplexity                257 non-null    float64\n",
      " 5   Cross-perplexity          257 non-null    float64\n",
      " 6   Perplexity ratio score    257 non-null    float64\n",
      " 7   Human TF-IDF              257 non-null    float64\n",
      " 8   Synthetic TF-IDF          257 non-null    float64\n",
      " 9   TF-IDF score              257 non-null    float64\n",
      "dtypes: float64(6), int64(2), object(2)\n",
      "memory usage: 22.1+ KB\n",
      "\n",
      "\n",
      "bin_450 training features:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 115 entries, 910 to 716\n",
      "Data columns (total 10 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Fragment length (words)   115 non-null    int64  \n",
      " 1   Fragment length (tokens)  115 non-null    int64  \n",
      " 2   Source                    115 non-null    object \n",
      " 3   String                    115 non-null    object \n",
      " 4   Perplexity                115 non-null    float64\n",
      " 5   Cross-perplexity          115 non-null    float64\n",
      " 6   Perplexity ratio score    115 non-null    float64\n",
      " 7   Human TF-IDF              115 non-null    float64\n",
      " 8   Synthetic TF-IDF          115 non-null    float64\n",
      " 9   TF-IDF score              115 non-null    float64\n",
      "dtypes: float64(6), int64(2), object(2)\n",
      "memory usage: 9.9+ KB\n",
      "\n",
      "\n",
      "bin_500 training features:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 44 entries, 322 to 301\n",
      "Data columns (total 10 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Fragment length (words)   44 non-null     int64  \n",
      " 1   Fragment length (tokens)  44 non-null     int64  \n",
      " 2   Source                    44 non-null     object \n",
      " 3   String                    44 non-null     object \n",
      " 4   Perplexity                44 non-null     float64\n",
      " 5   Cross-perplexity          44 non-null     float64\n",
      " 6   Perplexity ratio score    44 non-null     float64\n",
      " 7   Human TF-IDF              44 non-null     float64\n",
      " 8   Synthetic TF-IDF          44 non-null     float64\n",
      " 9   TF-IDF score              44 non-null     float64\n",
      "dtypes: float64(6), int64(2), object(2)\n",
      "memory usage: 3.8+ KB\n",
      "\n",
      "\n",
      "bin_600 training features:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 32 entries, 171 to 72\n",
      "Data columns (total 10 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Fragment length (words)   32 non-null     int64  \n",
      " 1   Fragment length (tokens)  32 non-null     int64  \n",
      " 2   Source                    32 non-null     object \n",
      " 3   String                    32 non-null     object \n",
      " 4   Perplexity                32 non-null     float64\n",
      " 5   Cross-perplexity          32 non-null     float64\n",
      " 6   Perplexity ratio score    32 non-null     float64\n",
      " 7   Human TF-IDF              32 non-null     float64\n",
      " 8   Synthetic TF-IDF          32 non-null     float64\n",
      " 9   TF-IDF score              32 non-null     float64\n",
      "dtypes: float64(6), int64(2), object(2)\n",
      "memory usage: 2.8+ KB\n",
      "\n",
      "\n",
      "combined training features:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2394 entries, 19567 to 12797\n",
      "Data columns (total 10 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Fragment length (words)   2394 non-null   int64  \n",
      " 1   Fragment length (tokens)  2394 non-null   int64  \n",
      " 2   Source                    2394 non-null   object \n",
      " 3   String                    2394 non-null   object \n",
      " 4   Perplexity                2394 non-null   float64\n",
      " 5   Cross-perplexity          2394 non-null   float64\n",
      " 6   Perplexity ratio score    2394 non-null   float64\n",
      " 7   Human TF-IDF              2394 non-null   float64\n",
      " 8   Synthetic TF-IDF          2394 non-null   float64\n",
      " 9   TF-IDF score              2394 non-null   float64\n",
      "dtypes: float64(6), int64(2), object(2)\n",
      "memory usage: 205.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# The dataset we want to bin - omit the file extension, it will be \n",
    "# added appropriately for the input and output files\n",
    "dataset_name = 'falcon-7b_scores_v2_10-1000_words'\n",
    "\n",
    "# Input file path\n",
    "input_file = f'{config.DATA_PATH}/{dataset_name}.h5'\n",
    "\n",
    "# Option to sample 10% of the data for rapid testing and development\n",
    "sample = False\n",
    "\n",
    "# Run the Kullback-Leibler score calculation on the TF-IDF score\n",
    "tf_idf_score(\n",
    "        hdf5_file = input_file,\n",
    "        score_sample = True # Run on 10% of data for rapid development\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. TF-IDF Kullback-Leibler divergence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Worker 0 - 853 fragments in bin_100\n",
      "Worker 1 - 808 fragments in bin_150\n",
      "Worker 2 - 700 fragments in bin_200\n",
      "Worker 3 - 600 fragments in bin_250\n",
      "Worker 4 - 519 fragments in bin_300\n",
      "Worker 0 - adding Kullback-Leibler score to training features\n",
      "Worker 5 - 417 fragments in bin_350\n",
      "Worker 6 - 257 fragments in bin_400\n",
      "Worker 2 - adding Kullback-Leibler score to training features\n",
      "Worker 7 - 115 fragments in bin_450\n",
      "Worker 8 - 44 fragments in bin_500\n",
      "Worker 9 - 32 fragments in bin_600\n",
      "Worker 1 - adding Kullback-Leibler score to training features\n",
      "Worker 5 - adding Kullback-Leibler score to training features\n",
      "Worker 10 - 2394 fragments in combined\n",
      "Worker 3 - adding Kullback-Leibler score to training features\n",
      "Worker 6 - adding Kullback-Leibler score to training features\n",
      "Worker 4 - adding Kullback-Leibler score to training features\n",
      "Worker 7 - adding Kullback-Leibler score to training features\n",
      "Worker 8 - adding Kullback-Leibler score to training features\n",
      "Worker 9 - adding Kullback-Leibler score to training features\n",
      "Worker 10 - adding Kullback-Leibler score to training features\n",
      "Worker 8 - adding Kullback-Leibler score to testing features\n",
      "Worker 9 - adding Kullback-Leibler score to testing features\n",
      "Worker 7 - adding Kullback-Leibler score to testing features\n",
      "Worker 6 - adding Kullback-Leibler score to testing features\n",
      "Worker 5 - adding Kullback-Leibler score to testing features\n",
      "Worker 0 - adding Kullback-Leibler score to testing features\n",
      "Worker 2 - adding Kullback-Leibler score to testing features\n",
      "Worker 4 - adding Kullback-Leibler score to testing features\n",
      "Worker 3 - adding Kullback-Leibler score to testing features\n",
      "Worker 1 - adding Kullback-Leibler score to testing features\n",
      "Worker 10 - adding Kullback-Leibler score to testing features\n",
      "\n",
      "bin_100 training features:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 853 entries, 6223 to 2451\n",
      "Data columns (total 11 columns):\n",
      " #   Column                                    Non-Null Count  Dtype  \n",
      "---  ------                                    --------------  -----  \n",
      " 0   Fragment length (words)                   853 non-null    int64  \n",
      " 1   Fragment length (tokens)                  853 non-null    int64  \n",
      " 2   Source                                    853 non-null    object \n",
      " 3   String                                    853 non-null    object \n",
      " 4   Perplexity                                853 non-null    float64\n",
      " 5   Cross-perplexity                          853 non-null    float64\n",
      " 6   Perplexity ratio score                    853 non-null    float64\n",
      " 7   Human TF-IDF                              853 non-null    float64\n",
      " 8   Synthetic TF-IDF                          853 non-null    float64\n",
      " 9   TF-IDF score                              853 non-null    float64\n",
      " 10  TF-IDF score Kullback-Leibler divergence  853 non-null    float64\n",
      "dtypes: float64(7), int64(2), object(2)\n",
      "memory usage: 80.0+ KB\n",
      "\n",
      "\n",
      "bin_150 training features:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 808 entries, 5034 to 7109\n",
      "Data columns (total 11 columns):\n",
      " #   Column                                    Non-Null Count  Dtype  \n",
      "---  ------                                    --------------  -----  \n",
      " 0   Fragment length (words)                   808 non-null    int64  \n",
      " 1   Fragment length (tokens)                  808 non-null    int64  \n",
      " 2   Source                                    808 non-null    object \n",
      " 3   String                                    808 non-null    object \n",
      " 4   Perplexity                                808 non-null    float64\n",
      " 5   Cross-perplexity                          808 non-null    float64\n",
      " 6   Perplexity ratio score                    808 non-null    float64\n",
      " 7   Human TF-IDF                              808 non-null    float64\n",
      " 8   Synthetic TF-IDF                          808 non-null    float64\n",
      " 9   TF-IDF score                              808 non-null    float64\n",
      " 10  TF-IDF score Kullback-Leibler divergence  808 non-null    float64\n",
      "dtypes: float64(7), int64(2), object(2)\n",
      "memory usage: 75.8+ KB\n",
      "\n",
      "\n",
      "bin_200 training features:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 700 entries, 6568 to 3921\n",
      "Data columns (total 11 columns):\n",
      " #   Column                                    Non-Null Count  Dtype  \n",
      "---  ------                                    --------------  -----  \n",
      " 0   Fragment length (words)                   700 non-null    int64  \n",
      " 1   Fragment length (tokens)                  700 non-null    int64  \n",
      " 2   Source                                    700 non-null    object \n",
      " 3   String                                    700 non-null    object \n",
      " 4   Perplexity                                700 non-null    float64\n",
      " 5   Cross-perplexity                          700 non-null    float64\n",
      " 6   Perplexity ratio score                    700 non-null    float64\n",
      " 7   Human TF-IDF                              700 non-null    float64\n",
      " 8   Synthetic TF-IDF                          700 non-null    float64\n",
      " 9   TF-IDF score                              700 non-null    float64\n",
      " 10  TF-IDF score Kullback-Leibler divergence  700 non-null    float64\n",
      "dtypes: float64(7), int64(2), object(2)\n",
      "memory usage: 65.6+ KB\n",
      "\n",
      "\n",
      "bin_250 training features:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 600 entries, 4090 to 3864\n",
      "Data columns (total 11 columns):\n",
      " #   Column                                    Non-Null Count  Dtype  \n",
      "---  ------                                    --------------  -----  \n",
      " 0   Fragment length (words)                   600 non-null    int64  \n",
      " 1   Fragment length (tokens)                  600 non-null    int64  \n",
      " 2   Source                                    600 non-null    object \n",
      " 3   String                                    600 non-null    object \n",
      " 4   Perplexity                                600 non-null    float64\n",
      " 5   Cross-perplexity                          600 non-null    float64\n",
      " 6   Perplexity ratio score                    600 non-null    float64\n",
      " 7   Human TF-IDF                              600 non-null    float64\n",
      " 8   Synthetic TF-IDF                          600 non-null    float64\n",
      " 9   TF-IDF score                              600 non-null    float64\n",
      " 10  TF-IDF score Kullback-Leibler divergence  600 non-null    float64\n",
      "dtypes: float64(7), int64(2), object(2)\n",
      "memory usage: 56.2+ KB\n",
      "\n",
      "\n",
      "bin_300 training features:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 519 entries, 1454 to 2169\n",
      "Data columns (total 11 columns):\n",
      " #   Column                                    Non-Null Count  Dtype  \n",
      "---  ------                                    --------------  -----  \n",
      " 0   Fragment length (words)                   519 non-null    int64  \n",
      " 1   Fragment length (tokens)                  519 non-null    int64  \n",
      " 2   Source                                    519 non-null    object \n",
      " 3   String                                    519 non-null    object \n",
      " 4   Perplexity                                519 non-null    float64\n",
      " 5   Cross-perplexity                          519 non-null    float64\n",
      " 6   Perplexity ratio score                    519 non-null    float64\n",
      " 7   Human TF-IDF                              519 non-null    float64\n",
      " 8   Synthetic TF-IDF                          519 non-null    float64\n",
      " 9   TF-IDF score                              519 non-null    float64\n",
      " 10  TF-IDF score Kullback-Leibler divergence  519 non-null    float64\n",
      "dtypes: float64(7), int64(2), object(2)\n",
      "memory usage: 48.7+ KB\n",
      "\n",
      "\n",
      "bin_350 training features:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 417 entries, 1317 to 1824\n",
      "Data columns (total 11 columns):\n",
      " #   Column                                    Non-Null Count  Dtype  \n",
      "---  ------                                    --------------  -----  \n",
      " 0   Fragment length (words)                   417 non-null    int64  \n",
      " 1   Fragment length (tokens)                  417 non-null    int64  \n",
      " 2   Source                                    417 non-null    object \n",
      " 3   String                                    417 non-null    object \n",
      " 4   Perplexity                                417 non-null    float64\n",
      " 5   Cross-perplexity                          417 non-null    float64\n",
      " 6   Perplexity ratio score                    417 non-null    float64\n",
      " 7   Human TF-IDF                              417 non-null    float64\n",
      " 8   Synthetic TF-IDF                          417 non-null    float64\n",
      " 9   TF-IDF score                              417 non-null    float64\n",
      " 10  TF-IDF score Kullback-Leibler divergence  417 non-null    float64\n",
      "dtypes: float64(7), int64(2), object(2)\n",
      "memory usage: 39.1+ KB\n",
      "\n",
      "\n",
      "bin_400 training features:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 257 entries, 1523 to 505\n",
      "Data columns (total 11 columns):\n",
      " #   Column                                    Non-Null Count  Dtype  \n",
      "---  ------                                    --------------  -----  \n",
      " 0   Fragment length (words)                   257 non-null    int64  \n",
      " 1   Fragment length (tokens)                  257 non-null    int64  \n",
      " 2   Source                                    257 non-null    object \n",
      " 3   String                                    257 non-null    object \n",
      " 4   Perplexity                                257 non-null    float64\n",
      " 5   Cross-perplexity                          257 non-null    float64\n",
      " 6   Perplexity ratio score                    257 non-null    float64\n",
      " 7   Human TF-IDF                              257 non-null    float64\n",
      " 8   Synthetic TF-IDF                          257 non-null    float64\n",
      " 9   TF-IDF score                              257 non-null    float64\n",
      " 10  TF-IDF score Kullback-Leibler divergence  257 non-null    float64\n",
      "dtypes: float64(7), int64(2), object(2)\n",
      "memory usage: 24.1+ KB\n",
      "\n",
      "\n",
      "bin_450 training features:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 115 entries, 910 to 716\n",
      "Data columns (total 11 columns):\n",
      " #   Column                                    Non-Null Count  Dtype  \n",
      "---  ------                                    --------------  -----  \n",
      " 0   Fragment length (words)                   115 non-null    int64  \n",
      " 1   Fragment length (tokens)                  115 non-null    int64  \n",
      " 2   Source                                    115 non-null    object \n",
      " 3   String                                    115 non-null    object \n",
      " 4   Perplexity                                115 non-null    float64\n",
      " 5   Cross-perplexity                          115 non-null    float64\n",
      " 6   Perplexity ratio score                    115 non-null    float64\n",
      " 7   Human TF-IDF                              115 non-null    float64\n",
      " 8   Synthetic TF-IDF                          115 non-null    float64\n",
      " 9   TF-IDF score                              115 non-null    float64\n",
      " 10  TF-IDF score Kullback-Leibler divergence  115 non-null    float64\n",
      "dtypes: float64(7), int64(2), object(2)\n",
      "memory usage: 10.8+ KB\n",
      "\n",
      "\n",
      "bin_500 training features:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 44 entries, 322 to 301\n",
      "Data columns (total 11 columns):\n",
      " #   Column                                    Non-Null Count  Dtype  \n",
      "---  ------                                    --------------  -----  \n",
      " 0   Fragment length (words)                   44 non-null     int64  \n",
      " 1   Fragment length (tokens)                  44 non-null     int64  \n",
      " 2   Source                                    44 non-null     object \n",
      " 3   String                                    44 non-null     object \n",
      " 4   Perplexity                                44 non-null     float64\n",
      " 5   Cross-perplexity                          44 non-null     float64\n",
      " 6   Perplexity ratio score                    44 non-null     float64\n",
      " 7   Human TF-IDF                              44 non-null     float64\n",
      " 8   Synthetic TF-IDF                          44 non-null     float64\n",
      " 9   TF-IDF score                              44 non-null     float64\n",
      " 10  TF-IDF score Kullback-Leibler divergence  44 non-null     float64\n",
      "dtypes: float64(7), int64(2), object(2)\n",
      "memory usage: 4.1+ KB\n",
      "\n",
      "\n",
      "bin_600 training features:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 32 entries, 171 to 72\n",
      "Data columns (total 11 columns):\n",
      " #   Column                                    Non-Null Count  Dtype  \n",
      "---  ------                                    --------------  -----  \n",
      " 0   Fragment length (words)                   32 non-null     int64  \n",
      " 1   Fragment length (tokens)                  32 non-null     int64  \n",
      " 2   Source                                    32 non-null     object \n",
      " 3   String                                    32 non-null     object \n",
      " 4   Perplexity                                32 non-null     float64\n",
      " 5   Cross-perplexity                          32 non-null     float64\n",
      " 6   Perplexity ratio score                    32 non-null     float64\n",
      " 7   Human TF-IDF                              32 non-null     float64\n",
      " 8   Synthetic TF-IDF                          32 non-null     float64\n",
      " 9   TF-IDF score                              32 non-null     float64\n",
      " 10  TF-IDF score Kullback-Leibler divergence  32 non-null     float64\n",
      "dtypes: float64(7), int64(2), object(2)\n",
      "memory usage: 3.0+ KB\n",
      "\n",
      "\n",
      "combined training features:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2394 entries, 19567 to 12797\n",
      "Data columns (total 11 columns):\n",
      " #   Column                                    Non-Null Count  Dtype  \n",
      "---  ------                                    --------------  -----  \n",
      " 0   Fragment length (words)                   2394 non-null   int64  \n",
      " 1   Fragment length (tokens)                  2394 non-null   int64  \n",
      " 2   Source                                    2394 non-null   object \n",
      " 3   String                                    2394 non-null   object \n",
      " 4   Perplexity                                2394 non-null   float64\n",
      " 5   Cross-perplexity                          2394 non-null   float64\n",
      " 6   Perplexity ratio score                    2394 non-null   float64\n",
      " 7   Human TF-IDF                              2394 non-null   float64\n",
      " 8   Synthetic TF-IDF                          2394 non-null   float64\n",
      " 9   TF-IDF score                              2394 non-null   float64\n",
      " 10  TF-IDF score Kullback-Leibler divergence  2394 non-null   float64\n",
      "dtypes: float64(7), int64(2), object(2)\n",
      "memory usage: 224.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Run the Kullback-Leibler score calculation on the TF-IDF score\n",
    "kld_funcs.kullback_leibler_score(\n",
    "        feature_name = 'TF-IDF score',\n",
    "        hdf5_file = input_file,\n",
    "        padding = 0.1,\n",
    "        sample_frequency = 0.001,\n",
    "        score_sample = sample\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
