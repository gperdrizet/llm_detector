{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity ratio score: Kullbackâ€“Leibler divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/arkk/llm_detector/classifier\n"
     ]
    }
   ],
   "source": [
    "# Change working directory to parent so we can import as we would from main.py\n",
    "%cd ..\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "\n",
    "import configuration as config\n",
    "import functions.kullback_leibler_divergence as kld_funcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan here is to take our sampling distributions of perplexity ratio (PR) scores for human and synthetic text and use them to generate a function that takes a perplexity ratio score and converts it into a Kullback-Leibler divergence (KLD) score. See the figure below from the [Wikipedia article on KLD](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence).\n",
    "\n",
    "Workflow is as follows:\n",
    "1. Get kernel density estimate of PR score distribution for human and synthetic text fragments in training data.\n",
    "2. Calculated KLD between the human and synthetic PR score distributions.\n",
    "3. Get get kernel density estimate of KLD.\n",
    "4. Use probability density function of KLD kernel density estimate to calculate KLD score for each text fragment in the training and testing data.\n",
    "5. Add the KLD score as a new feature.\n",
    "\n",
    "The above will be done individually for each fragment length bin and the combined data. This way the KLD score feature in each bin will capture the PR score distribution for text fragments in that specific length regime, rather that for the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/gperdrizet/llm_detector/benchmarking/benchmarking/notebooks/images/KL-Gauss-Example.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url = 'https://raw.githubusercontent.com/gperdrizet/llm_detector/benchmarking/benchmarking/notebooks/images/KL-Gauss-Example.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Worker 0 - 8423 fragments in bin_100\n",
      "Worker 1 - 7968 fragments in bin_150\n",
      "Worker 2 - 7000 fragments in bin_200\n",
      "Worker 3 - 6095 fragments in bin_250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/arkk/llm_detector/classifier/functions/kullback_leibler_divergence.py:189: RuntimeWarning: overflow encountered in scalar divide\n",
      "  kld_value = i * log2(i/j)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Worker 4 - 5283 fragments in bin_300\n",
      "Worker 5 - 4193 fragments in bin_350\n",
      "Worker 6 - 2582 fragments in bin_400\n",
      "Worker 7 - 1162 fragments in bin_450\n",
      "Worker 8 - 441 fragments in bin_500\n",
      "Worker 9 - 327 fragments in bin_600\n",
      "Worker 0 - adding Kullback-Leibler score to training features\n",
      "Worker 10 - 23937 fragments in combined\n",
      "Worker 1 - adding Kullback-Leibler score to training features\n",
      "Worker 2 - adding Kullback-Leibler score to training features\n",
      "Worker 3 - adding Kullback-Leibler score to training features\n",
      "Worker 4 - adding Kullback-Leibler score to training features\n",
      "Worker 5 - adding Kullback-Leibler score to training features\n",
      "Worker 6 - adding Kullback-Leibler score to training features\n",
      "Worker 7 - adding Kullback-Leibler score to training features\n",
      "Worker 9 - adding Kullback-Leibler score to training features\n",
      "Worker 8 - adding Kullback-Leibler score to training features\n",
      "Worker 10 - adding Kullback-Leibler score to training features"
     ]
    }
   ],
   "source": [
    "# Option to sample 10% of the data for rapid testing and development\n",
    "sample = False\n",
    "\n",
    "# The length bins\n",
    "bins = {\n",
    "    'combined': [0, np.inf],\n",
    "    'bin_100': [1, 100],\n",
    "    'bin_150': [51, 150],\n",
    "    'bin_200': [101, 200],\n",
    "    'bin_250': [151, 250],\n",
    "    'bin_300': [201, 300],\n",
    "    'bin_350': [251, 350],\n",
    "    'bin_400': [301, 400],\n",
    "    'bin_450': [351, 450],\n",
    "    'bin_500': [401, 500],\n",
    "    'bin_600': [451, 600]\n",
    "}\n",
    "\n",
    "\n",
    "# Run the Kullback-Leibler score calculation on the\n",
    "# perplexity ratio score\n",
    "\n",
    "kld_funcs.kullback_leibler_score(\n",
    "        feature_name = 'Perplexity ratio score' ,\n",
    "        #bins = bins,\n",
    "        hdf5_file = config.LENGTH_BINNED_DATASET,\n",
    "        padding = 0.1,\n",
    "        sample_frequency = 0.001,\n",
    "        score_sample = sample\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
