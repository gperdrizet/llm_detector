{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalized classifier v2.0\n",
    "\n",
    "Now all that's left to be done is wire up some routing logic between the models so we can easily do inference on incoming text. Workflow will looks something like this:\n",
    "\n",
    "1. **Stage I features**: calculate perplexity ratio and TF-IDF based features for input text.\n",
    "2. **Stage I classifier**: send feature vector to correct stage I classifiers based on text length.\n",
    "3. **Stage II features**: create new feature vector for with stage I class probabilities, perplexity ration and TF-IDF features.\n",
    "4. **Stage II classifier**: send new feature vector to correct stage II classifier for final prediction.\n",
    "\n",
    "Each step requires assets from the feature engineering and classifier training phases. Let's make a checklist to help make sure we have everything in place.\n",
    "\n",
    "1. **Stage I features**:\n",
    "    - Perplexity ratio: tokenizer + reader and writer models.\n",
    "    - Perplexity ratio Kullback-Leibler score: perplexity ratio Kullback-Leibler divergence kernel density estimate for each bin.\n",
    "    - TF-IDF score: human and synthetic TF-IDF look-up tables for each bin.\n",
    "    - TF-IDF Kullback-Leibler score: TF-IDF Kullback-Leibler divergence kernel density estimate for each bin.\n",
    "\n",
    "2. **Stage I classifier**:\n",
    "    - Trained XGBoost classifier model for each bin.\n",
    "\n",
    "3. **Stage II features**\n",
    "    - Perplexity ratio Kullback-Leibler score: perplexity ratio Kullback-Leibler divergence kernel density estimate for each bin.\n",
    "    - TF-IDF score: human and synthetic TF-IDF look-up tables for each bin.\n",
    "    - TF-IDF Kullback-Leibler score: TF-IDF Kullback-Leibler divergence kernel density estimate for each bin.\n",
    "\n",
    "4. **Stage II classifier**\n",
    "    - Trained XGBoost classifier model for each bin.\n",
    "\n",
    "## 1. Run setup\n",
    "\n",
    "The plan is to break the inference pipeline into steps and run a process or process pool for each, using queues to move text through the pipeline. The major goal of this notebook will be to tune the pipeline in terms of resource allocation for each step to maximize the overall inference rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /mnt/arkk/llm_detector/classifier\n"
     ]
    }
   ],
   "source": [
    "# Change working directory to parent so we can import as we would from main.py\n",
    "print(f'Working directory: ', end = '')\n",
    "%cd ..\n",
    "\n",
    "# PyPI imports\n",
    "# import h5py\n",
    "# import pickle\n",
    "# import pandas as pd\n",
    "from multiprocessing import Manager, Process\n",
    "\n",
    "# Internal imports\n",
    "# import configuration as config\n",
    "import functions.notebook_helper as helper_funcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a class for the inference pipeline - it will deal with setting up the queues and worker processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferencePipeline:\n",
    "    '''Holds and manages processes and queues for inference pipeline.'''\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        ##########################################################\n",
    "        # Set-up queues to move text though through the pipeline #\n",
    "        ##########################################################\n",
    "\n",
    "        # Star the multiprocessing manager\n",
    "        self.manager=Manager()\n",
    "\n",
    "        # Accepts input text and takes it to the LLM worker(s) for \n",
    "        # perplexity ratio calculation\n",
    "        self.input_queue=self.manager.Queue(maxsize=10)\n",
    "\n",
    "        # Takes text from LLM workers to the first stage classifier\n",
    "        self.stage_one_classifier_queue=self.manager.Queue(maxsize=10)\n",
    "\n",
    "        # Takes text from the first stage classifier to the \n",
    "        # second stage classifier\n",
    "        self.stage_two_classifier_queue=self.manager.Queue(maxsize=10)\n",
    "\n",
    "        # Returns completed work from the second stage classifier\n",
    "        self.output_queue=self.manager.Queue(maxsize=10)\n",
    "\n",
    "        ##########################################################\n",
    "        # Set-up a process for each step in the pipeline #########\n",
    "        ##########################################################\n",
    "\n",
    "        # Ingests text and calculate the perplexity ratio\n",
    "        self.perplexity_ratio_process=Process(\n",
    "            target=helper_funcs.get_perplexity_ratio,\n",
    "            args=(\n",
    "                self.input_queue,\n",
    "                self.stage_one_classifier_queue\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Does the feature engineering and classification for stage I\n",
    "        self.stage_one_classifier_process=Process(\n",
    "            target=helper_funcs.stage_one_classifier,\n",
    "            args=(\n",
    "                self.stage_one_classifier_queue,\n",
    "                self.stage_two_classifier_queue\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Does the feature engineering and classification for stage II\n",
    "        self.stage_two_classifier_process=Process(\n",
    "            target=helper_funcs.stage_two_classifier,\n",
    "            args=(\n",
    "                self.stage_two_classifier_queue,\n",
    "                self.output_queue\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    def start(self):\n",
    "        '''Starts the pipeline processes.'''\n",
    "\n",
    "        # Start LLM worker processes for perplexity ratio scoring\n",
    "        # and wait for 'ready' response before moving on.\n",
    "        self.perplexity_ratio_process.start()\n",
    "        response=self.stage_one_classifier_queue.get()\n",
    "        if response == 'ready':\n",
    "            print(f'Perplexity scoring process reports ready.\\n')\n",
    "\n",
    "        # Start stage I classifier process\n",
    "        # and wait for 'ready' response before moving on.\n",
    "        self.stage_one_classifier_process.start()\n",
    "        response=self.stage_two_classifier_queue.get()\n",
    "        if response == 'ready':\n",
    "            print(f'Stage I classifier process reports ready.\\n')\n",
    "\n",
    "        # Start stage II classifier process\n",
    "        # and wait for 'ready' response before moving on.\n",
    "        self.stage_two_classifier_process.start()\n",
    "        response=self.output_queue.get()\n",
    "        if response == 'ready':\n",
    "            print(f'Stage II classifier process reports ready.')\n",
    "\n",
    "\n",
    "    def stop(self):\n",
    "        '''Stops pipeline processes and shuts down.'''\n",
    "\n",
    "        # Send the 'done' signal to the input queue\n",
    "        self.input_queue.put('done')\n",
    "\n",
    "        # Join and then close each process\n",
    "        self.perplexity_ratio_process.join()\n",
    "        self.perplexity_ratio_process.close()\n",
    "\n",
    "        self.stage_one_classifier_process.join()\n",
    "        self.stage_one_classifier_process.close()\n",
    "\n",
    "        self.stage_two_classifier_process.join()\n",
    "        self.stage_two_classifier_process.close()\n",
    "\n",
    "        # Close the queues and stop the manager\n",
    "        self.manager.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing perplexity ratio scoring process.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5429264f148c4e4290241704a356ca42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66223d10e53f425db70577c1db9fe8f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity scoring process reports ready.\n",
      "\n",
      "Initializing stage I classifier process.\n",
      "Stage I classifier process reports ready.\n",
      "\n",
      "Initializing stage II classifier process.\n",
      "Stage II classifier process reports ready.\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline class instance and start the worker processes\n",
    "inference_pipeline=InferencePipeline()\n",
    "inference_pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text string: Documentation says you can equip node pools with GPU(s). A node pool is a g...\n",
      "Text length (words): 121\n",
      "Text length (tokens): 153\n",
      "Perplexity: 2.7805159091949463\n",
      "Cross-perplexity: 3.1987712383270264\n",
      "Perplexity ratio: 0.8692449927330017\n",
      "Stage I bin 0: bin_076_125\n",
      "Stage I bin 1: bin_101_150\n",
      "Stage I bin 0 perplexity ratio KLD score: 3.641113890300743\n",
      "Stage I bin 1 perplexity ratio KLD score: 5.0536270382667015\n",
      "Stage I bin 0 human TF-IDF mean: -2.3889493633094543\n",
      "Stage I bin 0 synthetic TF-IDF mean: -2.46329002605486\n",
      "Stage I bin 0 TF-IDF score: -0.36071869200470486\n",
      "Stage I bin 1 human TF-IDF mean: -2.6783157014974925\n",
      "Stage I bin 1 synthetic TF-IDF mean: -2.4304253393132176\n",
      "Stage I bin 1 TF-IDF score: 1.2664076669122366\n",
      "Stage I bin 0 TF-IDF KLD score: 0.22168183217364798\n",
      "Stage I bin 1 TF-IDF KLD score: 0.018374902510728517\n",
      "Stage I class probability bin 0: 0.026808738708496094\n",
      "Stage I class probability bin 1: 0.9809808731079102\n",
      "Stage II perplexity ratio KLD score: 4.7426710168839765\n",
      "Stage II human TF-IDF mean: -2.4092434814344994\n",
      "Stage II synthetic TF-IDF mean: -2.2820518638631633\n",
      "Stage II TF-IDF score: 0.5966934434732896\n",
      "Stage II TF-IDF KLD score: 0.004116118348069481\n",
      "Stage II class probability: 0.5382013320922852\n"
     ]
    }
   ],
   "source": [
    "# Send some test text though and take a look at the results\n",
    "test_text='''Documentation says you can equip node pools with GPU(s). A node pool is a group of nodes that share the same configuration. A node is an individual machine that runs containerized applications. So right now, we are running 3 'containers' (the telegram bot, the classification API and redis) on one 'node' in a 'node pool' with one member: pyrite. The only thing I am having trouble wrapping my mind around is how this scales past one node. I.e., if we go from one node to two, we have two redis servers, two classification APIs and two bots. I don't think that's what we want , but I'm not sure. I can think of two possibilities, each of which imply further questions.'''\n",
    "inference_pipeline.input_queue.put(test_text)\n",
    "result=inference_pipeline.output_queue.get()\n",
    "\n",
    "for key, value in result.items():\n",
    "    if key == 'Text string':\n",
    "        print(f'{key}: {value[:75]}...')\n",
    "\n",
    "    else:\n",
    "        print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the stage I training data and take just the text and labels. We will be treating each fragment as if it we submitted by a user and therefore all we will have is the text string. We will use the labels later to check the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the stage I training data and take just the text and labels\n",
    "\n",
    "# # Stage I dataset\n",
    "# dataset_name='falcon-7b_scores_v2_10-300_words_stage_I'\n",
    "\n",
    "# # Input file path\n",
    "# input_file=f'{config.DATA_PATH}/{dataset_name}.h5'\n",
    "\n",
    "# # Open the new hdf5 file with pandas so we can work with dataframes\n",
    "# data_lake=pd.HDFStore(input_file)\n",
    "\n",
    "# # Get the features and extract just the text\n",
    "# training_df=data_lake['training/combined/features']\n",
    "# texts=training_df['String'].to_list()\n",
    "# print(f'Have {len(texts)} training text fragments')\n",
    "\n",
    "# # Get the corresponding labels\n",
    "# labels=data_lake['training/combined/labels'].to_list()\n",
    "# print(f'Have {len(labels)} training text fragment labels')\n",
    "\n",
    "# # Close the connection to the hdf5 file\n",
    "# data_lake.close()\n",
    "\n",
    "# training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
