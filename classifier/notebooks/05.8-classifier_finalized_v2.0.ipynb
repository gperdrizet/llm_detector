{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalized classifier v2.0\n",
    "\n",
    "Now all that's left to be done is wire up some routing logic between the models so we can easily do inference on incoming text. Workflow will looks something like this:\n",
    "\n",
    "1. **Stage I features**: calculate perplexity ratio and TF-IDF based features for input text.\n",
    "2. **Stage I classifier**: send feature vector to correct stage I classifiers based on text length.\n",
    "3. **Stage II features**: create new feature vector for with stage I class probabilities, perplexity ration and TF-IDF features.\n",
    "4. **Stage II classifier**: send new feature vector to correct stage II classifier for final prediction.\n",
    "\n",
    "Each step requires assets from the feature engineering and classifier training phases. Let's make a checklist to help make sure we have everything in place.\n",
    "\n",
    "1. **Stage I features**:\n",
    "    - Perplexity ratio: tokenizer + reader and writer models.\n",
    "    - Perplexity ratio Kullback-Leibler score: perplexity ratio Kullback-Leibler divergence kernel density estimate for each bin.\n",
    "    - TF-IDF score: human and synthetic TF-IDF look-up tables for each bin.\n",
    "    - TF-IDF Kullback-Leibler score: TF-IDF Kullback-Leibler divergence kernel density estimate for each bin.\n",
    "\n",
    "2. **Stage I classifier**:\n",
    "    - Trained XGBoost classifier model for each bin.\n",
    "\n",
    "3. **Stage II features**\n",
    "    - Perplexity ratio Kullback-Leibler score: perplexity ratio Kullback-Leibler divergence kernel density estimate for each bin.\n",
    "    - TF-IDF score: human and synthetic TF-IDF look-up tables for each bin.\n",
    "    - TF-IDF Kullback-Leibler score: TF-IDF Kullback-Leibler divergence kernel density estimate for each bin.\n",
    "\n",
    "4. **Stage II classifier**\n",
    "    - Trained XGBoost classifier model for each bin.\n",
    "\n",
    "## 1. Run setup\n",
    "\n",
    "The plan is to break the inference pipeline into steps and run a process or process pool for each, using queues to move text through the pipeline. The major goal of this notebook will be to tune the pipeline in terms of resource allocation for each step to maximize the overall inference rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /mnt/arkk/llm_detector/classifier\n"
     ]
    }
   ],
   "source": [
    "# Change working directory to parent so we can import as we would from main.py\n",
    "print(f'Working directory: ', end = '')\n",
    "%cd ..\n",
    "\n",
    "# PyPI imports\n",
    "import h5py\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Internal imports\n",
    "import configuration as config\n",
    "import functions.notebook_helper as helper_funcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a class for the inference pipeline - it will deal with setting up the queues and worker processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from multiprocessing import Manager, Process\n",
    "\n",
    "class InferencePipeline:\n",
    "    '''Holds and manages processes and queues for inference pipeline.'''\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        ##########################################################\n",
    "        # Set-up queues to move text though through the pipeline #\n",
    "        ##########################################################\n",
    "\n",
    "        # Star the multiprocessing manager\n",
    "        self.manager=Manager()\n",
    "\n",
    "        # Accepts input text and takes it to the LLM worker(s) for \n",
    "        # perplexity ratio calculation\n",
    "        self.input_queue=self.manager.Queue(maxsize=10)\n",
    "\n",
    "        # Takes text from LLM workers to the first stage classifier\n",
    "        self.stage_one_classifier_queue=self.manager.Queue(maxsize=10)\n",
    "\n",
    "        # Takes text from the first stage classifier to the \n",
    "        # second stage classifier\n",
    "        self.stage_two_classifier_queue=self.manager.Queue(maxsize=10)\n",
    "\n",
    "        # Returns completed work from the second stage classifier\n",
    "        self.output_queue=self.manager.Queue(maxsize=10)\n",
    "\n",
    "        ##########################################################\n",
    "        # Set-up a process for each step in the pipeline #########\n",
    "        ##########################################################\n",
    "\n",
    "        # Ingests text and calculate the perplexity ratio\n",
    "        self.perplexity_ratio_process=Process(\n",
    "            target=helper_funcs.get_perplexity_ratio,\n",
    "            args=(\n",
    "                self.input_queue,\n",
    "                self.stage_one_classifier_queue\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Does the feature engineering and classification for stage I\n",
    "        self.stage_one_classifier_process=Process(\n",
    "            target=helper_funcs.stage_one_classifier,\n",
    "            args=(\n",
    "                self.stage_one_classifier_queue,\n",
    "                self.stage_two_classifier_queue\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Does the feature engineering and classification for stage II\n",
    "        self.stage_two_classifier_process=Process(\n",
    "            target=helper_funcs.stage_two_classifier,\n",
    "            args=(\n",
    "                self.stage_two_classifier_queue,\n",
    "                self.output_queue\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    def start(self):\n",
    "        '''Starts the pipeline processes.'''\n",
    "\n",
    "        self.perplexity_ratio_process.start()\n",
    "        self.stage_one_classifier_process.start()\n",
    "        self.stage_two_classifier_process.start()\n",
    "\n",
    "\n",
    "    def stop(self):\n",
    "        '''Stops pipeline processes and shuts down.'''\n",
    "\n",
    "        # Send the 'done' signal to the input queue\n",
    "        self.input_queue.put('done')\n",
    "\n",
    "        # Join and then close each process\n",
    "        self.perplexity_ratio_process.join()\n",
    "        self.perplexity_ratio_process.close()\n",
    "\n",
    "        self.stage_one_classifier_process.join()\n",
    "        self.stage_one_classifier_process.close()\n",
    "\n",
    "        self.stage_two_classifier_process.join()\n",
    "        self.stage_two_classifier_process.close()\n",
    "\n",
    "        # Close the queues and stop the manager\n",
    "        self.manager.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline=InferencePipeline()\n",
    "inference_pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text='''Documentation says you can equip node pools with GPU(s). A node pool is a group of nodes that share the same configuration. A node is an individual machine that runs containerized applications. So right now, we are running 3 'containers' (the telegram bot, the classification API and redis) on one 'node' in a 'node pool' with one member: pyrite. The only thing I am having trouble wrapping my mind around is how this scales past one node. I.e., if we go from one node to two, we have two redis servers, two classification APIs and two bots. I don't think that's what we want , but I'm not sure. I can think of two possibilities, each of which imply further questions.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded reader model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dfa8077fe4a4aea842780cb1bc5e067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded writer model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Text string': \"Documentation says you can equip node pools with GPU(s). A node pool is a group of nodes that share the same configuration. A node is an individual machine that runs containerized applications. So right now, we are running 3 'containers' (the telegram bot, the classification API and redis) on one 'node' in a 'node pool' with one member: pyrite. The only thing I am having trouble wrapping my mind around is how this scales past one node. I.e., if we go from one node to two, we have two redis servers, two classification APIs and two bots. I don't think that's what we want , but I'm not sure. I can think of two possibilities, each of which imply further questions.\", 'Text length (words)': 121, 'Text length (tokens)': 153, 'Perplexity': 2.780516, 'Cross-perplexity': 3.1987712, 'Perplexity ratio': 0.8692449927330017, 'Perplexity ratio Kullback-Leibler score': 5.0536270382667015, 'Human TF-IDF mean': -2.6783157014974925, 'Synthetic TF-IDF mean': -2.4304253393132176, 'TF-IDF score': 1.2664076669122366, 'TF-IDF Kullback-Leibler score': 0.018374902510728517}\n"
     ]
    }
   ],
   "source": [
    "inference_pipeline.input_queue.put(test_text)\n",
    "result=inference_pipeline.output_queue.get()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the stage I training data and take just the text and labels. We will be treating each fragment as if it we submitted by a user and therefore all we will have is the text string. We will use the labels later to check the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the stage I training data and take just the text and labels\n",
    "\n",
    "# # Stage I dataset\n",
    "# dataset_name='falcon-7b_scores_v2_10-300_words_stage_I'\n",
    "\n",
    "# # Input file path\n",
    "# input_file=f'{config.DATA_PATH}/{dataset_name}.h5'\n",
    "\n",
    "# # Open the new hdf5 file with pandas so we can work with dataframes\n",
    "# data_lake=pd.HDFStore(input_file)\n",
    "\n",
    "# # Get the features and extract just the text\n",
    "# training_df=data_lake['training/combined/features']\n",
    "# texts=training_df['String'].to_list()\n",
    "# print(f'Have {len(texts)} training text fragments')\n",
    "\n",
    "# # Get the corresponding labels\n",
    "# labels=data_lake['training/combined/labels'].to_list()\n",
    "# print(f'Have {len(labels)} training text fragment labels')\n",
    "\n",
    "# # Close the connection to the hdf5 file\n",
    "# data_lake.close()\n",
    "\n",
    "# training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
