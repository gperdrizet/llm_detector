{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity ratio score: Kullbackâ€“Leibler divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/arkk/llm_detector/classifier\n"
     ]
    }
   ],
   "source": [
    "# Change working directory to parent so we can import as we would from main.py\n",
    "%cd ..\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "from IPython.display import Image\n",
    "\n",
    "# import functions.notebook_helper as helper_funcs\n",
    "# import functions.notebook_plotting as plot_funcs\n",
    "import configuration as config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan here is to take our sampling distributions of perplexity ratio (PR) scores for human and synthetic text and use them to generate a function that takes a perplexity ratio score and converts it into a Kullback-Leibler divergence (KLD) score. See the figure below from the [Wikipedia article on KLD](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence).\n",
    "\n",
    "Workflow is as follows:\n",
    "1. Get kernel density estimate of PR score distribution for human and synthetic text fragments in training data.\n",
    "2. Calculated KLD between the human and synthetic PR score distributions.\n",
    "3. Get get kernel density estimate of KLD.\n",
    "4. Use probability density function of KLD kernel density estimate to calculate KLD score for each text fragment in the training and testing data.\n",
    "5. Add the KLD score as a new feature.\n",
    "\n",
    "The above will be done individually for each fragment length bin and the combined data. This way the KLD score feature in each bin will capture the PR score distribution for text fragments in that specific length regime, rather that for the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/gperdrizet/llm_detector/benchmarking/benchmarking/notebooks/images/KL-Gauss-Example.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url = 'https://raw.githubusercontent.com/gperdrizet/llm_detector/benchmarking/benchmarking/notebooks/images/KL-Gauss-Example.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to build a set of functions we can call to generate and add the KLD score feature. Then we will apply them in a loop to each length bin. This should/could be parallelized over the bins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Perplexity ratio score kernel density estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pr_score_kdes(data_df: pd.DataFrame) -> tuple[gaussian_kde, gaussian_kde]:\n",
    "    '''Takes Pandas dataframe with 'Perplexity ratio score' text 'Source'\n",
    "    features. Gets kernel density estimates of perplexity ratio score \n",
    "    distributions for human and synthetic text. Returns KDEs.'''\n",
    "\n",
    "    # Get PR score density in the bins for human and synthetic scores separately\n",
    "    human_scores = data_df['Perplexity ratio score'][data_df['Source'] == 'human']\n",
    "    synthetic_scores = data_df['Perplexity ratio score'][data_df['Source'] == 'synthetic']\n",
    "\n",
    "    # Get KDEs\n",
    "    human_pr_score_kde = gaussian_kde(human_scores)\n",
    "    synthetic_pr_score_kde = gaussian_kde(synthetic_scores)\n",
    "\n",
    "    return human_pr_score_kde, synthetic_pr_score_kde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perplexity ratio score distribution Kullback-Leibler divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p: list, q: list) -> np.ndarray:\n",
    "    '''Takes two lists, calculates Kullback-Leibler divergence'''\n",
    "\n",
    "    # Holder for results\n",
    "    results = []\n",
    "\n",
    "    # Loop on lists of values\n",
    "    for i, j in zip(p, q):\n",
    "\n",
    "        # Check for zeros\n",
    "        if i > 0 and j > 0:\n",
    "\n",
    "            # Add KLD to results\n",
    "            kld_value = i * log2(i/j)\n",
    "            results.append(kld_value)\n",
    "\n",
    "        # Add np.nan for cases where we have zeros\n",
    "        else:\n",
    "            results.append(np.nan)\n",
    "            \n",
    "    # Return the result as numpy array\n",
    "    return np.asarray(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pr_score_kld(\n",
    "        data_df: pd.DataFrame, \n",
    "        human_pr_score_kde: gaussian_kde, \n",
    "        synthetic_pr_score_kde: gaussian_kde,\n",
    "        padding: float = 0.1,\n",
    "        sample_frequency: float = 0.001\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \n",
    "    '''Takes kernel density estimates of perplexity ratio score distributions for\n",
    "    human and synthetic data and original dataset. Calculates Kullback-Leibler\n",
    "    divergences of distributions at set of regularly spaced sample points covering\n",
    "    the original data's range plus some padding on either edge. Returns the \n",
    "    Kullback-Leibler divergence values and the sample points used to calculate them.'''\n",
    "\n",
    "    # Get PR scores\n",
    "    pr_scores = data_df['Perplexity ratio score']\n",
    "\n",
    "    # Get a list of points covering the range of score values and extend\n",
    "    # the left and right edges a little bit, otherwise the kernel density\n",
    "    # estimate tends to droop at the edges of the range. We will clip\n",
    "    # the padding off later.\n",
    "    x = np.arange(\n",
    "        min(pr_scores) - padding, \n",
    "        max(pr_scores) + padding, \n",
    "        sample_frequency\n",
    "    )\n",
    "\n",
    "    # Get fitted values for the points\n",
    "    human_fitted_values = human_pr_score_kde.pdf(x)\n",
    "    synthetic_fitted_values = synthetic_pr_score_kde.pdf(x)\n",
    "    print(f'  Human fitted values: {human_fitted_values[3]}')\n",
    "    print(f'  Human fitted values range: {min(human_fitted_values), max(human_fitted_values)}')\n",
    "    print(f'  Synthetic fitted values: {synthetic_fitted_values[:3]}')\n",
    "    print(f'  Synthetic fitted values range: {min(synthetic_fitted_values), max(synthetic_fitted_values)}')\n",
    "\n",
    "    # Calculate the KL divergences of the fitted values\n",
    "    kld = kl_divergence(synthetic_fitted_values, human_fitted_values)\n",
    "    print(f'  Raw KLD values: {kld[:3]}')\n",
    "    print(f'  Raw KLD range: {min(kld), max(kld)}')\n",
    "\n",
    "    # Get rid of any np.nan, without changing the length\n",
    "    mask = np.isnan(kld)\n",
    "    kld[mask] = 0\n",
    "\n",
    "    # Get rid of any inf without changing the length\n",
    "    mask = np.isinf(kld)\n",
    "    kld[mask] = 0\n",
    "\n",
    "    print(f'  NAN/INF filtered KLD values: {kld[:3]}\\n')\n",
    "\n",
    "    return kld, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Perplexity ratio score distribution Kullback-Leibler divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kld_kde(kld: np.ndarray, x: np.ndarray) -> gaussian_kde:\n",
    "    '''Takes list of Kullback-Leibler divergence values, and regularly\n",
    "    spaced sample points taken from original data's range used to generate \n",
    "    them. Generates and returns gaussian kernel density estimate. Trick \n",
    "    here is that the KLD values are 'density' as they are derived from \n",
    "    the KDEs of the PR score distributions. Therefore they need to be \n",
    "    converted back to 'raw' data.'''\n",
    "\n",
    "    # Convert the KLD 'density' values into integer 'count' values\n",
    "    print(f'  KLD values: {kld[:3]}')\n",
    "\n",
    "    # Shift the kld values so that they are non-negative\n",
    "    kld = kld + abs(min(kld))\n",
    "\n",
    "    # Then scale the values so when we convert to integer we get good\n",
    "    # resolution, e.g. we don't want to collapse 2.1, 2.2, 2.3 etc.,\n",
    "    # to 2. Instead, 2100.0, 2200.0, 2300.0 become 2100, 2200, 2300 etc.\n",
    "    kld = kld * 1000\n",
    "\n",
    "    # Convert to integer\n",
    "    kld_counts = kld.astype(int)\n",
    "    print(f'  KLD counts: {kld_counts[:3]}')\n",
    "\n",
    "    # Now, construct a list where each value of x appears a number of times\n",
    "    # equal to it's KLD 'count'\n",
    "    kld_scores = []\n",
    "\n",
    "    for i in range(len(kld_counts)):\n",
    "        kld_scores.extend([x[i]] * kld_counts[i])\n",
    "\n",
    "    print(f'  KLD scores: {kld_scores[:3]}\\n')\n",
    "\n",
    "    # Then, run a KDE on the reconstructed KLD scores\n",
    "    kld_kde = gaussian_kde(kld_scores)\n",
    "\n",
    "    return kld_kde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## x. Put it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to build a set of functions we can call to generate and add the KLD score feature. Then we will apply them in a loop to each length bin. This should/could be parallelized over the bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The length bins\n",
    "bins = {\n",
    "    'combined': [0, np.inf],\n",
    "    'bin_100': [1, 100],\n",
    "    'bin_150': [51, 150],\n",
    "    'bin_200': [101, 200],\n",
    "    'bin_250': [151, 250],\n",
    "    'bin_300': [201, 300],\n",
    "    'bin_350': [251, 350],\n",
    "    'bin_400': [301, 400],\n",
    "    'bin_450': [351, 450],\n",
    "    'bin_500': [401, 500],\n",
    "    'bin_550': [451, 550],\n",
    "    'bin_600': [501, 600]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Getting training features for bin: combined\n",
      " Training features are type: <class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      " Calculating kernel density estimates\n",
      " Kernel density estimates are type: <class 'scipy.stats._kde.gaussian_kde'>\n",
      "\n",
      " Calculating Kullback-Leibler divergence\n",
      "  Human fitted values: 1.3373083605512705e-246\n",
      "  Human fitted values range: (8.51732283820921e-251, 7.076898076747471)\n",
      "  Synthetic fitted values: [1.08554668e-23 2.77509003e-23 7.02762987e-23]\n",
      "  Synthetic fitted values range: (2.1028846232550933e-165, 7.231800928324723)\n",
      "  Raw KLD values: [8.18966407e-21 2.08443204e-20 5.25537845e-20]\n",
      "  Raw KLD range: (-3.4733882330304398, 25.892866567424857)\n",
      "  NAN/INF filtered KLD values: [8.18966407e-21 2.08443204e-20 5.25537845e-20]\n",
      "\n",
      " Kullback-Leibler divergence is type: <class 'numpy.ndarray'>\n",
      "\n",
      " Calculating Kullback-Leibler kernel density estimate\n",
      "  KLD values: [8.18966407e-21 2.08443204e-20 5.25537845e-20]\n",
      "  KLD counts: [3473 3473 3473]\n",
      "  KLD scores: [0.1590156, 0.1590156, 0.1590156]\n",
      "\n",
      " Kullback-Leibler divergence kernel density estimate is type: <class 'scipy.stats._kde.gaussian_kde'>\n",
      "\n",
      "Getting training features for bin: bin_100\n",
      " Training features are type: <class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      " Calculating kernel density estimates\n",
      " Kernel density estimates are type: <class 'scipy.stats._kde.gaussian_kde'>\n",
      "\n",
      " Calculating Kullback-Leibler divergence\n",
      "  Human fitted values: 3.8362037343389125e-83\n",
      "  Human fitted values range: (1.5933990400420464e-84, 4.4991717310436075)\n",
      "  Synthetic fitted values: [1.08204979e-10 1.54109868e-10 2.18710736e-10]\n",
      "  Synthetic fitted values range: (4.7903555967214795e-64, 4.989999204394668)\n",
      "  Raw KLD values: [2.65388227e-08 3.76398465e-08 5.31938565e-08]\n",
      "  Raw KLD range: (-2.387699672508306, 9.07313951551442)\n",
      "  NAN/INF filtered KLD values: [2.65388227e-08 3.76398465e-08 5.31938565e-08]\n",
      "\n",
      " Kullback-Leibler divergence is type: <class 'numpy.ndarray'>\n",
      "\n",
      " Calculating Kullback-Leibler kernel density estimate\n",
      "  KLD values: [2.65388227e-08 3.76398465e-08 5.31938565e-08]\n",
      "  KLD counts: [2387 2387 2387]\n",
      "  KLD scores: [0.1590156, 0.1590156, 0.1590156]\n",
      "\n",
      " Kullback-Leibler divergence kernel density estimate is type: <class 'scipy.stats._kde.gaussian_kde'>\n",
      "\n",
      "Getting training features for bin: bin_150\n",
      " Training features are type: <class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      " Calculating kernel density estimates\n",
      " Kernel density estimates are type: <class 'scipy.stats._kde.gaussian_kde'>\n",
      "\n",
      " Calculating Kullback-Leibler divergence\n",
      "  Human fitted values: 3.723833111737689e-282\n",
      "  Human fitted values range: (8.136440153028907e-286, 6.0010561136779765)\n",
      "  Synthetic fitted values: [3.76603227e-17 7.26282927e-17 1.39142910e-16]\n",
      "  Synthetic fitted values range: (1.3669557015708283e-39, 6.213949489092433)\n",
      "  Raw KLD values: [3.36113599e-14 6.45936156e-14 1.23316410e-13]\n",
      "  Raw KLD range: (-2.938231922973147, 18.924445862160308)\n",
      "  NAN/INF filtered KLD values: [3.36113599e-14 6.45936156e-14 1.23316410e-13]\n",
      "\n",
      " Kullback-Leibler divergence is type: <class 'numpy.ndarray'>\n",
      "\n",
      " Calculating Kullback-Leibler kernel density estimate\n",
      "  KLD values: [3.36113599e-14 6.45936156e-14 1.23316410e-13]\n",
      "  KLD counts: [2938 2938 2938]\n",
      "  KLD scores: [0.1590156, 0.1590156, 0.1590156]\n",
      "\n",
      " Kullback-Leibler divergence kernel density estimate is type: <class 'scipy.stats._kde.gaussian_kde'>\n",
      "\n",
      "Getting training features for bin: bin_200\n",
      " Training features are type: <class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      " Calculating kernel density estimates\n",
      " Kernel density estimates are type: <class 'scipy.stats._kde.gaussian_kde'>\n",
      "\n",
      " Calculating Kullback-Leibler divergence\n",
      "  Human fitted values: 2.5628480338062283e-259\n",
      "  Human fitted values range: (1.510900374108993e-263, 7.079626535654361)\n",
      "  Synthetic fitted values: [1.45820658e-22 3.62451720e-22 8.92703205e-22]\n",
      "  Synthetic fitted values range: (2.2089516961665136e-113, 7.40545051477208)\n",
      "  Raw KLD values: [1.16734316e-19 2.88928730e-19 7.08599222e-19]\n",
      "  Raw KLD range: (-3.484909077372693, 32.093413614252135)\n",
      "  NAN/INF filtered KLD values: [1.16734316e-19 2.88928730e-19 7.08599222e-19]\n",
      "\n",
      " Kullback-Leibler divergence is type: <class 'numpy.ndarray'>\n",
      "\n",
      " Calculating Kullback-Leibler kernel density estimate\n",
      "  KLD values: [1.16734316e-19 2.88928730e-19 7.08599222e-19]\n",
      "  KLD counts: [3484 3484 3484]\n",
      "  KLD scores: [0.4116848000000001, 0.4116848000000001, 0.4116848000000001]\n",
      "\n",
      " Kullback-Leibler divergence kernel density estimate is type: <class 'scipy.stats._kde.gaussian_kde'>\n",
      "\n",
      "Getting training features for bin: bin_250\n",
      " Training features are type: <class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      " Calculating kernel density estimates\n",
      " Kernel density estimates are type: <class 'scipy.stats._kde.gaussian_kde'>\n",
      "\n",
      " Calculating Kullback-Leibler divergence\n",
      "  Human fitted values: 0.0\n",
      "  Human fitted values range: (0.0, 8.1583888035333)\n",
      "  Synthetic fitted values: [1.00791009e-25 2.90842554e-25 8.30364201e-25]\n",
      "  Synthetic fitted values range: (1.9483137940838275e-103, 8.542097978997592)\n",
      "  Raw KLD values: [nan nan nan]\n",
      "  Raw KLD range: (nan, nan)\n",
      "  NAN/INF filtered KLD values: [0. 0. 0.]\n",
      "\n",
      " Kullback-Leibler divergence is type: <class 'numpy.ndarray'>\n",
      "\n",
      " Calculating Kullback-Leibler kernel density estimate\n",
      "  KLD values: [0. 0. 0.]\n",
      "  KLD counts: [3576 3576 3576]\n",
      "  KLD scores: [0.38324667999999995, 0.38324667999999995, 0.38324667999999995]\n",
      "\n",
      " Kullback-Leibler divergence kernel density estimate is type: <class 'scipy.stats._kde.gaussian_kde'>\n",
      "\n",
      "Getting training features for bin: bin_300\n",
      " Training features are type: <class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      " Calculating kernel density estimates\n",
      " Kernel density estimates are type: <class 'scipy.stats._kde.gaussian_kde'>\n",
      "\n",
      " Calculating Kullback-Leibler divergence\n",
      "  Human fitted values: 0.0\n",
      "  Human fitted values range: (0.0, 9.386969022585825)\n",
      "  Synthetic fitted values: [2.03964588e-28 6.68502020e-28 2.16505677e-27]\n",
      "  Synthetic fitted values range: (9.820176453238389e-106, 9.253151441858032)\n",
      "  Raw KLD values: [nan nan nan]\n",
      "  Raw KLD range: (nan, nan)\n",
      "  NAN/INF filtered KLD values: [0. 0. 0.]\n",
      "\n",
      " Kullback-Leibler divergence is type: <class 'numpy.ndarray'>\n",
      "\n",
      " Calculating Kullback-Leibler kernel density estimate\n",
      "  KLD values: [0. 0. 0.]\n",
      "  KLD counts: [3195 3195 3195]\n",
      "  KLD scores: [0.38324667999999995, 0.38324667999999995, 0.38324667999999995]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1034752/61417125.py:14: RuntimeWarning: overflow encountered in scalar divide\n",
      "  kld_value = i * log2(i/j)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Kullback-Leibler divergence kernel density estimate is type: <class 'scipy.stats._kde.gaussian_kde'>\n",
      "\n",
      "Getting training features for bin: bin_350\n",
      " Training features are type: <class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      " Calculating kernel density estimates\n",
      " Kernel density estimates are type: <class 'scipy.stats._kde.gaussian_kde'>\n",
      "\n",
      " Calculating Kullback-Leibler divergence\n",
      "  Human fitted values: 2.055268881338936e-115\n",
      "  Human fitted values range: (7.357601476572686e-119, 10.23236415621304)\n",
      "  Synthetic fitted values: [1.83598253e-30 6.64365075e-30 2.37318499e-29]\n",
      "  Synthetic fitted values range: (3.581326042091337e-83, 10.192676061116998)\n",
      "  Raw KLD values: [5.39134283e-28 1.93774900e-27 6.87488307e-27]\n",
      "  Raw KLD range: (-2.707458628031779, 73.63025128815259)\n",
      "  NAN/INF filtered KLD values: [5.39134283e-28 1.93774900e-27 6.87488307e-27]\n",
      "\n",
      " Kullback-Leibler divergence is type: <class 'numpy.ndarray'>\n",
      "\n",
      " Calculating Kullback-Leibler kernel density estimate\n",
      "  KLD values: [5.39134283e-28 1.93774900e-27 6.87488307e-27]\n",
      "  KLD counts: [2707 2707 2707]\n",
      "  KLD scores: [0.5962825, 0.5962825, 0.5962825]\n",
      "\n",
      " Kullback-Leibler divergence kernel density estimate is type: <class 'scipy.stats._kde.gaussian_kde'>\n",
      "\n",
      "Getting training features for bin: bin_400\n",
      " Training features are type: <class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      " Calculating kernel density estimates\n",
      " Kernel density estimates are type: <class 'scipy.stats._kde.gaussian_kde'>\n",
      "\n",
      " Calculating Kullback-Leibler divergence\n",
      "  Human fitted values: 4.325770704872589e-214\n",
      "  Human fitted values range: (1.1916553816294653e-218, 10.266282709789014)\n",
      "  Synthetic fitted values: [3.82236398e-27 1.19920836e-26 3.71934682e-26]\n",
      "  Synthetic fitted values range: (1.5498641396560034e-112, 10.511941440863419)\n",
      "  Raw KLD values: [2.43167239e-24 7.58801678e-24 2.34071844e-23]\n",
      "  Raw KLD range: (-2.343910333260096, 118.23463644139733)\n",
      "  NAN/INF filtered KLD values: [2.43167239e-24 7.58801678e-24 2.34071844e-23]\n",
      "\n",
      " Kullback-Leibler divergence is type: <class 'numpy.ndarray'>\n",
      "\n",
      " Calculating Kullback-Leibler kernel density estimate\n",
      "  KLD values: [2.43167239e-24 7.58801678e-24 2.34071844e-23]\n",
      "  KLD counts: [2343 2343 2343]\n",
      "  KLD scores: [0.5142587000000001, 0.5142587000000001, 0.5142587000000001]\n",
      "\n",
      " Kullback-Leibler divergence kernel density estimate is type: <class 'scipy.stats._kde.gaussian_kde'>\n",
      "\n",
      "Getting training features for bin: bin_450\n",
      " Training features are type: <class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      " Calculating kernel density estimates\n",
      " Kernel density estimates are type: <class 'scipy.stats._kde.gaussian_kde'>\n",
      "\n",
      " Calculating Kullback-Leibler divergence\n",
      "  Human fitted values: 2.11e-321\n",
      "  Human fitted values range: (0.0, 11.674379102758971)\n",
      "  Synthetic fitted values: [5.82435945e-19 1.27604888e-18 2.77372336e-18]\n",
      "  Synthetic fitted values range: (2.2419128531442872e-74, 10.133428372043044)\n",
      "  Raw KLD values: [           nan            nan 2.80942052e-15]\n",
      "  Raw KLD range: (nan, nan)\n",
      "  NAN/INF filtered KLD values: [0.00000000e+00 0.00000000e+00 2.80942052e-15]\n",
      "\n",
      " Kullback-Leibler divergence is type: <class 'numpy.ndarray'>\n",
      "\n",
      " Calculating Kullback-Leibler kernel density estimate\n",
      "  KLD values: [0.00000000e+00 0.00000000e+00 2.80942052e-15]\n",
      "  KLD counts: [2215 2215 2215]\n",
      "  KLD scores: [0.5142587000000001, 0.5142587000000001, 0.5142587000000001]\n",
      "\n",
      " Kullback-Leibler divergence kernel density estimate is type: <class 'scipy.stats._kde.gaussian_kde'>\n",
      "\n",
      "Getting training features for bin: bin_500\n",
      " Training features are type: <class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      " Calculating kernel density estimates\n",
      " Kernel density estimates are type: <class 'scipy.stats._kde.gaussian_kde'>\n",
      "\n",
      " Calculating Kullback-Leibler divergence\n",
      "  Human fitted values: 4.836042502086263e-137\n",
      "  Human fitted values range: (3.5092591767147534e-140, 12.055488196975476)\n",
      "  Synthetic fitted values: [7.21927722e-08 9.78657367e-08 1.32263391e-07]\n",
      "  Synthetic fitted values range: (5.208475150873496e-24, 9.7215384645342)\n",
      "  Raw KLD values: [3.17312643e-05 4.27168994e-05 5.73286564e-05]\n",
      "  Raw KLD range: (-3.00524845104282, 179.40491153430617)\n",
      "  NAN/INF filtered KLD values: [3.17312643e-05 4.27168994e-05 5.73286564e-05]\n",
      "\n",
      " Kullback-Leibler divergence is type: <class 'numpy.ndarray'>\n",
      "\n",
      " Calculating Kullback-Leibler kernel density estimate\n",
      "  KLD values: [3.17312643e-05 4.27168994e-05 5.73286564e-05]\n",
      "  KLD counts: [3005 3005 3005]\n",
      "  KLD scores: [0.6226361, 0.6226361, 0.6226361]\n",
      "\n",
      " Kullback-Leibler divergence kernel density estimate is type: <class 'scipy.stats._kde.gaussian_kde'>\n",
      "\n",
      "Getting training features for bin: bin_550\n",
      " Training features are type: <class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      " Calculating kernel density estimates\n",
      " Kernel density estimates are type: <class 'scipy.stats._kde.gaussian_kde'>\n",
      "\n",
      " Calculating Kullback-Leibler divergence\n",
      "  Human fitted values: 1.1404123135280968e-58\n",
      "  Human fitted values range: (1.2289143753007255e-60, 12.544641788615941)\n",
      "  Synthetic fitted values: [5.87644331e-08 8.30073549e-08 1.16860570e-07]\n",
      "  Synthetic fitted values range: (4.573023224657458e-47, 11.793925268243807)\n",
      "  Raw KLD values: [1.02836477e-05 1.43856015e-05 2.00556046e-05]\n",
      "  Raw KLD range: (-3.3207587413802178, 255.45343994098857)\n",
      "  NAN/INF filtered KLD values: [1.02836477e-05 1.43856015e-05 2.00556046e-05]\n",
      "\n",
      " Kullback-Leibler divergence is type: <class 'numpy.ndarray'>\n",
      "\n",
      " Calculating Kullback-Leibler kernel density estimate\n",
      "  KLD values: [1.02836477e-05 1.43856015e-05 2.00556046e-05]\n",
      "  KLD counts: [3320 3320 3320]\n",
      "  KLD scores: [0.72063127, 0.72063127, 0.72063127]\n",
      "\n",
      " Kullback-Leibler divergence kernel density estimate is type: <class 'scipy.stats._kde.gaussian_kde'>\n",
      "\n",
      "Getting training features for bin: bin_600\n",
      " Training features are type: <class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      " Calculating kernel density estimates\n",
      " Kernel density estimates are type: <class 'scipy.stats._kde.gaussian_kde'>\n",
      "\n",
      " Calculating Kullback-Leibler divergence\n",
      "  Human fitted values: 3.661709711815138e-105\n",
      "  Human fitted values range: (9.063283776252482e-108, 12.989397164685265)\n",
      "  Synthetic fitted values: [1.36476676e-05 1.71623097e-05 2.15324208e-05]\n",
      "  Synthetic fitted values range: (1.708796556706857e-41, 10.844847855537367)\n",
      "  Raw KLD values: [0.00463239 0.00578129 0.00719831]\n",
      "  Raw KLD range: (-2.405244086376866, 289.3997845410687)\n",
      "  NAN/INF filtered KLD values: [0.00463239 0.00578129 0.00719831]\n",
      "\n",
      " Kullback-Leibler divergence is type: <class 'numpy.ndarray'>\n",
      "\n",
      " Calculating Kullback-Leibler kernel density estimate\n",
      "  KLD values: [0.00463239 0.00578129 0.00719831]\n",
      "  KLD counts: [2409 2411 2412]\n",
      "  KLD scores: [0.6553011, 0.6553011, 0.6553011]\n",
      "\n",
      " Kullback-Leibler divergence kernel density estimate is type: <class 'scipy.stats._kde.gaussian_kde'>\n"
     ]
    }
   ],
   "source": [
    "# Reopen out hdf5 file with pandas so we can work with dataframes\n",
    "data_lake = pd.HDFStore(config.LENGTH_BINNED_DATASET)\n",
    "\n",
    "# Loop on the bins\n",
    "for bin_id in bins.keys():\n",
    "\n",
    "    # Pull the training features for this bin\n",
    "    print(f'\\nGetting training features for bin: {bin_id}')\n",
    "    bin_training_features_df = data_lake[f'training/{bin_id}/features']\n",
    "    print(f' Training features are type: {type(bin_training_features_df)}')\n",
    "\n",
    "    # Calculate the PR score distribution kernel density estimates\n",
    "    print('\\n Calculating kernel density estimates')\n",
    "    human_pr_score_kde, synthetic_pr_score_kde = get_pr_score_kdes(bin_training_features_df)\n",
    "    print(f' Kernel density estimates are type: {type(human_pr_score_kde)}')\n",
    "\n",
    "    # Calculate the Kullback-Leibler divergence\n",
    "    print('\\n Calculating Kullback-Leibler divergence')\n",
    "    pr_score_kld, x = get_pr_score_kld(\n",
    "        bin_training_features_df, \n",
    "        human_pr_score_kde, \n",
    "        synthetic_pr_score_kde,\n",
    "        padding = 0.1,\n",
    "        sample_frequency = 0.001\n",
    "    )\n",
    "    print(f' Kullback-Leibler divergence is type: {type(pr_score_kld)}')\n",
    "\n",
    "    # Get kernel density estimate of Kullback-Leibler divergence\n",
    "    print('\\n Calculating Kullback-Leibler kernel density estimate')\n",
    "    kld_kde = get_kld_kde(pr_score_kld, x)\n",
    "    print(f' Kullback-Leibler divergence kernel density estimate is type: {type(kld_kde)}')\n",
    "\n",
    "data_lake.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
