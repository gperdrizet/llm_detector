{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity ratio score: Kullbackâ€“Leibler divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/arkk/llm_detector/classifier\n"
     ]
    }
   ],
   "source": [
    "# Change working directory to parent so we can import as we would from main.py\n",
    "%cd ..\n",
    "\n",
    "# import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "from IPython.display import Image\n",
    "\n",
    "import functions.notebook_helper as helper_funcs\n",
    "import functions.notebook_plotting as plot_funcs\n",
    "import configuration as config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan here is to take our sampling distributions of perplexity ratio (PR) scores for human and synthetic text and use them to generate a function that takes a perplexity ratio score and converts it into a Kullback-Leibler divergence (KLD) score. See the figure below from the [Wikipedia article on KLD](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence).\n",
    "\n",
    "Workflow is as follows:\n",
    "1. Get kernel density estimate of PR score distribution for human and synthetic text fragments in training data.\n",
    "2. Calculated KLD between the human and synthetic PR score distributions.\n",
    "3. Get get kernel density estimate of KLD.\n",
    "4. Use probability density function of KLD kernel density estimate to calculate KLD score for each text fragment in the training and testing data.\n",
    "5. Add the KLD score as a new feature.\n",
    "\n",
    "The above will be done individually for each fragment length bin and the combined data. This way the KLD score feature in each bin will capture the PR score distribution for text fragments in that specific length regime, rather that for the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/gperdrizet/llm_detector/benchmarking/benchmarking/notebooks/images/KL-Gauss-Example.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url = 'https://raw.githubusercontent.com/gperdrizet/llm_detector/benchmarking/benchmarking/notebooks/images/KL-Gauss-Example.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to build a set of functions we can call to generate and add the KLD score feature. Then we will apply them in a loop to each length bin. This should/could be parallelized over the bins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Perplexity ratio score kernel density estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_score_kde(data_df: pd.DataFrame, n_bins: int = 100) -> pd.DataFrame:\n",
    "    '''Takes Pandas dataframe with 'Perplexity ratio score' text 'Source'\n",
    "    features. Gets kernel density estimates of perplexity ratio score \n",
    "    distributions for human and synthetic text. Returns KDEs.'''\n",
    "\n",
    "    # Get PR score density in the bins for human and synthetic scores separately\n",
    "    human_scores = data_df['Perplexity ratio score'][data_df['Source'] == 'human']\n",
    "    synthetic_scores = data_df['Perplexity ratio score'][data_df['Source'] == 'human']\n",
    "\n",
    "    # Get KDEs\n",
    "    human_pr_score_kde = gaussian_kde(human_scores)\n",
    "    synthetic_pr_score_kde = gaussian_kde(synthetic_scores)\n",
    "\n",
    "    return human_pr_score_kde, synthetic_pr_score_kde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perplexity ratio score distribution Kullback-Leibler divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p, q):\n",
    "    '''Takes two lists, calculates Kullback-Leibler divergence'''\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i, j in zip(p, q):\n",
    "        if i > 0 and j > 0:\n",
    "            results.append(i * log2(i/j))\n",
    "\n",
    "        else:\n",
    "            results.append(np.nan)\n",
    "\n",
    "    return np.asarray(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_score_kld(\n",
    "        data_df: pd.DataFrame, \n",
    "        human_pr_score_kde, \n",
    "        synthetic_pr_score_kde,\n",
    "        padding: float = 0.1,\n",
    "        sample_frequency: float = 0.001\n",
    "):\n",
    "    '''Takes kernel density estimates of perplexity ratio score distributions for\n",
    "    human and synthetic data and original dataset. Calculates Kullback-Leibler\n",
    "    divergences of distributions at set regularly spaced sample points covering\n",
    "    the original data's range plus some padding on either edge.'''\n",
    "\n",
    "    # Get PR scores\n",
    "    pr_scores = data_df['Perplexity ratio score']\n",
    "\n",
    "    # Get a list of points covering the range of score values and extend\n",
    "    # the left and right edges a little bit, otherwise the kernel density\n",
    "    # estimate tends to droop at the edges of the range. We will clip\n",
    "    # the padding off later.\n",
    "    x = np.arange(\n",
    "        min(pr_scores) - padding, \n",
    "        max(pr_scores) + padding, \n",
    "        sample_frequency\n",
    "    ).tolist()\n",
    "\n",
    "    # Get fitted values for the points\n",
    "    human_fitted_values = human_pr_score_kde.pdf(x)\n",
    "    synthetic_fitted_values = synthetic_pr_score_kde.pdf(x)\n",
    "\n",
    "    # Calculate the KL divergences of the fitted values\n",
    "    kld = kl_divergence(synthetic_fitted_values, human_fitted_values)\n",
    "\n",
    "    # Get rid of any np.nan, without changing the length\n",
    "    mask = np.isnan(kld)\n",
    "    kld[mask] = 0\n",
    "\n",
    "    # Get rid of any inf without changing the length\n",
    "    mask = np.isinf(kld)\n",
    "    kld[mask] = 0\n",
    "\n",
    "    return kld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## x. Put it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to build a set of functions we can call to generate and add the KLD score feature. Then we will apply them in a loop to each length bin. This should/could be parallelized over the bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The length bins\n",
    "bins = {\n",
    "    'combined': [0, np.inf],\n",
    "    'bin_100': [1, 100],\n",
    "    'bin_150': [51, 150],\n",
    "    'bin_200': [101, 200],\n",
    "    'bin_250': [151, 250],\n",
    "    'bin_300': [201, 300],\n",
    "    'bin_350': [251, 350],\n",
    "    'bin_400': [301, 400],\n",
    "    'bin_450': [351, 450],\n",
    "    'bin_500': [401, 500],\n",
    "    'bin_550': [451, 550],\n",
    "    'bin_600': [501, 600]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Getting training features for bin: combined\n",
      "Calculating kernel density estimates for bin: combined\n",
      "\n",
      "Getting training features for bin: bin_100\n",
      "Calculating kernel density estimates for bin: bin_100\n",
      "\n",
      "Getting training features for bin: bin_150\n",
      "Calculating kernel density estimates for bin: bin_150\n",
      "\n",
      "Getting training features for bin: bin_200\n",
      "Calculating kernel density estimates for bin: bin_200\n",
      "\n",
      "Getting training features for bin: bin_250\n",
      "Calculating kernel density estimates for bin: bin_250\n",
      "\n",
      "Getting training features for bin: bin_300\n",
      "Calculating kernel density estimates for bin: bin_300\n",
      "\n",
      "Getting training features for bin: bin_350\n",
      "Calculating kernel density estimates for bin: bin_350\n",
      "\n",
      "Getting training features for bin: bin_400\n",
      "Calculating kernel density estimates for bin: bin_400\n",
      "\n",
      "Getting training features for bin: bin_450\n",
      "Calculating kernel density estimates for bin: bin_450\n",
      "\n",
      "Getting training features for bin: bin_500\n",
      "Calculating kernel density estimates for bin: bin_500\n",
      "\n",
      "Getting training features for bin: bin_550\n",
      "Calculating kernel density estimates for bin: bin_550\n",
      "\n",
      "Getting training features for bin: bin_600\n",
      "Calculating kernel density estimates for bin: bin_600\n"
     ]
    }
   ],
   "source": [
    "# Reopen out hdf5 file with pandas so we can work with dataframes\n",
    "data_lake = pd.HDFStore(config.LENGTH_BINNED_DATASET)\n",
    "\n",
    "# Loop on the bins\n",
    "for bin_id in bins.keys():\n",
    "\n",
    "    # Pull the training features for this bin\n",
    "    print(f'\\nGetting training features for bin: {bin_id}')\n",
    "    bin_training_features_df = data_lake[f'training/{bin_id}/features']\n",
    "\n",
    "    # Calculate the PR score distribution kernel density estimates\n",
    "    print(f'Calculating kernel density estimates for bin: {bin_id}')\n",
    "    human_pr_score_kde, synthetic_pr_score_kde = pr_score_kde(bin_training_features_df)\n",
    "    print(f'Kernel density estimates are type: {type(human_pr_score_kde)}')\n",
    "\n",
    "    # Calculate the Kullback-Leibler divergences\n",
    "    print(f'Calculating Kullback-Leibler divergence for bin: {bin_id}')\n",
    "    pr_score_kld = pr_score_kld(\n",
    "        bin_training_features_df, \n",
    "        human_pr_score_kde, \n",
    "        synthetic_pr_score_kde,\n",
    "        padding = 0.1,\n",
    "        sample_frequency = 0.001\n",
    "    )\n",
    "    print(f'Calculating Kullback-Leibler divergence is type: {type(human_pr_score_kde)}')\n",
    "\n",
    "data_lake.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
