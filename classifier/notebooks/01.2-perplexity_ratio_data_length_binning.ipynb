{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity ratio data length binning\n",
    "\n",
    "The purpose of this notebook is to design a strategy to load text fragment data with perplexity ratio score feature generated by the scoring algorithm, bin it by text fragment length and get it ready for input into feature engineering. The plan is to handle data as Pandas dataframes and store it using hdf5. The text fragments will be put into overlapping length bins so that feature engineering and classifier training can be conducted separately for different length regimes. This general strategy is based on early observations of classifier performance on short fragments, long fragments and un-binned fragments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Run set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /mnt/arkk/llm_detector/classifier\n",
      "\n",
      "H5py: 3.11.0\n",
      "Numpy: 1.24.4\n",
      "Pandas: 2.0.3\n",
      "Sklearn: 1.3.2\n"
     ]
    }
   ],
   "source": [
    "# Change working directory to parent so we can import as we would from __main__.py\n",
    "print(f'Working directory: ', end = '')\n",
    "%cd ..\n",
    "print()\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import configuration as config\n",
    "\n",
    "print(f'H5py: {h5py.__version__}')\n",
    "print(f'Numpy: {np.__version__}')\n",
    "print(f'Pandas: {pd.__version__}')\n",
    "print(f'Sklearn: {sk.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset we want to bin - omit the file extension, it will be \n",
    "# added appropriately for the input and output files\n",
    "dataset_name = 'falcon-7b_scores_v2_10-300_words'\n",
    "\n",
    "# Construct input and output file paths\n",
    "input_file = f'{config.HANS_DATA_PATH}/{dataset_name}.json'\n",
    "output_file = f'{config.DATA_PATH}/{dataset_name}.h5'\n",
    "\n",
    "# # Bins for 10-1000 word dataset\n",
    "# bins = {\n",
    "#     'combined': [0, np.inf],\n",
    "#     'bin_100': [1, 100],\n",
    "#     'bin_150': [51, 150],\n",
    "#     'bin_200': [101, 200],\n",
    "#     'bin_250': [151, 250],\n",
    "#     'bin_300': [201, 300],\n",
    "#     'bin_350': [251, 350],\n",
    "#     'bin_400': [301, 400],\n",
    "#     'bin_450': [351, 450],\n",
    "#     'bin_500': [401, 500],\n",
    "#     'bin_600': [451, 600]\n",
    "# }\n",
    "\n",
    "# Bins for 10-300 word dataset\n",
    "bins = {\n",
    "    'combined': [0, np.inf],\n",
    "    'bin_50': [1, 50],\n",
    "    'bin_75': [26, 75],\n",
    "    'bin_100': [51, 100],\n",
    "    'bin_125': [76, 125],\n",
    "    'bin_150': [101, 150],\n",
    "    'bin_175': [126, 175],\n",
    "    'bin_200': [151, 200],\n",
    "    'bin_225': [176, 225],\n",
    "    'bin_250': [201, 250],\n",
    "    'bin_275': [226, 275],\n",
    "    'bin_300': [251, 300]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fragment ID</th>\n",
       "      <th>Source record num</th>\n",
       "      <th>Fragment length (words)</th>\n",
       "      <th>Fragment length (tokens)</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Source</th>\n",
       "      <th>Generator</th>\n",
       "      <th>String</th>\n",
       "      <th>Perplexity</th>\n",
       "      <th>Cross-perplexity</th>\n",
       "      <th>Perplexity ratio score</th>\n",
       "      <th>Reader time (seconds)</th>\n",
       "      <th>Writer time (seconds)</th>\n",
       "      <th>Reader peak memory (GB)</th>\n",
       "      <th>Writer peak memory (GB)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3680</td>\n",
       "      <td>113</td>\n",
       "      <td>147</td>\n",
       "      <td>cc_news</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>llama2-13b</td>\n",
       "      <td>for his coverage of 2008 Mumbai terror attacks...</td>\n",
       "      <td>2.459</td>\n",
       "      <td>2.787109</td>\n",
       "      <td>0.882270</td>\n",
       "      <td>2.406446</td>\n",
       "      <td>2.624167</td>\n",
       "      <td>5.897318</td>\n",
       "      <td>5.856219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10784</td>\n",
       "      <td>251</td>\n",
       "      <td>342</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>llama2-13b</td>\n",
       "      <td>was resuscitated successfully but subsequently...</td>\n",
       "      <td>1.187</td>\n",
       "      <td>1.419922</td>\n",
       "      <td>0.835626</td>\n",
       "      <td>4.489942</td>\n",
       "      <td>5.005341</td>\n",
       "      <td>8.505339</td>\n",
       "      <td>8.402437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3313</td>\n",
       "      <td>121</td>\n",
       "      <td>167</td>\n",
       "      <td>cc_news</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>llama2-13b</td>\n",
       "      <td>chiropractor from Lake Oswego, Oregon. \"I don'...</td>\n",
       "      <td>2.924</td>\n",
       "      <td>2.888672</td>\n",
       "      <td>1.012170</td>\n",
       "      <td>2.445095</td>\n",
       "      <td>2.690631</td>\n",
       "      <td>6.186151</td>\n",
       "      <td>6.132349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>8361</td>\n",
       "      <td>275</td>\n",
       "      <td>375</td>\n",
       "      <td>cnn</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>llama2-13b</td>\n",
       "      <td>every one of these superfruits, but can they a...</td>\n",
       "      <td>2.299</td>\n",
       "      <td>2.509766</td>\n",
       "      <td>0.915953</td>\n",
       "      <td>4.490003</td>\n",
       "      <td>5.042619</td>\n",
       "      <td>8.633420</td>\n",
       "      <td>8.534275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>8274</td>\n",
       "      <td>182</td>\n",
       "      <td>233</td>\n",
       "      <td>cnn</td>\n",
       "      <td>human</td>\n",
       "      <td>human</td>\n",
       "      <td>(CNN)Well, I'll be the first to admit, I got c...</td>\n",
       "      <td>2.840</td>\n",
       "      <td>2.917969</td>\n",
       "      <td>0.973226</td>\n",
       "      <td>3.147694</td>\n",
       "      <td>3.505678</td>\n",
       "      <td>5.785840</td>\n",
       "      <td>5.773574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fragment ID  Source record num  Fragment length (words)  \\\n",
       "0            0               3680                      113   \n",
       "1            1              10784                      251   \n",
       "2            2               3313                      121   \n",
       "3            3               8361                      275   \n",
       "4            4               8274                      182   \n",
       "\n",
       "   Fragment length (tokens)  Dataset     Source   Generator  \\\n",
       "0                       147  cc_news  synthetic  llama2-13b   \n",
       "1                       342   pubmed  synthetic  llama2-13b   \n",
       "2                       167  cc_news  synthetic  llama2-13b   \n",
       "3                       375      cnn  synthetic  llama2-13b   \n",
       "4                       233      cnn      human       human   \n",
       "\n",
       "                                              String  Perplexity  \\\n",
       "0  for his coverage of 2008 Mumbai terror attacks...       2.459   \n",
       "1  was resuscitated successfully but subsequently...       1.187   \n",
       "2  chiropractor from Lake Oswego, Oregon. \"I don'...       2.924   \n",
       "3  every one of these superfruits, but can they a...       2.299   \n",
       "4  (CNN)Well, I'll be the first to admit, I got c...       2.840   \n",
       "\n",
       "   Cross-perplexity  Perplexity ratio score  Reader time (seconds)  \\\n",
       "0          2.787109                0.882270               2.406446   \n",
       "1          1.419922                0.835626               4.489942   \n",
       "2          2.888672                1.012170               2.445095   \n",
       "3          2.509766                0.915953               4.490003   \n",
       "4          2.917969                0.973226               3.147694   \n",
       "\n",
       "   Writer time (seconds)  Reader peak memory (GB)  Writer peak memory (GB)  \n",
       "0               2.624167                 5.897318                 5.856219  \n",
       "1               5.005341                 8.505339                 8.402437  \n",
       "2               2.690631                 6.186151                 6.132349  \n",
       "3               5.042619                 8.633420                 8.534275  \n",
       "4               3.505678                 5.785840                 5.773574  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "data_df = pd.read_json(input_file)\n",
    "\n",
    "# Replace and remove string 'OOM' and 'NAN' values\n",
    "data_df.replace('NAN', np.nan, inplace = True)\n",
    "data_df.replace('OOM', np.nan, inplace = True)\n",
    "data_df.dropna(inplace = True)\n",
    "\n",
    "# Shuffle the deck, resetting the index\n",
    "data_df = data_df.sample(frac = 1).reset_index(drop = True)\n",
    "\n",
    "# Use the index to add a unique fragment id\n",
    "data_df.reset_index(inplace = True)\n",
    "data_df.rename({'index': 'Fragment ID'}, axis = 1, inplace = True)\n",
    "\n",
    "# Enforce dtypes\n",
    "data_df = data_df.astype({\n",
    "    'Fragment ID': np.int64,\n",
    "    'Source record num': np.int64,\n",
    "    'Fragment length (words)': np.int64,\n",
    "    'Fragment length (tokens)': np.int64,\n",
    "    'Dataset': object, #pd.StringDtype(), pandas recommends these, but PyTables for hdf5 can't use them\n",
    "    'Source': object, #pd.StringDtype(),\n",
    "    'Generator': object, #pd.StringDtype(),\n",
    "    'String': object, #pd.StringDtype(),\n",
    "    'Perplexity': np.float64,\n",
    "    'Cross-perplexity': np.float64,\n",
    "    'Perplexity ratio score': np.float64,\n",
    "    'Reader time (seconds)': np.float64,\n",
    "    'Writer time (seconds)': np.float64,\n",
    "    'Reader peak memory (GB)': np.float64,\n",
    "    'Writer peak memory (GB)': np.float64\n",
    "})\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 55813 entries, 0 to 55812\n",
      "Data columns (total 15 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Fragment ID               55813 non-null  int64  \n",
      " 1   Source record num         55813 non-null  int64  \n",
      " 2   Fragment length (words)   55813 non-null  int64  \n",
      " 3   Fragment length (tokens)  55813 non-null  int64  \n",
      " 4   Dataset                   55813 non-null  object \n",
      " 5   Source                    55813 non-null  object \n",
      " 6   Generator                 55813 non-null  object \n",
      " 7   String                    55813 non-null  object \n",
      " 8   Perplexity                55812 non-null  float64\n",
      " 9   Cross-perplexity          55813 non-null  float64\n",
      " 10  Perplexity ratio score    55812 non-null  float64\n",
      " 11  Reader time (seconds)     55813 non-null  float64\n",
      " 12  Writer time (seconds)     55813 non-null  float64\n",
      " 13  Reader peak memory (GB)   55813 non-null  float64\n",
      " 14  Writer peak memory (GB)   55813 non-null  float64\n",
      "dtypes: float64(7), int64(4), object(4)\n",
      "memory usage: 6.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate hdf5 data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's start structuring the dataset. We want two top level groups, one for training data and one for reserved testing data. Inside those groups will live datasets for the bins. We will also use attributes to save some metadata etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top level groups: ['testing', 'training']\n",
      "\n",
      "Bin attributes:\n",
      " bin_100: [ 51 100]\n",
      " bin_125: [ 76 125]\n",
      " bin_150: [101 150]\n",
      " bin_175: [126 175]\n",
      " bin_200: [151 200]\n",
      " bin_225: [176 225]\n",
      " bin_250: [201 250]\n",
      " bin_275: [226 275]\n",
      " bin_300: [251 300]\n",
      " bin_50: [ 1 50]\n",
      " bin_75: [26 75]\n",
      " combined: [ 0. inf]\n"
     ]
    }
   ],
   "source": [
    "# Prepare the hdf5 output - create or open for read/write\n",
    "output = h5py.File(output_file, 'a')\n",
    "\n",
    "# Create the top-level groups\n",
    "_ = output.require_group('training')\n",
    "_ = output.require_group('testing')\n",
    "\n",
    "print(f'Top level groups: {(list(output.keys()))}')\n",
    "\n",
    "# Next, we need to add a group for each fragment length bin,\n",
    "# and one for the un-binned data\n",
    "for group in output.keys():\n",
    "\n",
    "    # Add the un-binned data group\n",
    "    _ = output.require_group(f'{group}/combined')\n",
    "\n",
    "    # Loop on the bins and add a group for each\n",
    "    for bin in bins.keys():\n",
    "        _ = output.require_group(f'{group}/{bin}')\n",
    "\n",
    "# Finally, add the bins under group bins\n",
    "output.attrs.update(bins)\n",
    "\n",
    "print(f'\\nBin attributes:')\n",
    "for key, value in output.attrs.items():\n",
    "    print(f' {key}: {value}')\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Populate hdf5 data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, the basic data structure is ready to go. Next thing to do is add data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_labels(training_df, testing_df):\n",
    "    '''Takes training and testing dataframes, gets and encode human/synthetic\n",
    "    labels, encodes labels and returns. Note: this function leaves the \n",
    "    'Source' label column in the features dataframe for downstream use in \n",
    "    feature engineering.'''\n",
    "\n",
    "    # Get the labels\n",
    "    training_labels = training_df['Source']\n",
    "    testing_labels = testing_df['Source']\n",
    "\n",
    "    # Encode string class values as integers\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder = label_encoder.fit(training_labels)\n",
    "    training_labels = pd.Series(label_encoder.transform(training_labels)).astype(np.int64)\n",
    "    testing_labels = pd.Series(label_encoder.transform(testing_labels)).astype(np.int64)\n",
    "\n",
    "    return training_labels, testing_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reopen out hdf5 file with pandas so we can work with dataframes\n",
    "data_lake = pd.HDFStore(output_file)\n",
    "\n",
    "# Add the raw data set at the top level as 'master' incase we want it later.\n",
    "data_lake['master'] = data_df\n",
    "\n",
    "# Next, get rid of un-trainable/unnecessary features\n",
    "feature_drops = [\n",
    "    'Fragment ID',\n",
    "    'Source record num',\n",
    "    'Dataset',\n",
    "    'Generator',\n",
    "    'Reader time (seconds)',\n",
    "    'Writer time (seconds)',\n",
    "    'Reader peak memory (GB)',\n",
    "    'Writer peak memory (GB)'\n",
    "]\n",
    "\n",
    "data_df.drop(feature_drops, axis = 1, inplace = True)\n",
    "\n",
    "# Split the data into training and testing\n",
    "training_df = data_df.sample(frac = 0.7, random_state = 42)\n",
    "testing_df = data_df.drop(training_df.index)\n",
    "\n",
    "# Now do the same for the bins\n",
    "for bin_id, bin_range in bins.items():\n",
    "\n",
    "    # Pull the fragments for this bin\n",
    "    bin_training_df = training_df[(training_df['Fragment length (words)'] >= bin_range[0]) & (training_df['Fragment length (words)'] <= bin_range[1])]\n",
    "    bin_testing_df = testing_df[(testing_df['Fragment length (words)'] >= bin_range[0]) & (testing_df['Fragment length (words)'] <= bin_range[1])]\n",
    "\n",
    "    # Fix the index\n",
    "    bin_training_df.reset_index(inplace = True, drop = True)\n",
    "    bin_testing_df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "    # Split un-binned data into features and labels\n",
    "    bin_training_labels, bin_testing_labels = make_labels(bin_training_df, bin_testing_df)\n",
    "\n",
    "    # Add the data to the data lake\n",
    "    data_lake.put(f'training/{bin_id}/features', bin_training_df)\n",
    "    data_lake.put(f'training/{bin_id}/labels', bin_training_labels)\n",
    "    data_lake.put(f'testing/{bin_id}/features', bin_training_df)\n",
    "    data_lake.put(f'testing/{bin_id}/labels', bin_training_labels)\n",
    "\n",
    "data_lake.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sanity check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "master contains:\n",
      " axis0\n",
      " axis1\n",
      " block0_items\n",
      " block0_values\n",
      " block1_items\n",
      " block1_values\n",
      " block2_items\n",
      " block2_values\n",
      "\n",
      "testing contains:\n",
      "  bin_100 contains: features labels \n",
      "  bin_125 contains: features labels \n",
      "  bin_150 contains: features labels \n",
      "  bin_175 contains: features labels \n",
      "  bin_200 contains: features labels \n",
      "  bin_225 contains: features labels \n",
      "  bin_250 contains: features labels \n",
      "  bin_275 contains: features labels \n",
      "  bin_300 contains: features labels \n",
      "  bin_50 contains: features labels \n",
      "  bin_75 contains: features labels \n",
      "  combined contains: features labels \n",
      "\n",
      "training contains:\n",
      "  bin_100 contains: features labels \n",
      "  bin_125 contains: features labels \n",
      "  bin_150 contains: features labels \n",
      "  bin_175 contains: features labels \n",
      "  bin_200 contains: features labels \n",
      "  bin_225 contains: features labels \n",
      "  bin_250 contains: features labels \n",
      "  bin_275 contains: features labels \n",
      "  bin_300 contains: features labels \n",
      "  bin_50 contains: features labels \n",
      "  bin_75 contains: features labels \n",
      "  combined contains: features labels \n"
     ]
    }
   ],
   "source": [
    "# Open the data lake to check out the result\n",
    "data_lake = h5py.File(output_file, 'a')\n",
    "\n",
    "# Print the result\n",
    "for group in data_lake.keys():\n",
    "    print(f'\\n{group} contains:')\n",
    "\n",
    "    for subgroup in data_lake[group]:\n",
    "\n",
    "        if 'bin' in subgroup:\n",
    "            print(f'  {subgroup} contains: ', end = '')\n",
    "\n",
    "            for subsubgroup in data_lake[group][subgroup].keys():\n",
    "                print(f'{subsubgroup} ', end = '')\n",
    "            \n",
    "            print()\n",
    "\n",
    "        else:\n",
    "            print(f' {subgroup}')\n",
    "\n",
    "data_lake.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined training features:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 39069 entries, 0 to 39068\n",
      "Data columns (total 7 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Fragment length (words)   39069 non-null  int64  \n",
      " 1   Fragment length (tokens)  39069 non-null  int64  \n",
      " 2   Source                    39069 non-null  object \n",
      " 3   String                    39069 non-null  object \n",
      " 4   Perplexity                39069 non-null  float64\n",
      " 5   Cross-perplexity          39069 non-null  float64\n",
      " 6   Perplexity ratio score    39069 non-null  float64\n",
      "dtypes: float64(3), int64(2), object(2)\n",
      "memory usage: 2.4+ MB\n",
      "None\n",
      "\n",
      "Combined training labels:\n",
      "\n",
      "<class 'pandas.core.series.Series'>\n",
      "Index: 39069 entries, 0 to 39068\n",
      "Series name: None\n",
      "Non-Null Count  Dtype\n",
      "--------------  -----\n",
      "39069 non-null  int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 610.5 KB\n",
      "None\n",
      "\n",
      "Bin 100 training features:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 8071 entries, 0 to 8070\n",
      "Data columns (total 7 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Fragment length (words)   8071 non-null   int64  \n",
      " 1   Fragment length (tokens)  8071 non-null   int64  \n",
      " 2   Source                    8071 non-null   object \n",
      " 3   String                    8071 non-null   object \n",
      " 4   Perplexity                8071 non-null   float64\n",
      " 5   Cross-perplexity          8071 non-null   float64\n",
      " 6   Perplexity ratio score    8071 non-null   float64\n",
      "dtypes: float64(3), int64(2), object(2)\n",
      "memory usage: 504.4+ KB\n",
      "None\n",
      "\n",
      "Bin 100 training labels:\n",
      "\n",
      "<class 'pandas.core.series.Series'>\n",
      "Index: 8071 entries, 0 to 8070\n",
      "Series name: None\n",
      "Non-Null Count  Dtype\n",
      "--------------  -----\n",
      "8071 non-null   int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 126.1 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Reopen out hdf5 file with pandas so we can work with dataframes\n",
    "data_lake = pd.HDFStore(output_file)\n",
    "\n",
    "print('Combined training features:\\n')\n",
    "print(data_lake['training/combined/features'].info())\n",
    "print('\\nCombined training labels:\\n')\n",
    "print(data_lake['training/combined/labels'].info())\n",
    "print('\\nBin 100 training features:\\n')\n",
    "print(data_lake['training/bin_100/features'].info())\n",
    "print('\\nBin 100 training labels:\\n')\n",
    "print(data_lake['training/bin_100/labels'].info())\n",
    "\n",
    "data_lake.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROUP: /testing\n",
      "GROUP: /training\n",
      "KEY: /master\n",
      "GROUP: /testing/bin_100\n",
      "GROUP: /testing/bin_125\n",
      "GROUP: /testing/bin_150\n",
      "GROUP: /testing/bin_175\n",
      "GROUP: /testing/bin_200\n",
      "GROUP: /testing/bin_225\n",
      "GROUP: /testing/bin_250\n",
      "GROUP: /testing/bin_275\n",
      "GROUP: /testing/bin_300\n",
      "GROUP: /testing/bin_50\n",
      "GROUP: /testing/bin_75\n",
      "GROUP: /testing/combined\n",
      "GROUP: /training/bin_100\n",
      "GROUP: /training/bin_125\n",
      "GROUP: /training/bin_150\n",
      "GROUP: /training/bin_175\n",
      "GROUP: /training/bin_200\n",
      "GROUP: /training/bin_225\n",
      "GROUP: /training/bin_250\n",
      "GROUP: /training/bin_275\n",
      "GROUP: /training/bin_300\n",
      "GROUP: /training/bin_50\n",
      "GROUP: /training/bin_75\n",
      "GROUP: /training/combined\n",
      "KEY: /training/bin_100/features\n",
      "KEY: /training/bin_100/labels\n",
      "KEY: /training/bin_125/features\n",
      "KEY: /training/bin_125/labels\n",
      "KEY: /training/bin_150/features\n",
      "KEY: /training/bin_150/labels\n",
      "KEY: /training/bin_175/features\n",
      "KEY: /training/bin_175/labels\n",
      "KEY: /training/bin_200/features\n",
      "KEY: /training/bin_200/labels\n",
      "KEY: /training/bin_225/features\n",
      "KEY: /training/bin_225/labels\n",
      "KEY: /training/bin_250/features\n",
      "KEY: /training/bin_250/labels\n",
      "KEY: /training/bin_275/features\n",
      "KEY: /training/bin_275/labels\n",
      "KEY: /training/bin_300/features\n",
      "KEY: /training/bin_300/labels\n",
      "KEY: /training/bin_50/features\n",
      "KEY: /training/bin_50/labels\n",
      "KEY: /training/bin_75/features\n",
      "KEY: /training/bin_75/labels\n",
      "KEY: /training/combined/features\n",
      "KEY: /training/combined/labels\n",
      "KEY: /testing/bin_100/features\n",
      "KEY: /testing/bin_100/labels\n",
      "KEY: /testing/bin_125/features\n",
      "KEY: /testing/bin_125/labels\n",
      "KEY: /testing/bin_150/features\n",
      "KEY: /testing/bin_150/labels\n",
      "KEY: /testing/bin_175/features\n",
      "KEY: /testing/bin_175/labels\n",
      "KEY: /testing/bin_200/features\n",
      "KEY: /testing/bin_200/labels\n",
      "KEY: /testing/bin_225/features\n",
      "KEY: /testing/bin_225/labels\n",
      "KEY: /testing/bin_250/features\n",
      "KEY: /testing/bin_250/labels\n",
      "KEY: /testing/bin_275/features\n",
      "KEY: /testing/bin_275/labels\n",
      "KEY: /testing/bin_300/features\n",
      "KEY: /testing/bin_300/labels\n",
      "KEY: /testing/bin_50/features\n",
      "KEY: /testing/bin_50/labels\n",
      "KEY: /testing/bin_75/features\n",
      "KEY: /testing/bin_75/labels\n",
      "KEY: /testing/combined/features\n",
      "KEY: /testing/combined/labels\n"
     ]
    }
   ],
   "source": [
    "# Reopen out hdf5 file with pandas so we can work with dataframes\n",
    "data_lake = pd.HDFStore(output_file)\n",
    "\n",
    "for (path, subgroups, subkeys) in data_lake.walk():\n",
    "\n",
    "    for subgroup in subgroups:\n",
    "\n",
    "        print(\"GROUP: {}/{}\".format(path, subgroup))\n",
    "\n",
    "    for subkey in subkeys:\n",
    "\n",
    "        key = \"/\".join([path, subkey])\n",
    "\n",
    "        print(\"KEY: {}\".format(key))\n",
    "\n",
    "data_lake.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, I think we are happy with this to start with. Let's move on and do some feature engineering in the bins."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
