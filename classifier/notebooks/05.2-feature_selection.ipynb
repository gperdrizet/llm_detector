{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "\n",
    "Training data contains a lot of correlated, redundant, un-trainable or otherwise likely unnecessary features. Plan here is to work through some standard feature selection techniques from scikit-learn to see if we can come up with a good, minimal feature-set to cary forward for other experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change working directory to parent so we can import as we would from __main__.py\n",
    "print(f'Working directory: ', end = '')\n",
    "%cd ..\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest, f_classif\n",
    "\n",
    "import functions.notebook_helper as helper_funcs\n",
    "import functions.notebook_plotting as plot_funcs\n",
    "import configuration as config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset - omit the file extension, it will be\n",
    "# added appropriately for the input and output files\n",
    "dataset_name = 'falcon-7b_scores_v2_10-300_words'\n",
    "\n",
    "# Input file path\n",
    "working_hdf5_file = f'{config.DATA_PATH}/{dataset_name}_stage_I_experimental.h5'\n",
    "\n",
    "# Open a connection to the hdf5 dataset via PyTables with Pandas so we can\n",
    "# load the data from each bin as a dataframe\n",
    "data_lake = pd.HDFStore(working_hdf5_file)\n",
    "\n",
    "# Load bin data\n",
    "data_df = data_lake[f'master']\n",
    "\n",
    "# Take small sample for rapid development and testing\n",
    "data_df = data_df.sample(n = 5000)\n",
    "\n",
    "# Close hdf5 connection\n",
    "data_lake.close()\n",
    "\n",
    "data_df.info()\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's get the data into shape to train a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing\n",
    "training_data_df = data_df.sample(frac = 0.7, random_state = 42)\n",
    "testing_data_df = data_df.drop(training_data_df.index)\n",
    "\n",
    "# Set length threshold\n",
    "training_data_df = training_data_df[training_data_df['Fragment length (words)'] > 50].copy()\n",
    "testing_data_df = testing_data_df[testing_data_df['Fragment length (words)'] > 50].copy()\n",
    "\n",
    "# Remove rows containing NAN\n",
    "training_data_df.dropna(inplace = True)\n",
    "testing_data_df.dropna(inplace = True)\n",
    "\n",
    "# Drop unnecessary and un-trainable\n",
    "feature_drops = [\n",
    "    'Fragment ID',\n",
    "    'Source record num',\n",
    "    'Dataset',\n",
    "    'Generator',\n",
    "    'String',\n",
    "    'Reader time (seconds)',\n",
    "    'Writer time (seconds)',\n",
    "    'Reader peak memory (GB)',\n",
    "    'Writer peak memory (GB)'\n",
    "]\n",
    "\n",
    "training_data_df.drop(feature_drops, axis = 1, inplace = True)\n",
    "testing_data_df.drop(feature_drops, axis = 1, inplace = True)\n",
    "\n",
    "# Split the data into features and labels\n",
    "labels_train = training_data_df['Source']\n",
    "features_train_df = training_data_df.drop('Source', axis = 1)\n",
    "\n",
    "labels_test = testing_data_df['Source']\n",
    "features_test_df = testing_data_df.drop('Source', axis = 1)\n",
    "\n",
    "# Encode string class values as integers\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(labels_train)\n",
    "labels_train = label_encoder.transform(labels_train)\n",
    "labels_test = label_encoder.transform(labels_test)\n",
    "\n",
    "print(f'Training data: {len(features_train_df)} examples')\n",
    "print(f'Test data: {len(features_test_df)} examples')\n",
    "\n",
    "# Grab the original feature names for future reference\n",
    "feature_column_names = features_train_df.columns\n",
    "feature_column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, standard scale the data. This is not strictly necessary for a tree-based classifier, but it will be helpful when eyeballing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_df, features_test_df = helper_funcs.standard_scale_data(features_train_df, features_test_df, feature_column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the features we are working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_funcs.plot_feature_distributions(features_train_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_funcs.plot_cross_correlation_matrix(features_train_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_funcs.plot_scatter_matrix(features_train_df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun let's add some synthetic features to the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_features_train, poly_features_test = helper_funcs.add_poly_features(features_train_df, features_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spline_features_train, spline_features_test = helper_funcs.add_spline_features(features_train_df, features_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Univariate feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Mutual information\n",
    "#### 2.1.1 Original feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = mutual_info_classif(features_train_df, labels_train)\n",
    "\n",
    "univariate_mi_results = {}\n",
    "\n",
    "for feature, result in zip(feature_column_names, results):\n",
    "    univariate_mi_results[feature] = result\n",
    "\n",
    "univariate_mi_results = dict(sorted(univariate_mi_results.items(), key = lambda item: item[1]))\n",
    "\n",
    "plt.title('Univariate feature mutual information: original features')\n",
    "\n",
    "plt.barh(\n",
    "    np.arange(len(univariate_mi_results.values())),\n",
    "    univariate_mi_results.values(),\n",
    "    tick_label = list(univariate_mi_results.keys())\n",
    ")\n",
    "\n",
    "plt.xlabel('Mutual information (nats)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. Spline synthetic feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = mutual_info_classif(spline_features_train, labels_train)\n",
    "\n",
    "univariate_mi_results = {}\n",
    "\n",
    "for feature, result in enumerate(results):\n",
    "    univariate_mi_results[feature] = result\n",
    "\n",
    "univariate_mi_results = dict(sorted(univariate_mi_results.items(), key = lambda item: item[1]))\n",
    "\n",
    "plt.title('Univariate feature mutual information: spline features')\n",
    "\n",
    "plt.scatter(\n",
    "    univariate_mi_results.values(),\n",
    "    np.arange(len(univariate_mi_results.values()))\n",
    ")\n",
    "\n",
    "plt.xlabel('Mutual information (nats)')\n",
    "plt.ylabel('Feature')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3. Polynomial synthetic feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = mutual_info_classif(poly_features_train, labels_train)\n",
    "\n",
    "univariate_mi_results = {}\n",
    "\n",
    "for feature, result in enumerate(results):\n",
    "    univariate_mi_results[feature] = result\n",
    "\n",
    "univariate_mi_results = dict(sorted(univariate_mi_results.items(), key=lambda item: item[1]))\n",
    "\n",
    "plt.title('Univariate feature mutual information: polynomial features')\n",
    "\n",
    "plt.scatter(\n",
    "    univariate_mi_results.values(),\n",
    "    np.arange(len(univariate_mi_results.values()))\n",
    ")\n",
    "\n",
    "plt.xlabel('Mutual information (nats)')\n",
    "plt.ylabel('Feature')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. F-Test\n",
    "#### 2.2.1. Original feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(f_classif, k = 4)\n",
    "selector.fit(features_train_df, labels_train)\n",
    "\n",
    "univariate_ftest_results = {}\n",
    "\n",
    "for feature, result in zip(feature_column_names, selector.pvalues_):\n",
    "    univariate_ftest_results[feature] = result\n",
    "\n",
    "univariate_ftest_results = dict(reversed(sorted(univariate_ftest_results.items(), key = lambda item: item[1])))\n",
    "\n",
    "plt.title('Univariate feature F-Test: original features')\n",
    "\n",
    "plt.barh(\n",
    "    np.arange(len(univariate_ftest_results.values())),\n",
    "    univariate_ftest_results.values(),\n",
    "   \n",
    "    tick_label=list(univariate_ftest_results.keys())\n",
    ")\n",
    "plt.xscale('log')\n",
    "plt.xlabel('p-value')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Spline synthetic feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(f_classif, k = 4)\n",
    "selector.fit(spline_features_train, labels_train)\n",
    "\n",
    "univariate_ftest_results = {}\n",
    "\n",
    "for feature, result in enumerate(selector.pvalues_):\n",
    "    univariate_ftest_results[feature] = result\n",
    "\n",
    "univariate_ftest_results = dict(reversed(sorted(univariate_ftest_results.items(), key = lambda item: item[1])))\n",
    "\n",
    "plt.title('Univariate feature F-Test: spline features')\n",
    "\n",
    "plt.scatter(\n",
    "    univariate_ftest_results.values(),\n",
    "    np.arange(len(univariate_ftest_results.values()))\n",
    ")\n",
    "plt.xscale('log')\n",
    "plt.xlabel('p-value')\n",
    "plt.ylabel('Feature')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. Polynomial synthetic feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(f_classif, k = 4)\n",
    "selector.fit(poly_features_train, labels_train)\n",
    "\n",
    "univariate_ftest_results = {}\n",
    "\n",
    "for feature, result in enumerate(selector.pvalues_):\n",
    "    univariate_ftest_results[feature] = result\n",
    "\n",
    "univariate_ftest_results = dict(reversed(sorted(univariate_ftest_results.items(), key = lambda item: item[1])))\n",
    "\n",
    "plt.title('Univariate feature F-Test: polynomial features')\n",
    "\n",
    "plt.scatter(\n",
    "    univariate_ftest_results.values(),\n",
    "    np.arange(len(univariate_ftest_results.values()))\n",
    ")\n",
    "plt.xscale('log')\n",
    "plt.xlabel('p-value')\n",
    "plt.ylabel('Feature')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, cool - both methods seem to agree and many of our features are clearly significant. Question is now, do we need all of them? We already know we have some highly correlated/similar features that we could probably afford to loose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Recursive feature elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_folds = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Original feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv, cv_results, plt = helper_funcs.recursive_feature_elimination(\n",
    "    features_train_df = features_train_df,\n",
    "    labels_train = labels_train,\n",
    "    cv_folds = cv_folds,\n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "optimal_feature_count = rfecv.n_features_\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Spline synthetic feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spline_rfecv, spline_cv_results, spline_plt = helper_funcs.recursive_feature_elimination(\n",
    "    features_train_df = spline_features_train, \n",
    "    labels_train = labels_train, \n",
    "    cv_folds = cv_folds,\n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "optimal_spline_feature_count = spline_rfecv.n_features_\n",
    "spline_plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Polynomial synthetic feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_rfecv, poly_cv_results, poly_plt = helper_funcs.recursive_feature_elimination(\n",
    "    features_train_df = poly_features_train, \n",
    "    labels_train = labels_train, \n",
    "    cv_folds = cv_folds,\n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "optimal_poly_feature_count = poly_rfecv.n_features_\n",
    "poly_plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reduced feature set evaluation\n",
    "\n",
    "After a bunch of fiddling around, it seems like the sweet spot for number of features is around 7 or 8 for the original set and 30 or so for the polynomial set. Though, this seems highly dependent on the number of cross-validation folds and the split method. Also, *RFECV* and *SequentialFeatureSelector* seem to often disagree on exactly what those features should be. I think it's time to generate datasets with those numbers of features and train and evaluate some classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make evaluation metrics scorers\n",
    "scoring = {\n",
    "    'binary_cross_entropy': make_scorer(helper_funcs.binary_cross_entropy), \n",
    "    'accuracy': make_scorer(helper_funcs.percent_accuracy),\n",
    "    'false_positive_rate': make_scorer(helper_funcs.false_positive_rate),\n",
    "    'false_negative_rate': make_scorer(helper_funcs.false_negative_rate)\n",
    "}\n",
    "\n",
    "# Dictionary to hold results\n",
    "results = {\n",
    "    'Fold': [],\n",
    "    'Condition': [],\n",
    "    'Fit time (sec.)': [],\n",
    "    'Accuracy (%)': [],\n",
    "    'False positive rate': [],\n",
    "    'False negative rate': [],\n",
    "    'Binary cross-entropy': []\n",
    "}\n",
    "\n",
    "# Plots to draw\n",
    "plots = ['Fit time (sec.)', 'Accuracy (%)', 'False positive rate', 'False negative rate', 'Binary cross-entropy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Original feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The feature set sizes to test\n",
    "feature_set_sizes = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "\n",
    "# Cross validation folds to run\n",
    "cv_folds = 5\n",
    "\n",
    "# Loop on the feature set sizes\n",
    "for feature_set_size in feature_set_sizes:\n",
    "\n",
    "    # Run sequential feature selection for this feature set size\n",
    "    sfs, fitted_sfs = helper_funcs.sequential_feature_selection(\n",
    "        features_train = features_train_df, \n",
    "        labels_train = labels_train, \n",
    "        feature_count = feature_set_size, \n",
    "        cv_folds = cv_folds,\n",
    "        n_jobs = -1\n",
    "    )\n",
    "\n",
    "    feature_set_train = fitted_sfs.transform(features_train_df)\n",
    "    print(f'Data shape: {feature_set_train.shape}')\n",
    "\n",
    "    # Instantiate an XGBoost model using the sklearn API\n",
    "    model = XGBClassifier()\n",
    "\n",
    "    # Run cross-validation\n",
    "    scores = cross_validate(\n",
    "        model,\n",
    "        feature_set_train,\n",
    "        labels_train,\n",
    "        cv = cv_folds,\n",
    "        n_jobs = -1,\n",
    "        scoring = scoring\n",
    "    )\n",
    "\n",
    "    # Collect the results\n",
    "    results = helper_funcs.add_cv_scores(results, scores, f'{feature_set_size} features')\n",
    "\n",
    "plot_funcs.plot_cross_validation(plots, results).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Spline synthetic feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the results dictionary\n",
    "results = {\n",
    "    'Fold': [],\n",
    "    'Condition': [],\n",
    "    'Fit time (sec.)': [],\n",
    "    'Accuracy (%)': [],\n",
    "    'False positive rate': [],\n",
    "    'False negative rate': [],\n",
    "    'Binary cross-entropy': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The feature set sizes to test\n",
    "feature_set_sizes = np.arange(optimal_spline_feature_count - 5, optimal_spline_feature_count + 5)\n",
    "\n",
    "# Loop on the feature set sizes\n",
    "for feature_set_size in feature_set_sizes:\n",
    "\n",
    "    # Run sequential feature selection for this feature set size\n",
    "    sfs, fitted_sfs = helper_funcs.sequential_feature_selection(\n",
    "        features_train = spline_features_train, \n",
    "        labels_train = labels_train, \n",
    "        feature_count = feature_set_size, \n",
    "        cv_folds = cv_folds,\n",
    "        n_jobs = -1\n",
    "    )\n",
    "\n",
    "    feature_set_train = fitted_sfs.transform(spline_features_train)\n",
    "    print(f'Data shape: {feature_set_train.shape}')\n",
    "\n",
    "    # Instantiate an XGBoost model using the sklearn API\n",
    "    model = XGBClassifier()\n",
    "\n",
    "    # Run cross-validation\n",
    "    scores = cross_validate(\n",
    "        model,\n",
    "        feature_set_train,\n",
    "        labels_train,\n",
    "        cv = cv_folds,\n",
    "        n_jobs = -1,\n",
    "        scoring = scoring\n",
    "    )\n",
    "\n",
    "    # Collect the results\n",
    "    results = helper_funcs.add_cv_scores(results, scores, f'{feature_set_size} features')\n",
    "\n",
    "plot_funcs.plot_cross_validation(plots, results).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Polynomial synthetic feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the results dictionary\n",
    "results = {\n",
    "    'Fold': [],\n",
    "    'Condition': [],\n",
    "    'Fit time (sec.)': [],\n",
    "    'Accuracy (%)': [],\n",
    "    'False positive rate': [],\n",
    "    'False negative rate': [],\n",
    "    'Binary cross-entropy': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The feature set sizes to test\n",
    "feature_set_sizes = np.arange(optimal_poly_feature_count - 5, optimal_poly_feature_count + 5)\n",
    "\n",
    "# Loop on the feature set sizes\n",
    "for feature_set_size in feature_set_sizes:\n",
    "\n",
    "    # Run sequential feature selection for this feature set size\n",
    "    sfs, fitted_sfs = helper_funcs.sequential_feature_selection(\n",
    "        features_train = poly_features_train, \n",
    "        labels_train = labels_train, \n",
    "        feature_count = feature_set_size, \n",
    "        cv_folds = cv_folds,\n",
    "        n_jobs = -1\n",
    "    )\n",
    "\n",
    "    feature_set_train = fitted_sfs.transform(poly_features_train)\n",
    "    print(f'Data shape: {feature_set_train.shape}')\n",
    "\n",
    "    # Instantiate an XGBoost model using the sklearn API\n",
    "    model = XGBClassifier()\n",
    "\n",
    "    # Run cross-validation\n",
    "    scores = cross_validate(\n",
    "        model,\n",
    "        feature_set_train,\n",
    "        labels_train,\n",
    "        cv = cv_folds,\n",
    "        n_jobs = -1,\n",
    "        scoring = scoring\n",
    "    )\n",
    "\n",
    "    # Collect the results\n",
    "    results = helper_funcs.add_cv_scores(results, scores, f'{feature_set_size} features')\n",
    "\n",
    "plot_funcs.plot_cross_validation(plots, results).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
