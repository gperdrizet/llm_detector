{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost classifier experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/arkk/llm_detector/classifier\n"
     ]
    }
   ],
   "source": [
    "# Change working directory to parent so we can import as we would from main.py\n",
    "%cd ..\n",
    "\n",
    "import xgboost\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_validate, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from hyperopt import fmin, tpe, hp, anneal, Trials\n",
    "\n",
    "import configuration as config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training data\n",
    "Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text fragments are 73.34373800263734% human\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 119818 entries, 0 to 119817\n",
      "Data columns (total 12 columns):\n",
      " #   Column                                   Non-Null Count   Dtype  \n",
      "---  ------                                   --------------   -----  \n",
      " 0   Fragment length (tokens)                 119818 non-null  int64  \n",
      " 1   Dataset                                  119818 non-null  object \n",
      " 2   Source                                   119818 non-null  object \n",
      " 3   String                                   119818 non-null  object \n",
      " 4   Perplexity                               119818 non-null  float64\n",
      " 5   Cross-perplexity                         119818 non-null  float64\n",
      " 6   Perplexity ratio score                   119818 non-null  float64\n",
      " 7   Perplexity ratio Kullback-Leibler score  119730 non-null  float64\n",
      " 8   Human TF-IDF                             119818 non-null  float64\n",
      " 9   Synthetic TF-IDF                         119818 non-null  float64\n",
      " 10  TF-IDF score                             119818 non-null  float64\n",
      " 11  TF-IDF Kullback-Leibler score            119804 non-null  float64\n",
      "dtypes: float64(8), int64(1), object(3)\n",
      "memory usage: 11.9+ MB\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fragment length (tokens)</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Source</th>\n",
       "      <th>String</th>\n",
       "      <th>Perplexity</th>\n",
       "      <th>Cross-perplexity</th>\n",
       "      <th>Perplexity ratio score</th>\n",
       "      <th>Perplexity ratio Kullback-Leibler score</th>\n",
       "      <th>Human TF-IDF</th>\n",
       "      <th>Synthetic TF-IDF</th>\n",
       "      <th>TF-IDF score</th>\n",
       "      <th>TF-IDF Kullback-Leibler score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>229</td>\n",
       "      <td>cc_news-falcon7</td>\n",
       "      <td>human</td>\n",
       "      <td>H. B. Fuller Company (NYSE:FUL) VP Traci L. Je...</td>\n",
       "      <td>0.971085</td>\n",
       "      <td>1.146746</td>\n",
       "      <td>0.846818</td>\n",
       "      <td>1.498175</td>\n",
       "      <td>-4.154520</td>\n",
       "      <td>-3.375011</td>\n",
       "      <td>5.869343</td>\n",
       "      <td>0.022587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>267</td>\n",
       "      <td>cc_news-falcon7</td>\n",
       "      <td>human</td>\n",
       "      <td>specialty chemicals company reported $0.65 ear...</td>\n",
       "      <td>1.185710</td>\n",
       "      <td>1.238428</td>\n",
       "      <td>0.957431</td>\n",
       "      <td>0.189686</td>\n",
       "      <td>-4.439004</td>\n",
       "      <td>-3.483652</td>\n",
       "      <td>7.568930</td>\n",
       "      <td>0.020660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>290</td>\n",
       "      <td>cc_news-falcon7</td>\n",
       "      <td>human</td>\n",
       "      <td>reposted in violation of US and international ...</td>\n",
       "      <td>1.212603</td>\n",
       "      <td>1.270731</td>\n",
       "      <td>0.954256</td>\n",
       "      <td>0.198995</td>\n",
       "      <td>-4.464970</td>\n",
       "      <td>-3.488189</td>\n",
       "      <td>7.768490</td>\n",
       "      <td>0.020301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156</td>\n",
       "      <td>cc_news-falcon7</td>\n",
       "      <td>human</td>\n",
       "      <td>one has issued a buy rating to the company. Th...</td>\n",
       "      <td>1.285289</td>\n",
       "      <td>1.332959</td>\n",
       "      <td>0.964237</td>\n",
       "      <td>0.174337</td>\n",
       "      <td>-4.764929</td>\n",
       "      <td>-3.801451</td>\n",
       "      <td>8.253517</td>\n",
       "      <td>0.019349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>231</td>\n",
       "      <td>cc_news-falcon7</td>\n",
       "      <td>human</td>\n",
       "      <td>specialty chemicals company’s stock valued at ...</td>\n",
       "      <td>1.272878</td>\n",
       "      <td>1.475141</td>\n",
       "      <td>0.862886</td>\n",
       "      <td>1.220695</td>\n",
       "      <td>-4.019600</td>\n",
       "      <td>-3.178213</td>\n",
       "      <td>6.056150</td>\n",
       "      <td>0.022488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fragment length (tokens)          Dataset Source  \\\n",
       "0                       229  cc_news-falcon7  human   \n",
       "1                       267  cc_news-falcon7  human   \n",
       "2                       290  cc_news-falcon7  human   \n",
       "3                       156  cc_news-falcon7  human   \n",
       "4                       231  cc_news-falcon7  human   \n",
       "\n",
       "                                              String  Perplexity  \\\n",
       "0  H. B. Fuller Company (NYSE:FUL) VP Traci L. Je...    0.971085   \n",
       "1  specialty chemicals company reported $0.65 ear...    1.185710   \n",
       "2  reposted in violation of US and international ...    1.212603   \n",
       "3  one has issued a buy rating to the company. Th...    1.285289   \n",
       "4  specialty chemicals company’s stock valued at ...    1.272878   \n",
       "\n",
       "   Cross-perplexity  Perplexity ratio score  \\\n",
       "0          1.146746                0.846818   \n",
       "1          1.238428                0.957431   \n",
       "2          1.270731                0.954256   \n",
       "3          1.332959                0.964237   \n",
       "4          1.475141                0.862886   \n",
       "\n",
       "   Perplexity ratio Kullback-Leibler score  Human TF-IDF  Synthetic TF-IDF  \\\n",
       "0                                 1.498175     -4.154520         -3.375011   \n",
       "1                                 0.189686     -4.439004         -3.483652   \n",
       "2                                 0.198995     -4.464970         -3.488189   \n",
       "3                                 0.174337     -4.764929         -3.801451   \n",
       "4                                 1.220695     -4.019600         -3.178213   \n",
       "\n",
       "   TF-IDF score  TF-IDF Kullback-Leibler score  \n",
       "0      5.869343                       0.022587  \n",
       "1      7.568930                       0.020660  \n",
       "2      7.768490                       0.020301  \n",
       "3      8.253517                       0.019349  \n",
       "4      6.056150                       0.022488  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the raw training data\n",
    "data_df = pd.read_json(config.COMBINED_SCORED_HANS_DATA_PR_TFIDF)\n",
    "\n",
    "percent_human_fragments = (len(data_df[data_df['Source'] == 'human']) / len(data_df)) * 100\n",
    "print(f'Text fragments are {percent_human_fragments}% human\\n')\n",
    "\n",
    "data_df.info()\n",
    "print()\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [('human', 0), ('human', 0), ('human', 0), ('human', 0), ('human', 0), ('human', 0), ('synthetic', 1), ('synthetic', 1), ('human', 0), ('human', 0)]\n"
     ]
    }
   ],
   "source": [
    "# Split the data into features and labels\n",
    "labels = data_df['Source']\n",
    "features = data_df.drop('Source', axis = 1)\n",
    "\n",
    "# Encode string class values as integers\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(labels)\n",
    "encoded_labels = label_encoder.transform(labels)\n",
    "print(f'Labels: {list(zip(labels[:10], encoded_labels[:10]))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 83872 examples\n",
      "Test data: 35946 examples\n",
      "\n",
      "Training features:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 83872 entries, 48609 to 68268\n",
      "Data columns (total 11 columns):\n",
      " #   Column                                   Non-Null Count  Dtype  \n",
      "---  ------                                   --------------  -----  \n",
      " 0   Fragment length (tokens)                 83872 non-null  int64  \n",
      " 1   Dataset                                  83872 non-null  object \n",
      " 2   String                                   83872 non-null  object \n",
      " 3   Perplexity                               83872 non-null  float64\n",
      " 4   Cross-perplexity                         83872 non-null  float64\n",
      " 5   Perplexity ratio score                   83872 non-null  float64\n",
      " 6   Perplexity ratio Kullback-Leibler score  83808 non-null  float64\n",
      " 7   Human TF-IDF                             83872 non-null  float64\n",
      " 8   Synthetic TF-IDF                         83872 non-null  float64\n",
      " 9   TF-IDF score                             83872 non-null  float64\n",
      " 10  TF-IDF Kullback-Leibler score            83861 non-null  float64\n",
      "dtypes: float64(8), int64(1), object(2)\n",
      "memory usage: 7.7+ MB\n",
      "None\n",
      "\n",
      "Training labels:\n",
      "[0 1 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Split into training and test sets\n",
    "features_train_df, features_test_df, labels_train, labels_test = train_test_split(features, encoded_labels, test_size = 0.3, random_state = 0)\n",
    "\n",
    "# Keep dataframe copy for easy manipulation later and make a numpy copy for training\n",
    "# without the dataset or string columns\n",
    "features_train = features_train_df.drop(['Dataset', 'String'], axis = 1).to_numpy()\n",
    "features_test = features_test_df.drop(['Dataset', 'String'], axis = 1).to_numpy()\n",
    "# labels_train = labels_train_df.to_numpy()\n",
    "# labels_test = labels_test_df.to_numpy()\n",
    "\n",
    "\n",
    "print(f'Training data: {len(labels_train)} examples')\n",
    "print(f'Test data: {len(features_test)} examples')\n",
    "print()\n",
    "print('Training features:')\n",
    "print(features_train_df.info())\n",
    "print()\n",
    "print('Training labels:')\n",
    "print(labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Off-the-shelf XGBoost classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 DMLC XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \n",
    "\n",
    "# Fit model on training data\n",
    "model = xgboost.XGBClassifier()\n",
    "model.fit(features_train, labels_train)\n",
    "\n",
    "# Make predictions for test data\n",
    "y_pred = model.predict(features_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# Evaluate predictions\n",
    "accuracy = accuracy_score(labels_test, predictions)\n",
    "print('\\nAccuracy: %.1f%%' % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(labels_test, predictions)\n",
    "print('\\nConfusion matrix:')\n",
    "print(cm)\n",
    "\n",
    "# Normalize confusion matrix\n",
    "print(f'\\nNormalized confusion matrix:')\n",
    "normalized_cm = cm / sum(sum(cm))\n",
    "print(normalized_cm)\n",
    "print()\n",
    "\n",
    "# Plot the confusion matrix\n",
    "_ = ConfusionMatrixDisplay.from_estimator(model, features_test, labels_test, normalize = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost: k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validate the model\n",
    "model = xgboost.XGBClassifier()\n",
    "kfold = KFold(n_splits = 10, shuffle = True, random_state = 7)\n",
    "results = cross_validate(model, features_train, labels_train, cv = kfold, return_train_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit for real on all training data\n",
    "model.fit(features_train, labels_train)\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred = model.predict(features_test)\n",
    "accuracy = accuracy_score(labels_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('10-fold CV accuracy: %.2f%% (%.3f%%)' % (results['test_score'].mean() * 100, results['test_score'].std() * 100))\n",
    "print('Hold out test set accuracy: %.2f%%' % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, pretty good - we are not over-fitting or just guessing the major class, which is good. For reference, guessing human all the time would give us ~76% accuracy. Let's step it up and see if we can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_cv(\n",
    "      params: dict = None, \n",
    "      random_state: int = 42, \n",
    "      kfolds: int = 10,\n",
    "      fold_split: float = 0.5,\n",
    "      features: np.ndarray = None, \n",
    "      labels: np.ndarray = None\n",
    ") -> float:\n",
    "   \n",
    "   '''Cross validate an XGBoost classifier with a set of hyperparameters, returns mean CV core'''\n",
    "    \n",
    "   # Get the a set of variable parameters from params\n",
    "   params = {\n",
    "      'eta': params['eta'],\n",
    "      # 'gamma': params['gamma'],\n",
    "      'max_depth': int(params['max_depth']),\n",
    "      # 'min_child_weight': params['min_child_weight'],\n",
    "      # 'max_delta_step': int(params['max_delta_step']),\n",
    "      # 'subsample': int(params['subsample']),\n",
    "      # 'reg_alpha' : params['reg_alpha'],\n",
    "      # 'reg_lambda' : params['reg_lambda'],\n",
    "      'n_estimators': int(params['n_estimators'])\n",
    "   }\n",
    "\n",
    "   # Create a new XGB model using the values from params\n",
    "   model = xgboost.XGBClassifier(device = 'cuda', random_state = random_state, **params)\n",
    "\n",
    "   # Get number of examples in dataset\n",
    "   n = labels.shape[0]\n",
    "\n",
    "   # Set score to zero at start\n",
    "   score = 0\n",
    "\n",
    "   # Run k-fold with random samples\n",
    "   for k in range(kfolds):\n",
    "      \n",
    "      # Pick random indices without replacement for data to include in validation set\n",
    "      validation_indices = np.random.choice(range(n), size = (int(n*fold_split),), replace = False)    \n",
    "      validation_mask = np.zeros(n, dtype = bool)\n",
    "      validation_mask[validation_indices] = True\n",
    "      training_mask = ~validation_mask\n",
    "\n",
    "      labels_train = labels[training_mask]\n",
    "      features_train = features[training_mask]\n",
    "      labels_validation = labels[validation_mask]\n",
    "      features_validation = features[validation_mask]\n",
    "\n",
    "      # Move data to GPU\n",
    "      gpu_features_train = cp.array(features_train)\n",
    "      gpu_labels_train = cp.array(labels_train)\n",
    "      gpu_features_validation = cp.array(features_validation)\n",
    "   \n",
    "      # Fit the model\n",
    "      model.fit(gpu_features_train, gpu_labels_train)\n",
    "\n",
    "      # Make predictions for validation data\n",
    "      labels_predicted = model.predict(gpu_features_validation)\n",
    "\n",
    "      # Evaluate predictions, summing score across the folds\n",
    "      score += accuracy_score(labels_validation, labels_predicted)\n",
    "\n",
    "   # Return negated mean score for minimization\n",
    "   return -score / kfolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# How many iterations to run\n",
    "n_iter = 50\n",
    "\n",
    "# Fix random state\n",
    "random_state = 42\n",
    "\n",
    "# Parameter optimization space\n",
    "space = {\n",
    "       'eta': hp.uniform('eta', 0.001, 1.0),\n",
    "       # 'gamma': hp.quniform('gamma', 0, 10, 1),\n",
    "       'max_depth': hp.quniform('max_depth', 1, 100, 1),\n",
    "       # 'min_child_weight': hp.quniform('min_child_weight', 0, 10, 1),\n",
    "       # 'max_delta_step': hp.quniform('max_delta_step', 0, 10, 1),\n",
    "       # 'subsample': hp.uniform('subsample', 0, 1),\n",
    "       # 'reg_alpha': hp.quniform('reg_alpha', 0, 10, 1),\n",
    "       # 'reg_lambda': hp.uniform('reg_lambda', 0, 10),\n",
    "       'n_estimators': hp.quniform('n_estimators', 1, 100, 1)\n",
    "}\n",
    "\n",
    "# Set up trial logging\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(\n",
    "       fn = partial(\n",
    "              hyperopt_cv, \n",
    "              features = features_train, \n",
    "              labels = labels_train\n",
    "       ),\n",
    "       space = space,           # Parameter: value dictionary\n",
    "       algo = tpe.suggest,      # Optimization algorithm, hyperopt will select its parameters automatically\n",
    "       max_evals = n_iter,      # Maximum number of iterations\n",
    "       trials = trials,         # Logging\n",
    "       rstate = np.random.default_rng(random_state)\n",
    ")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Optimized parameters:')\n",
    "print(f\" Learning rate: {best['eta']}\")\n",
    "print(f\" Max tree depth: {best['max_depth']}\")\n",
    "print(f\" Estimators: {best['n_estimators']}\")\n",
    "\n",
    "# Fit on the complete training set with the winning parameters\n",
    "model = xgboost.XGBClassifier(\n",
    "    eta = best['eta'],\n",
    "    # gamma = best['gamma'],\n",
    "    max_depth = int(best['max_depth']),\n",
    "    # min_child_weight = best['min_child_weight'],\n",
    "    # max_delta_step = int(best['max_delta_step']),\n",
    "    # subsample = int(best['subsample']),\n",
    "    # reg_alpha = best['reg_alpha'],\n",
    "    # reg_lambda = best['reg_lambda'],\n",
    "    n_estimators = int(best['n_estimators']),\n",
    "    random_state = random_state,\n",
    "    device = 'cuda'\n",
    ")\n",
    "\n",
    "model.fit(cp.array(features_train), cp.array(labels_train))\n",
    "\n",
    "# Evaluate on test data\n",
    "labels_predicted = model.predict(cp.array(features_test))\n",
    "accuracy = accuracy_score(labels_test, labels_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('10-fold CV accuracy: %.2f%% (%.3f%%)' % (results['test_score'].mean() * 100, results['test_score'].std() * 100))\n",
    "print('Hold out test set accuracy: %.2f%%' % (accuracy * 100.0))\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(labels_test, labels_predicted)\n",
    "print('\\nConfusion matrix:')\n",
    "print(cm)\n",
    "\n",
    "# Normalize confusion matrix\n",
    "print(f'\\nNormalized confusion matrix:')\n",
    "normalized_cm = cm / sum(sum(cm))\n",
    "print(normalized_cm)\n",
    "print()\n",
    "\n",
    "# Plot the confusion matrix\n",
    "_ = ConfusionMatrixDisplay.from_estimator(model, features_test, labels_test, normalize = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpe_results = np.array([[\n",
    "    x['result']['loss'],\n",
    "    x['misc']['vals']['eta'][0],\n",
    "    # x['misc']['vals']['gamma'][0],\n",
    "    x['misc']['vals']['max_depth'][0],\n",
    "    # x['misc']['vals']['min_child_weight'][0],\n",
    "    # x['misc']['vals']['max_delta_step'][0],\n",
    "    # x['misc']['vals']['subsample'][0],\n",
    "    # x['misc']['vals']['reg_alpha'][0],\n",
    "    # x['misc']['vals']['reg_lambda'][0],\n",
    "    x['misc']['vals']['n_estimators'][0],\n",
    "\n",
    "] for x in trials.trials])\n",
    "\n",
    "tpe_results_df = pd.DataFrame(\n",
    "    tpe_results, \n",
    "    columns = [\n",
    "        'score', \n",
    "        'eta',\n",
    "        # 'gamma',\n",
    "        'max_depth',\n",
    "        # 'min_child_weight',\n",
    "        # 'max_delta_step',\n",
    "        # 'subsample',\n",
    "        # 'reg_alpha',\n",
    "        # 'reg_lambda',\n",
    "        'n_estimators'\n",
    "    ]\n",
    ")\n",
    "\n",
    "optimization_plot = tpe_results_df.plot(subplots = True, figsize = (10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where are we failing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, it feels like we have hit the limit of what the classifier is capable of. Doesn't seem like we will be able to get past 90% accuracy without changing/improving features and/or data. Let's take a look at the text fragments which are bing miss-classified and see if anything jumps out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error_indices = []\n",
    "i = 0\n",
    "n = 1\n",
    "\n",
    "for features, label, prediction in zip(features_test, labels_test, predictions):\n",
    "\n",
    "    print(f'\\nLabel = {label}, prediction = {prediction}', end = '')\n",
    "\n",
    "    if label != prediction:\n",
    "        print(' - error!', end = '')\n",
    "        test_error_indices.append(i)\n",
    "\n",
    "    n += 1\n",
    "    i += 1\n",
    "\n",
    "print(f'\\nHave {len(test_error_indices)} ({round((len(test_error_indices) / len(labels_predicted)) * 100, 1)}%) miss-classed text fragments from the test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover the miss-classed fragments\n",
    "errors = []\n",
    "test_strings = list(features_test_df['String'])\n",
    "\n",
    "for i in test_error_indices:\n",
    "    errors.append(test_strings[i])\n",
    "\n",
    "for error in errors[:10]:\n",
    "    print(f'{error}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, hard to draw any easy conclusions why these are being miss-classified. Some are short or very low entropy, but many are not. Maybe let's look at our features' distributions in the correctly and incorrectly classed fragments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = features_test_df.iloc[test_error_indices]\n",
    "ax = errors.plot.hist(bins=100, alpha=0.5, density=True, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = features_test_df.plot.hist(bins=100, alpha=0.5, density=True, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, nothing jumps right out at me... Maybe let's just do some general feature checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f = plt.figure(figsize=(19, 15))\n",
    "plt.matshow(features_test_df.drop(['String', 'Dataset'], axis = 1).corr())\n",
    "plt.xticks(range(features_test_df.select_dtypes(['number']).shape[1]), features_test_df.select_dtypes(['number']).columns, fontsize=14, rotation=90)\n",
    "plt.yticks(range(features_test_df.select_dtypes(['number']).shape[1]), features_test_df.select_dtypes(['number']).columns, fontsize=14)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "plt.title('Correlation Matrix', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so we do have some strongly correlated and anti-correlated features. Let's try just removing some of them and see if things improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drops = ['Dataset', 'String']#, 'Perplexity', 'Human TF-IDF', 'Perplexity ratio score', 'TF-IDF score', 'Synthetic TF-IDF', 'Cross-perplexity']\n",
    "\n",
    "trimmed_features_train_df = features_train_df.drop(drops, axis = 1)\n",
    "trimmed_features_test_df = features_test_df.drop(drops, axis = 1)\n",
    "\n",
    "f = plt.figure(figsize=(19, 15))\n",
    "plt.matshow(trimmed_features_test_df.corr())\n",
    "plt.xticks(range(trimmed_features_test_df.select_dtypes(['number']).shape[1]), trimmed_features_test_df.select_dtypes(['number']).columns, fontsize=14, rotation=90)\n",
    "plt.yticks(range(trimmed_features_test_df.select_dtypes(['number']).shape[1]), trimmed_features_test_df.select_dtypes(['number']).columns, fontsize=14)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "plt.title('Correlation Matrix', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model on training data\n",
    "model = xgboost.XGBClassifier()\n",
    "model.fit(trimmed_features_train_df, labels_train)\n",
    "\n",
    "# Make predictions for test data\n",
    "y_pred = model.predict(trimmed_features_test_df)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# Evaluate predictions\n",
    "accuracy = accuracy_score(labels_test, predictions)\n",
    "print('\\nAccuracy: %.1f%%' % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eh, dropping features doesn't make things convincingly better. Let's try some crack-head stuff, \"y'all got any more of them features?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Perplexity', 'Cross-perplexity', 'Perplexity ratio score', 'Perplexity ratio Kullback-Leibler score', 'Human TF-IDF', 'Synthetic TF-IDF', 'TF-IDF score', 'TF-IDF Kullback-Leibler score']\n",
    "\n",
    "for feature in features:\n",
    "    synthetic_feature = f'{feature} squared'\n",
    "    data_df[synthetic_feature] = data_df[feature] ** 2\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and labels\n",
    "labels = data_df['Source']\n",
    "features = data_df.drop('Source', axis = 1)\n",
    "\n",
    "# Encode string class values as integers\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(labels)\n",
    "encoded_labels = label_encoder.transform(labels)\n",
    "print(f'Labels: {list(zip(labels[:10], encoded_labels[:10]))}')\n",
    "\n",
    "# Split into training and test sets\n",
    "features_train_df, features_test_df, labels_train, labels_test = train_test_split(features, encoded_labels, test_size = 0.3, random_state = 0)\n",
    "\n",
    "# Keep dataframe copy for easy manipulation later and make a numpy copy for training\n",
    "# without the dataset or string columns\n",
    "features_train = features_train_df.drop(['Dataset', 'String'], axis = 1).to_numpy()\n",
    "features_test = features_test_df.drop(['Dataset', 'String'], axis = 1).to_numpy()\n",
    "\n",
    "print()\n",
    "print(features_train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model on training data\n",
    "model = xgboost.XGBClassifier()\n",
    "model.fit(features_train, labels_train)\n",
    "\n",
    "# Make predictions for test data\n",
    "y_pred = model.predict(features_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# Evaluate predictions\n",
    "accuracy = accuracy_score(labels_test, predictions)\n",
    "print('\\nAccuracy: %.1f%%' % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, apparently sklearn has this built in..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate the types of features created\n",
    "from numpy import asarray\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Get rid of nan\n",
    "features.dropna(axis = 1, inplace = True)\n",
    "\n",
    "# perform a polynomial features transform of the dataset\n",
    "trans = PolynomialFeatures(degree = 3)\n",
    "poly_features = trans.fit_transform(features.drop(['Dataset', 'String'], axis = 1))\n",
    "\n",
    "# Split into training and test sets\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(poly_features, encoded_labels, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model on training data\n",
    "model = xgboost.XGBClassifier(device = 'cuda')\n",
    "model.fit(cp.array(features_train), cp.array(labels_train))\n",
    "\n",
    "# Make predictions for test data\n",
    "y_pred = model.predict(cp.array(features_test))\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# Evaluate predictions\n",
    "accuracy = accuracy_score(labels_test, predictions)\n",
    "print('\\nAccuracy: %.1f%%' % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope, OK, need to move on here. Looks like there is no easy way to improve... We can revisit this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
