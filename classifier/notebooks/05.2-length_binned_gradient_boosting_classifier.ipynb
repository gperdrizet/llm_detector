{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Length binned gradient boosting classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Run setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /mnt/arkk/llm_detector/classifier\n"
     ]
    }
   ],
   "source": [
    "# Change working directory to parent so we can import as we would from main.py\n",
    "print(f'Working directory: ', end = '')\n",
    "%cd ..\n",
    "\n",
    "import pickle\n",
    "\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import configuration as config\n",
    "import functions.notebook_helper as helper_funcs\n",
    "import functions.notebook_plotting as plot_funcs\n",
    "import functions.parallel_xgboost as xgb_funcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some run parameters and decide whether to run or load data for each experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of folds to run for cross validation\n",
    "cv_folds = 5\n",
    "workers = 3\n",
    "hyperparameter_iterations = 6\n",
    "\n",
    "rerun_cross_validation = True\n",
    "rerun_cross_validation_control = False\n",
    "rerun_hyperparameter_optimization = False\n",
    "\n",
    "cross_validation_results_filename = f'{config.DATA_PATH}/cross_validation_results_{cv_folds}_folds.pkl'\n",
    "cross_validation_control_results_filename = f'{config.DATA_PATH}/cross_validation_control_results_{cv_folds}_folds.pkl'\n",
    "hyperparameter_optimization_results_filename = f'{config.DATA_PATH}/hyperparameter_optimization_results_{hyperparameter_iterations}_iterations.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get ready to read the input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset to train on - omit the file extension, it will be added\n",
    "dataset_name = 'falcon-7b_scores_v2_10-300_words'\n",
    "\n",
    "# Input file path\n",
    "input_file = f'{config.DATA_PATH}/{dataset_name}.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also make a data structure to collect results for plotting as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to hold testing/experimentation results\n",
    "parsed_results = {\n",
    "    'Fold': [],\n",
    "    'Condition': [],\n",
    "    'Optimized': [],\n",
    "    'Accuracy (%)': [],\n",
    "    'False positive rate': [],\n",
    "    'False negative rate': [],\n",
    "    'Binary cross-entropy': []\n",
    "}\n",
    "\n",
    "# Plots to draw\n",
    "plots = ['Accuracy (%)', 'False positive rate', 'False negative rate', 'Binary cross-entropy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a dictionary of scoring functions to be used as metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make evaluation metrics scorers\n",
    "scoring_funcs = {\n",
    "    'binary_cross_entropy': make_scorer(helper_funcs.binary_cross_entropy), \n",
    "    'accuracy': make_scorer(helper_funcs.percent_accuracy),\n",
    "    'false_positive_rate': make_scorer(helper_funcs.false_positive_rate),\n",
    "    'false_negative_rate': make_scorer(helper_funcs.false_negative_rate)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stage I classifiers: baseline performance\n",
    "### 2.1. Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will assign each worker 6 CPUs\n",
      "Running cross-validation on bin_050 with 6 threads, CPUs: 0, 1, 2, 3, 4, 5\n",
      "Running cross-validation on bin_075 with 6 threads, CPUs: 6, 7, 8, 9, 10, 11\n",
      "Running cross-validation on bin_100 with 6 threads, CPUs: 12, 13, 14, 15, 16, 17\n",
      "Running cross-validation on bin_125 with 6 threads, CPUs: 0, 1, 2, 3, 4, 5\n",
      "Running cross-validation on bin_150 with 6 threads, CPUs: 6, 7, 8, 9, 10, 11\n",
      "Running cross-validation on bin_175 with 6 threads, CPUs: 12, 13, 14, 15, 16, 17\n",
      "Running cross-validation on bin_200 with 6 threads, CPUs: 0, 1, 2, 3, 4, 5\n",
      "Running cross-validation on bin_225 with 6 threads, CPUs: 6, 7, 8, 9, 10, 11\n",
      "Running cross-validation on bin_250 with 6 threads, CPUs: 12, 13, 14, 15, 16, 17\n",
      "Running cross-validation on bin_275 with 6 threads, CPUs: 0, 1, 2, 3, 4, 5\n",
      "Running cross-validation on bin_300 with 6 threads, CPUs: 6, 7, 8, 9, 10, 11\n",
      "Running cross-validation on combined with 6 threads, CPUs: 12, 13, 14, 15, 16, 17\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Source', 'String'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/mnt/arkk/llm_detector/classifier/functions/parallel_xgboost.py\", line 127, in cross_validate_bin\n    features = prep_data(\n  File \"/mnt/arkk/llm_detector/classifier/functions/parallel_xgboost.py\", line 155, in prep_data\n    features_df.drop(feature_drops, axis = 1, inplace = True)\n  File \"/mnt/arkk/llm_detector/.venv/lib/python3.8/site-packages/pandas/core/frame.py\", line 5258, in drop\n    return super().drop(\n  File \"/mnt/arkk/llm_detector/.venv/lib/python3.8/site-packages/pandas/core/generic.py\", line 4549, in drop\n    obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n  File \"/mnt/arkk/llm_detector/.venv/lib/python3.8/site-packages/pandas/core/generic.py\", line 4591, in _drop_axis\n    new_axis = axis.drop(labels, errors=errors)\n  File \"/mnt/arkk/llm_detector/.venv/lib/python3.8/site-packages/pandas/core/indexes/base.py\", line 6699, in drop\n    raise KeyError(f\"{list(labels[mask])} not found in axis\")\nKeyError: \"['Source', 'String'] not found in axis\"\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Re-run the cross-validation only if asked\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rerun_cross_validation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Do the cross-validation\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     parsed_results \u001b[38;5;241m=\u001b[39m \u001b[43mxgb_funcs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_validate_bins\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscoring_funcs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscoring_funcs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparsed_results\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparsed_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcv_folds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcv_folds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Save the result\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(cross_validation_results_filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m result_output_file:\n",
      "File \u001b[0;32m/mnt/arkk/llm_detector/classifier/functions/parallel_xgboost.py:94\u001b[0m, in \u001b[0;36mcross_validate_bins\u001b[0;34m(input_file, scoring_funcs, parsed_results, cv_folds, workers, shuffle_control)\u001b[0m\n\u001b[1;32m     91\u001b[0m pool\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Get the results\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m results \u001b[38;5;241m=\u001b[39m [async_result\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Add the results\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[0;32m/mnt/arkk/llm_detector/classifier/functions/parallel_xgboost.py:94\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     91\u001b[0m pool\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Get the results\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m results \u001b[38;5;241m=\u001b[39m [\u001b[43masync_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Add the results\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py:771\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Source', 'String'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Re-run the cross-validation only if asked\n",
    "if rerun_cross_validation is True:\n",
    "\n",
    "    # Do the cross-validation\n",
    "    parsed_results = xgb_funcs.cross_validate_bins(\n",
    "        input_file = input_file, \n",
    "        scoring_funcs = scoring_funcs, \n",
    "        parsed_results = parsed_results,\n",
    "        cv_folds = cv_folds,\n",
    "        workers = workers\n",
    "    )\n",
    "\n",
    "    # Save the result\n",
    "    with open(cross_validation_results_filename, 'wb') as result_output_file:\n",
    "        pickle.dump(parsed_results, result_output_file, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# If we don't want to re-run it, load the old data\n",
    "elif rerun_cross_validation is False:\n",
    "\n",
    "    with open(cross_validation_results_filename, 'rb') as result_input_file:\n",
    "        parsed_results = pickle.load(result_input_file)\n",
    "        print(f'Data loaded from previous run.')\n",
    "\n",
    "# Plot the results\n",
    "plot_funcs.plot_two_factor_cross_validation(plots, parsed_results).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Cross-validation: shuffled control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run the control cross-validation only if asked\n",
    "if rerun_cross_validation_control is True:\n",
    "\n",
    "    # Start a fresh results dictionary so we don't over-write our baseline results\n",
    "    parsed_control_results = {\n",
    "        'Fold': [],\n",
    "        'Condition': [],\n",
    "        'Optimized': [],\n",
    "        'Accuracy (%)': [],\n",
    "        'False positive rate': [],\n",
    "        'False negative rate': [],\n",
    "        'Binary cross-entropy': []\n",
    "    }\n",
    "\n",
    "    # Do the cross-validation\n",
    "    parsed_control_results = xgb_funcs.cross_validate_bins(\n",
    "        input_file = input_file, \n",
    "        scoring_funcs = scoring_funcs, \n",
    "        parsed_results = parsed_control_results,\n",
    "        cv_folds = cv_folds,\n",
    "        workers = workers, \n",
    "        shuffle_control = True\n",
    "    )\n",
    "\n",
    "    # Save the result\n",
    "    with open(cross_validation_control_results_filename, 'wb') as result_output_file:\n",
    "        pickle.dump(parsed_control_results, result_output_file, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# If we don't want to re-run it, load the old data\n",
    "elif rerun_cross_validation_control is False:\n",
    "\n",
    "    with open(cross_validation_control_results_filename, 'rb') as result_input_file:\n",
    "        parsed_control_results = pickle.load(result_input_file)\n",
    "        print(f'Data loaded from previous run.')\n",
    "\n",
    "# Plot the results\n",
    "plot_funcs.plot_two_factor_cross_validation(plots, parsed_control_results).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, cool! That works great - the real classifier is definitely performing better than the shuffled control. Lots of things to say here, so I'll make a list:\n",
    "\n",
    "1. The performance in the longer bins is impressive - we get to and well above 95% accuracy with no tuning or hyperparameter optimization at all.\n",
    "2. Performance on short text fragments is not great in comparison. We don't get to 90% accuracy until bin 75 which contains fragments between 26 and 75 words. One of the things I know people would want to use this for is spotting bot posts on social media. But i'm starting to think content moderation is a totally different problem all together.\n",
    "3. We may be leaving some accuracy on the table - remember, the bins overlap so each fragment is seen by two different classifiers. E.g. a 30 word fragment is scored by the classifiers for bins 50 and 75. Next step should probably be to combine the outputs to come up with the final classification.\n",
    "4. At this point I think we might benefit from more data - some of these bins have only a few hundred or a few thousand fragments in them. Unfortunately, the bins we are performing best on are the ones with the least data, but still...\n",
    "5. Cross-validation is smoking fast - using multiprocessing to parallelize over the bins and then a joblib context with a threading backend to give scikit-learn control over parallelism in the workers works great. Thought that was going to be a harder problem than it was.\n",
    "\n",
    "Stick with it, we are almost there. I swear all of this will be worth it in the end. Next, we need to do two things:\n",
    "\n",
    "1. Tune hyperparameters in each bin, saving the classifier for each after it has been trained using all of the training data with the winning parameters.\n",
    "2. Figure out how to combine the class probabilities coming from each bin into a single prediction.\n",
    "\n",
    "To help with thinking about that last point, let's draw a diagram for the first two bins:\n",
    "\n",
    "```text\n",
    "\n",
    "Fragment length (words): 1   5   10  15  25  30  35  40  45  50  55  60  65  70  75  80  85  90  95  100 105\n",
    "                         +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
    "                 bin 50: |1                                50|                   |                   |\n",
    "                         +-----------------------------------+                   |                   |\n",
    "                 bin 75:                 |26                                   75|                   |\n",
    "                                         +---------------------------------------+                   |\n",
    "                bin 100:                                     |51                                  100|\n",
    "                                                             +---------------------------------------+\n",
    "```\n",
    "\n",
    "OK, cool, that helped. So ignoring the start and end of the range where we will have some fragments that are only scored by one classifier, the solution is easy, the 'second stage' classifier is a second set of models each take a length range equal to each overlap between the bins. So for example, here's how it would work for a fragment of length 36 looking at the above diagram.\n",
    "\n",
    "1. Fragment gets scored by first stage bin 50 classifier.\n",
    "2. Fragment gets scored by first stage bin 75 classifier.\n",
    "3. Score from each classifier becomes new feature, maybe 'short score' and 'long score' or something similar.\n",
    "4. Dual scored fragment is sent to a second stage '26-50' classifier that takes all of the features, including the short and long scores from the first stage classifiers, to predict a final class probability. Bang. Done.\n",
    "\n",
    "Only other edge cases are fragments that are below 26 words or above 275 words. These are only scored by the first stage classifier for their bin and that class probability is used to make the call. For production, we can send a warning about short fragments with the prediction and for 'too long' fragments we can split and/or sample, something like that. Easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stage I classifiers: hyperparameter tuning\n",
    "### 3.1. Random search with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run or load hyperparameter optimization results as asked\n",
    "if rerun_hyperparameter_optimization is True:\n",
    "\n",
    "    # Define hyperparameter distributions for randomized grid search\n",
    "    parameter_distributions = {\n",
    "        'learning_rate': uniform(loc = 0.0001, scale = 0.9999),\n",
    "        'gamma': uniform(loc = 0.0, scale = 50.0),\n",
    "        'max_depth': randint(1, 500),\n",
    "        'min_child_weight': uniform(loc = 0.0001, scale = 0.9999),\n",
    "        'subsample': uniform(loc = 0.5, scale = 0.5),\n",
    "        'reg_alpha': uniform(loc = 0.0, scale = 1.0),\n",
    "        'reg_lambda': uniform(loc = 0.0, scale = 1.0),\n",
    "        'n_estimators': randint(1, 500),\n",
    "        'num_parallel_tree': randint(1, 200)\n",
    "    }\n",
    "\n",
    "    # Redefine the scoring functions, this time including negated binary\n",
    "    # cross-entropy so that when RandomizedSearchCV try to maximize it, it's\n",
    "    # doing the right thing.\n",
    "    scoring_funcs = {\n",
    "        'negated_binary_cross_entropy': make_scorer(helper_funcs.negated_binary_cross_entropy),\n",
    "        'binary_cross_entropy': make_scorer(helper_funcs.binary_cross_entropy), \n",
    "        'accuracy': make_scorer(helper_funcs.percent_accuracy),\n",
    "        'false_positive_rate': make_scorer(helper_funcs.false_positive_rate),\n",
    "        'false_negative_rate': make_scorer(helper_funcs.false_negative_rate)\n",
    "    }\n",
    "\n",
    "    # Do the optimization\n",
    "    results = xgb_funcs.hyperparameter_tune_bins(\n",
    "        input_file = input_file, \n",
    "        parameter_distributions = parameter_distributions, \n",
    "        scoring_funcs = scoring_funcs,\n",
    "        cv_folds = cv_folds,\n",
    "        n_iterations = hyperparameter_iterations, \n",
    "        workers = workers\n",
    "    )\n",
    "\n",
    "    # Save the result\n",
    "    with open(hyperparameter_optimization_results_filename, 'wb') as result_output_file:\n",
    "        pickle.dump(results, result_output_file, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# If we don't want to re-run it, load the old data\n",
    "elif rerun_hyperparameter_optimization is False:\n",
    "\n",
    "    with open(hyperparameter_optimization_results_filename, 'rb') as result_input_file:\n",
    "        results = pickle.load(result_input_file)\n",
    "        print(f'Data loaded from previous run.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "1. 1,000 iteration run went for 760 minutes without slowing down. No memory issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Hyperparameter optimization results results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the results\n",
    "winners, cv_results = xgb_funcs.parse_hyperparameter_tuning_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plot_funcs.plot_hyperparameter_tuning(cv_results).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Winning models comparison to baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to go through the hyperparameter optimization results and get\n",
    "# them formatted so we can add them to the results from the baseline cross-validation\n",
    "parsed_results = xgb_funcs.add_winners_to_parsed_results(\n",
    "    cv_results, \n",
    "    parsed_results, \n",
    "    cv_folds\n",
    ")\n",
    "\n",
    "plot_funcs.plot_two_factor_cross_validation(plots, parsed_results).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad - hyperparameter optimization gained us a few accuracy percentage points, especially in the short bins where we need it most. We did specify a pretty high-dimensional parameter space, so set's do a bigger, longer run with more iterations overnight tonight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Winning models evaluation on hold-out test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix for hold-out test data in each bin\n",
    "plot_funcs.plot_testing_confusion_matrices(winners, input_file).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, looks great! Performance on the test data is as good as or better than expected from cross-validation. Over fitting is not obviously a concern.\n",
    "\n",
    "Let's move on to setting up the stage II classifiers. Plan here is as described above - we need to:\n",
    "\n",
    "1. Add the class probability output from both applicable stage I classifiers to each fragment as a new training feature.\n",
    "2. Build and train a second set of classifiers that correspond to the bin overlaps and take the original features plus the class probabilities added in step 1.\n",
    "\n",
    "## 4. Stage I class probabilities feature addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xgb_funcs.add_stage_one_probabilities_features(\n",
    "    input_file, \n",
    "    hyperparameter_optimization_results_filename\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result\n",
    "import pandas as pd\n",
    "\n",
    "# Open a connection to the hdf5 dataset via PyTables with Pandas\n",
    "data_lake = pd.HDFStore(input_file)\n",
    "\n",
    "# Load the combined features\n",
    "features_df = data_lake['training/combined/features']\n",
    "\n",
    "# Close the connection\n",
    "data_lake.close()\n",
    "\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, cool. Looks good. Next is to set-up for the stage II classifiers.\n",
    "\n",
    "## 5. Stage II classifier\n",
    "\n",
    "The strategy here is to re-use as much code as possible from the stage I classifier. We need to do two things:\n",
    "\n",
    "1. Re-bin the data with the stage I class probability features added.\n",
    "2. Re-train on the new bins.\n",
    "\n",
    "The good news here, is I think it is going to be pretty trivial to do this. We need to modify the length binning slightly for the second pass so that it takes it's input data from 'combined' rather than a dataframe loaded from a separate file but, tother than that everything is the same. We just need to use different bins.\n",
    "\n",
    "### 5.1. Data re-binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New stage II bins\n",
    "bins = {\n",
    "    'combined': [26, 275],\n",
    "    'bin_026-050': [26, 50],\n",
    "    'bin_051-075': [51, 75],\n",
    "    'bin_076-100': [76, 100],\n",
    "    'bin_101-125': [101, 125],\n",
    "    'bin_126-150': [126, 150],\n",
    "    'bin_151-175': [151, 175],\n",
    "    'bin_176-200': [176, 200],\n",
    "    'bin_201-225': [201, 225],\n",
    "    'bin_226-250': [226, 250],\n",
    "    'bin_251-275': [251, 275]\n",
    "}\n",
    "\n",
    "# Construct input and output file paths\n",
    "dataset_name = 'falcon-7b_scores_v2_10-300_words'\n",
    "input_file = f'{config.DATA_PATH}/{dataset_name}.h5'\n",
    "output_file = f'{config.DATA_PATH}/{dataset_name}_stage_II.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the hdf5 output - create or open for read/write\n",
    "output = h5py.File(output_file, 'a')\n",
    "\n",
    "# Create the top-level groups\n",
    "_ = output.require_group('training')\n",
    "_ = output.require_group('testing')\n",
    "\n",
    "print(f'Top level groups: {(list(output.keys()))}')\n",
    "\n",
    "# Next, we need to add a group for each fragment length bin,\n",
    "# and one for the un-binned data\n",
    "for group in output.keys():\n",
    "\n",
    "    # Add the un-binned data group\n",
    "    _ = output.require_group(f'{group}/combined')\n",
    "\n",
    "    # Loop on the bins and add a group for each\n",
    "    for bin in bins.keys():\n",
    "        _ = output.require_group(f'{group}/{bin}')\n",
    "\n",
    "# Finally, add the bins under group bins\n",
    "output.attrs.update(bins)\n",
    "\n",
    "print(f'\\nBin attributes:')\n",
    "for key, value in output.attrs.items():\n",
    "    print(f' {key}: {value}')\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_labels(training_df, testing_df):\n",
    "    '''Takes training and testing dataframes, gets and encode human/synthetic\n",
    "    labels and returns.'''\n",
    "\n",
    "    # Get the labels\n",
    "    training_labels = training_df['Source']\n",
    "    testing_labels = testing_df['Source']\n",
    "\n",
    "    # Encode string class values as integers\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder = label_encoder.fit(training_labels)\n",
    "    training_labels = pd.Series(label_encoder.transform(training_labels)).astype(np.int64)\n",
    "    testing_labels = pd.Series(label_encoder.transform(testing_labels)).astype(np.int64)\n",
    "\n",
    "    return training_labels, testing_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reopen out hdf5 file with pandas so we can work with dataframes\n",
    "output_data_lake = pd.HDFStore(output_file)\n",
    "\n",
    "# And open the in hdf5 also, so we can get data from it\n",
    "input_data_lake = pd.HDFStore(input_file)\n",
    "\n",
    "# Load the combined training and testing data\n",
    "training_df = input_data_lake['training/combined/features']\n",
    "testing_df = input_data_lake['testing/combined/features']\n",
    "\n",
    "# Close the input connection\n",
    "input_data_lake.close()\n",
    "\n",
    "# Loop on the bins to bin the data\n",
    "for bin_id, bin_range in bins.items():\n",
    "\n",
    "    # Pull the fragments for this bin\n",
    "    bin_training_df = training_df[(training_df['Fragment length (words)'] >= bin_range[0]) & (training_df['Fragment length (words)'] <= bin_range[1])]\n",
    "    bin_testing_df = testing_df[(testing_df['Fragment length (words)'] >= bin_range[0]) & (testing_df['Fragment length (words)'] <= bin_range[1])]\n",
    "\n",
    "    # Fix the index\n",
    "    bin_training_df.reset_index(inplace = True, drop = True)\n",
    "    bin_testing_df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "    # Get bin labels\n",
    "    bin_training_labels, bin_testing_labels = make_labels(bin_training_df, bin_testing_df)\n",
    "\n",
    "    # Add the data to the data lake\n",
    "    output_data_lake.put(f'training/{bin_id}/features', bin_training_df)\n",
    "    output_data_lake.put(f'training/{bin_id}/labels', bin_training_labels)\n",
    "    output_data_lake.put(f'testing/{bin_id}/features', bin_training_df)\n",
    "    output_data_lake.put(f'testing/{bin_id}/labels', bin_training_labels)\n",
    "\n",
    "output_data_lake.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes some plots with the combined data\n",
    "\n",
    "# Open a connection to the hdf5 dataset via PyTables with Pandas\n",
    "data_lake = pd.HDFStore(output_file)\n",
    "\n",
    "# Pull the training features from the combined bin\n",
    "bin_training_features_df = data_lake[f'training/combined/features']\n",
    "data_lake.close()\n",
    "\n",
    "# Make the plots\n",
    "plot_funcs.data_exploration_plot_v2(bin_training_features_df).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
