{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity ratio score: Kullbackâ€“Leibler divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/arkk/llm_detector/classifier\n"
     ]
    }
   ],
   "source": [
    "# Change working directory to parent so we can import as we would from main.py\n",
    "%cd ..\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "from IPython.display import Image\n",
    "\n",
    "import functions.notebook_helper as helper_funcs\n",
    "import functions.notebook_plotting as plot_funcs\n",
    "import configuration as config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan here is to take our sampling distributions of perplexity ratio scores for human and synthetic text and use them to generate a function that takes a perplexity ratio score and converts it into a Kullback-Leibler score. See the figure below from the [Wikipedia article on KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/gperdrizet/llm_detector/benchmarking/benchmarking/notebooks/images/KL-Gauss-Example.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url = 'https://raw.githubusercontent.com/gperdrizet/llm_detector/benchmarking/benchmarking/notebooks/images/KL-Gauss-Example.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan is as follows:\n",
    "2. Plot and fit the distributions of perplexity ratio scores from human and synthetic text from the training data.\n",
    "3. Calculate the Kullback-Leibler divergence between the fitted distributions.\n",
    "4. Use a kernel density estimate of the Kullback-Leibler divergence to get it's probability density function.\n",
    "5. Use the Kullback-Leibler divergence PDF to calculate a 'Kullback-Leibler score' from the perplexity ratio score for each text fragment in the training and testing datasets.\n",
    "\n",
    "## TODO\n",
    "1. ~~Since KL divergence is not symmetrical, maybe we should include two features one for KL divergence between the human and synthetic score distributions in each direction...~~\n",
    "2. ~~Maybe try some of the other fits, e.g. gaussian rather than exponential gaussian.~~ Took a good second look at the old version of the experimentation notebook and the fit residuals are without a doubt best with exponential gaussian.\n",
    "3. ~~What if we use a kernel density estimate instead of a gaussian fit on the perplexity ratio density data? This might give us a better 'fit'.~~ Seems like classifier performance downstream in just slightly better with KDE than gaussian fit. Also feature selection methods seem to prefer the KDE based score as well. But, it's not a large difference.\n",
    "\n",
    "## Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config.FEATURE_ENGINEERING_CLASS_INSTANCE, 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need the distributions of perplexity ratio score values for human and synthetic text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "autodetected range of [nan, nan] is not finite",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate bins for perplexity ratio score from the training data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m scores \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mtraining\u001b[38;5;241m.\u001b[39mall\u001b[38;5;241m.\u001b[39mcombined[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPerplexity ratio score\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m counts, bins \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdensity\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Get bin centers\u001b[39;00m\n\u001b[1;32m      6\u001b[0m bin_centers \u001b[38;5;241m=\u001b[39m (bins[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m bins[\u001b[38;5;241m1\u001b[39m:]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mhistogram\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/mnt/arkk/llm_detector/.venv/lib/python3.8/site-packages/numpy/lib/histograms.py:780\u001b[0m, in \u001b[0;36mhistogram\u001b[0;34m(a, bins, range, density, weights)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;124;03mCompute the histogram of a dataset.\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    776\u001b[0m \n\u001b[1;32m    777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    778\u001b[0m a, weights \u001b[38;5;241m=\u001b[39m _ravel_and_check_weights(a, weights)\n\u001b[0;32m--> 780\u001b[0m bin_edges, uniform_bins \u001b[38;5;241m=\u001b[39m \u001b[43m_get_bin_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;66;03m# Histogram is an integer or a float array depending on the weights.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/arkk/llm_detector/.venv/lib/python3.8/site-packages/numpy/lib/histograms.py:426\u001b[0m, in \u001b[0;36m_get_bin_edges\u001b[0;34m(a, bins, range, weights)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_equal_bins \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`bins` must be positive, when an integer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 426\u001b[0m     first_edge, last_edge \u001b[38;5;241m=\u001b[39m \u001b[43m_get_outer_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(bins) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    429\u001b[0m     bin_edges \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(bins)\n",
      "File \u001b[0;32m/mnt/arkk/llm_detector/.venv/lib/python3.8/site-packages/numpy/lib/histograms.py:323\u001b[0m, in \u001b[0;36m_get_outer_edges\u001b[0;34m(a, range)\u001b[0m\n\u001b[1;32m    321\u001b[0m     first_edge, last_edge \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mmin(), a\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (np\u001b[38;5;241m.\u001b[39misfinite(first_edge) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(last_edge)):\n\u001b[0;32m--> 323\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautodetected range of [\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m] is not finite\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(first_edge, last_edge))\n\u001b[1;32m    326\u001b[0m \u001b[38;5;66;03m# expand empty range to avoid divide by zero\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_edge \u001b[38;5;241m==\u001b[39m last_edge:\n",
      "\u001b[0;31mValueError\u001b[0m: autodetected range of [nan, nan] is not finite"
     ]
    }
   ],
   "source": [
    "# Calculate bins for perplexity ratio score from the training data\n",
    "scores = data.training.all.combined['Perplexity ratio score']\n",
    "counts, bins = np.histogram(np.array(scores), bins = 500, density = True)\n",
    "\n",
    "# Get bin centers\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "# Calculate histograms for the human and combined synthetic data\n",
    "human_scores = data.training.all.human['Perplexity ratio score']\n",
    "synthetic_scores = data.training.all.synthetic_combined['Perplexity ratio score']\n",
    "\n",
    "human_density, human_bins = np.histogram(human_scores, bins = bins, density = True)\n",
    "synthetic_density, synthetic_bins = np.histogram(synthetic_scores, bins = bins, density = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, fit the individual distributions with an exponential gaussian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an exponential gaussian to the human and synthetic scores and get fitted values for\n",
    "# the bin centers calculated from the combined data\n",
    "print('\\nHuman fit:')\n",
    "human_exp_gaussian = helper_funcs.exp_gaussian_fit(human_scores)\n",
    "human_exp_gaussian_values = human_exp_gaussian.pdf(bin_centers)\n",
    "print(f'  Fitted values: {human_exp_gaussian_values[:3]}...')\n",
    "\n",
    "print('\\nSynthetic fit:')\n",
    "synthetic_exp_gaussian = helper_funcs.exp_gaussian_fit(synthetic_scores)\n",
    "synthetic_exp_gaussian_values = synthetic_exp_gaussian.pdf(bin_centers)\n",
    "print(f'  Fitted values: {synthetic_exp_gaussian_values[:3]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for comparison, also use a guassian kernel density estimate to get the fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the KDE and then get fitted values for the bin centers\n",
    "human_kde = gaussian_kde(human_scores)\n",
    "human_kde_values = human_kde.pdf(bin_centers)\n",
    "print(f'Human KDE values: {human_kde_values[:3]}...')\n",
    "\n",
    "synthetic_kde = gaussian_kde(synthetic_scores)\n",
    "synthetic_kde_values = synthetic_kde.pdf(bin_centers)\n",
    "print(f'Synthetic KDE values: {synthetic_kde_values[:3]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_funcs.plot_score_distribution_fits(\n",
    "        'Perplexity ratio score distribution',\n",
    "        bin_centers,\n",
    "        human_density,\n",
    "        human_exp_gaussian_values,\n",
    "        human_kde_values,\n",
    "        synthetic_density,\n",
    "        synthetic_exp_gaussian_values,\n",
    "        synthetic_kde_values\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, looks good to me - let's take a closer look at the fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_funcs.plot_fit_diagnostics(\n",
    "        'Perplexity ratio score fit diagnostics',\n",
    "        bin_centers,\n",
    "        human_density,\n",
    "        synthetic_density,\n",
    "        human_exp_gaussian_values,\n",
    "        synthetic_exp_gaussian_values,\n",
    "        human_kde_values,\n",
    "        synthetic_kde_values\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, looks good enough to me - now let's take a look at the Kullback-Leibler divergence of the two fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_funcs.plot_kl_divergences(\n",
    "        'Perplexity ratio score distributions: Kullback-Leibler divergence',\n",
    "        bin_centers,\n",
    "        human_exp_gaussian_values,\n",
    "        synthetic_exp_gaussian_values,\n",
    "        human_kde_values,\n",
    "        synthetic_kde_values\n",
    "\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to use fitted perplexity ratio score distributions to calculate and add the KL score for each text fragment in the dataset. E.g. on the plot above a fragment with a perplexity ratio score of ~0.8 would get a synthetic-human KL divergence score of about 25, etc. To do that we need kernel density estimates for the KL divergence distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_gaussian_synthetic_human_kld_kde, exp_gaussian_human_synthetic_kld_kde, plt = helper_funcs.get_kl_kde(\n",
    "    'Kullback-Leibler divergence KDE: perplexity ratio exponential gaussian fits', \n",
    "    scores, \n",
    "    human_exp_gaussian, \n",
    "    synthetic_exp_gaussian,\n",
    "    0.1,\n",
    "    0.001\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde_synthetic_human_kld_kde, kde_human_synthetic_kld_kde, plt = helper_funcs.get_kl_kde(\n",
    "    'Kullback-Leibler divergence KDE: perplexity ratio kernel density estimate', \n",
    "    scores, \n",
    "    human_kde, \n",
    "    synthetic_kde,\n",
    "    0.1,\n",
    "    0.001\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, cool. Looks good. Now we can run the perplexity ratio score column from the complete dataset through the Kullback-Leibler divergence distribution's kernel density estimate probability density function to get 'Kullback-Leibler scores' and add them to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the KLD scores\n",
    "synthetic_human_kld_scores = exp_gaussian_synthetic_human_kld_kde.pdf(data.all.combined['Perplexity ratio score'])\n",
    "human_synthetic_kld_scores = exp_gaussian_human_synthetic_kld_kde.pdf(data.all.combined['Perplexity ratio score'])\n",
    "\n",
    "# Add the scores to the dataframe\n",
    "data.all.combined['Synthetic-human perplexity ratio exponential gaussian fit Kullback-Leibler score'] = synthetic_human_kld_scores\n",
    "data.all.combined['Human-synthetic perplexity ratio exponential gaussian fit Kullback-Leibler score'] = human_synthetic_kld_scores\n",
    "\n",
    "# Get the KLD scores\n",
    "synthetic_human_kld_scores = kde_synthetic_human_kld_kde.pdf(data.all.combined['Perplexity ratio score'])\n",
    "human_synthetic_kld_scores = kde_human_synthetic_kld_kde.pdf(data.all.combined['Perplexity ratio score'])\n",
    "\n",
    "# Add the scores to the dataframe\n",
    "data.all.combined['Synthetic-human perplexity ratio kernel density estimate Kullback-Leibler score'] = synthetic_human_kld_scores\n",
    "data.all.combined['Human-synthetic perplexity ratio kernel density estimate Kullback-Leibler score'] = human_synthetic_kld_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the rest of the data in the class instance so the new columns propagate\n",
    "data.update_data(data.all.combined)\n",
    "\n",
    "# Take a look one of the other datasets in the feature engineering class instance\n",
    "# to be sure that the new columns propagated correctly\n",
    "data.training.all.combined.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-plot just to be sure we got the result we expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Kullback-Leibler score, exponential gaussian fits')\n",
    "plt.scatter(data.all.combined['Perplexity ratio score'], data.all.combined['Synthetic-human perplexity ratio exponential gaussian fit Kullback-Leibler score'], s = 1, label = 'synthetic-human')\n",
    "plt.scatter(data.all.combined['Perplexity ratio score'], data.all.combined['Human-synthetic perplexity ratio exponential gaussian fit Kullback-Leibler score'], s = 1, label = 'human-synthetic')\n",
    "plt.xlabel('Perplexity ratio score')\n",
    "plt.ylabel('Kullback-Leibler score')\n",
    "plt.legend(loc = 'upper right', fontsize = 'small', markerscale = 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Kullback-Leibler score, kernel density estimates')\n",
    "plt.scatter(data.all.combined['Perplexity ratio score'], data.all.combined['Synthetic-human perplexity ratio kernel density estimate Kullback-Leibler score'], s = 1, label = 'synthetic-human')\n",
    "plt.scatter(data.all.combined['Perplexity ratio score'], data.all.combined['Human-synthetic perplexity ratio kernel density estimate Kullback-Leibler score'], s = 1, label = 'human-synthetic')\n",
    "plt.xlabel('Perplexity ratio score')\n",
    "plt.ylabel('Kullback-Leibler score')\n",
    "plt.legend(loc = 'upper right', fontsize = 'small', markerscale = 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nailed it! Save for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist the feature engineering class instance so we can\n",
    "# use it in other notebooks\n",
    "with open(config.FEATURE_ENGINEERING_CLASS_INSTANCE, 'wb') as file:\n",
    "    pickle.dump(data, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
