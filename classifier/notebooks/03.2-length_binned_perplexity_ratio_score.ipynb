{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity ratio score: Kullbackâ€“Leibler divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan here is to take our sampling distributions of perplexity ratio (PR) scores for human and synthetic text and use them to generate a function that takes a perplexity ratio score and converts it into a Kullback-Leibler divergence (KLD) score. See the figure below from the [Wikipedia article on KLD](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence).\n",
    "\n",
    "Workflow is as follows:\n",
    "1. Get kernel density estimate of PR score distributions for human and synthetic text fragments in the training data.\n",
    "2. Calculated the KLD between the human and synthetic PR score distributions.\n",
    "3. Get a get kernel density estimate of the KLD.\n",
    "4. Use the probability density function from the KLD kernel density estimate to calculate a KLD score for each text fragment in the training and testing data.\n",
    "5. Add the KLD score as a new feature.\n",
    "\n",
    "The above will be done individually for each fragment length bin and the combined data. This way the KLD score feature in each bin will capture the PR score distribution for text fragments in that specific length regime, rather that for the whole dataset.\n",
    "\n",
    "One additional note - after some initial testing - the 4th step above is very slow due to the sheer number of points that need to be evaluated. Therefore, the workflow will be split up as follows to more efficiently utilize compute resources.\n",
    "\n",
    "**Version 2 workflow:**\n",
    "\n",
    "**Part I**: parallelized over the length bins\n",
    "1. Get kernel density estimate of score distributions for human and synthetic text fragments from the training data.\n",
    "2. Calculate the KLD between the human and synthetic score distributions.\n",
    "3. Get a kernel density estimate of the KLD.\n",
    "4. Serialize the KLD's kernel density estimate to disk.\n",
    "\n",
    "**Park II**: Parallelized over evaluation points in each bin, bins processed serially.\n",
    "1. Load KLD kernel density estimate.\n",
    "2. Calculate KDE value for each score in bin.\n",
    "3. Add scores as new feature in bin dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/gperdrizet/llm_detector/benchmarking/benchmarking/notebooks/images/KL-Gauss-Example.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url = 'https://raw.githubusercontent.com/gperdrizet/llm_detector/benchmarking/benchmarking/notebooks/images/KL-Gauss-Example.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Run set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /mnt/arkk/llm_detector/classifier\n"
     ]
    }
   ],
   "source": [
    "# Change working directory to parent so we can import as we would from main.py\n",
    "print(f'Working directory: ', end = '')\n",
    "%cd ..\n",
    "\n",
    "# Do the imports\n",
    "import h5py\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import configuration as config\n",
    "import functions.helper as helper_funcs\n",
    "import functions.kullback_leibler_divergence as kld_funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset we want to bin - omit the file extension, it will be\n",
    "# added appropriately for the input and output files\n",
    "dataset_name = 'falcon-7b_scores_v2_10-300_words'\n",
    "\n",
    "# Input file path\n",
    "input_file = f'{config.DATA_PATH}/{dataset_name}.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get Kullback-Leibler divergence kernel density estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will log to: /mnt/arkk/llm_detector/classifier/logs/perplexity_ratio_KLD_KDE.log\n",
      "bin_100 finished: True\n",
      "bin_125 finished: True\n",
      "bin_150 finished: True\n",
      "bin_175 finished: True\n",
      "bin_200 finished: True\n",
      "bin_225 finished: True\n",
      "bin_250 finished: True\n",
      "bin_275 finished: True\n",
      "bin_300 finished: True\n",
      "bin_50 finished: True\n",
      "bin_75 finished: True\n",
      "combined finished: True\n"
     ]
    }
   ],
   "source": [
    "# Run part I of the workflow in parallel over the bins - gets kernel density estimate\n",
    "# of Kullback-Leibler divergence between the feature's distribution for human and\n",
    "# synthetic text fragments in the training data, saves to disk for later use.\n",
    "\n",
    "kld_funcs.get_kullback_leibler_KDEs(\n",
    "    feature_name = 'Perplexity ratio score',\n",
    "    hdf5_file = input_file,\n",
    "    logfile_name = 'perplexity_ratio_KLD_KDE.log'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate perplexity ratio scores and add feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run part II of the workflow described above - uses previously stored\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# kernel density estimates of the Kullback-Leibler divergence between\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# the perplexity ratio scores for human and synthetic text fragments\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# in each bin. Loads the data from each bin sequentially and evaluates\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# each text fragment's perplexity ratio score. Parallelizes evaluation\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# over data in the bin. Adds result back to hdf5 as new feature column.\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mkld_funcs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_kullback_leibler_feature\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPerplexity ratio score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhdf5_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogfile_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mperplexity_ratio_KLD_feature_addition.log\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/arkk/llm_detector/classifier/functions/kullback_leibler_divergence.py:398\u001b[0m, in \u001b[0;36mmake_kullback_leibler_feature\u001b[0;34m(feature_name, hdf5_file, logfile_name)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m# Load the KLD KDE\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(input_filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[0;32m--> 398\u001b[0m     kld_kde \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(input_file)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# Evaluate the feature scores\u001b[39;00m\n\u001b[1;32m    401\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "# Run part II of the workflow described above - uses previously stored\n",
    "# kernel density estimates of the Kullback-Leibler divergence between\n",
    "# the perplexity ratio scores for human and synthetic text fragments\n",
    "# in each bin. Loads the data from each bin sequentially and evaluates\n",
    "# each text fragment's perplexity ratio score. Parallelizes evaluation\n",
    "# over data in the bin. Adds result back to hdf5 as new feature column.\n",
    "\n",
    "kld_funcs.make_kullback_leibler_feature(\n",
    "    feature_name = 'Perplexity ratio score',\n",
    "    hdf5_file = input_file,\n",
    "    logfile_name = 'perplexity_ratio_KLD_feature_addition.log'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get bins from hdf5 metadata\n",
    "data_lake = h5py.File(input_file, 'r')\n",
    "bins = dict(data_lake.attrs.items())\n",
    "bin_ids = list(bins.keys())\n",
    "data_lake.close()\n",
    "\n",
    "# Open a connection to the hdf5 dataset via PyTables with Pandas so we can\n",
    "# load the data from each bin as a dataframe\n",
    "data_lake = pd.HDFStore(input_file)\n",
    "\n",
    "# Now we want to make 3 plots for each bin: the distributions of perplexity ratio score,\n",
    "# the Kullback-Leibler divergence kernel density estimate, and the distribution of \n",
    "# Kullback-Leibler score values\n",
    "\n",
    "feature_name = 'Perplexity ratio score'\n",
    "\n",
    "# Set up a figure for n bins x 3 plots\n",
    "fig, axs = plt.subplots(\n",
    "    len(bin_ids),\n",
    "    3,\n",
    "    figsize = (9, (3 * len(bin_ids))),\n",
    "    gridspec_kw = {'wspace':0.4, 'hspace':0.4},\n",
    "    #sharex='col'\n",
    ")\n",
    "\n",
    "# Loop on the bins to draw each plot\n",
    "for i, bin_id in enumerate(bin_ids):\n",
    "\n",
    "    # Load bin data\n",
    "    bin_training_features_df = data_lake[f'training/{bin_id}/features']\n",
    "\n",
    "    # Get human and synthetic perplexity ratio score\n",
    "    human_feature = bin_training_features_df[feature_name][bin_training_features_df['Source'] == 'human']\n",
    "    synthetic_feature = bin_training_features_df[feature_name][bin_training_features_df['Source'] == 'synthetic']\n",
    "\n",
    "    # Draw histograms for human and synthetic perplexity ratio scores in the first plot\n",
    "    axs[i, 0].set_title(f'{bin_id} {feature_name.lower()}', fontsize = 'medium')\n",
    "    axs[i, 0].set_xlabel('Score')\n",
    "    axs[i, 0].set_ylabel('Count')\n",
    "    axs[i, 0].hist(human_feature, bins = 50, alpha = 0.5, label = 'human')\n",
    "    axs[i, 0].hist(synthetic_feature, bins = 50, alpha = 0.5, label = 'synthetic')\n",
    "    axs[i, 0].legend(loc = 'upper left', fontsize = 'x-small')\n",
    "\n",
    "    # Turn axis tick labels back on for shared x axis\n",
    "    axs[i, 0].tick_params(labelbottom = True)\n",
    "\n",
    "    # For the second plot load and evaluate the Kullback-Leibler divergence kernel density estimate\n",
    "        \n",
    "    # Make the filename for the stored Kullback-Leibler divergence kernel density estimate for this bin\n",
    "    formatted_feature_name = feature_name.replace(' ', '_').lower()\n",
    "    input_filename = f'{config.MODELS_PATH}/{formatted_feature_name}_KLD_KDE_{bin_id}.pkl'\n",
    "\n",
    "    # Load the KLD KDE\n",
    "    with open(input_filename, 'rb') as input_file:\n",
    "        kld_kde = pickle.load(input_file)\n",
    "\n",
    "    # Make 100 evaluation points across the data's range\n",
    "    x = helper_funcs.make_padded_range(bin_training_features_df[feature_name])\n",
    "\n",
    "    # Evaluate\n",
    "    y = kld_kde(x)\n",
    "\n",
    "    # Plot\n",
    "    axs[i, 1].set_title(f'{bin_id} Kullback-Leibler KDE', fontsize = 'medium')\n",
    "    axs[i, 1].set_xlabel('Score')\n",
    "    axs[i, 1].set_ylabel('KDE value')\n",
    "    axs[i, 1].plot(x, y)\n",
    "\n",
    "    # Turn axis tick labels back on for shared x axis\n",
    "    axs[i, 1].tick_params(labelbottom = True)\n",
    "\n",
    "    # For the third plot make a histogram of the KLD KDE values in the bin\n",
    "\n",
    "    # Get human and KLD scores\n",
    "    human_feature = bin_training_features_df[f'{feature_name} Kullback-Leibler divergence'][bin_training_features_df['Source'] == 'human']\n",
    "    synthetic_feature = bin_training_features_df[f'{feature_name} Kullback-Leibler divergence'][bin_training_features_df['Source'] == 'synthetic']\n",
    "\n",
    "    # Draw histograms for human and synthetic Kullback-Leibler divergence scores\n",
    "    axs[i, 2].set_title(f'{bin_id} Kullback-Leibler scores', fontsize = 'medium')\n",
    "    axs[i, 2].set_xlabel('Score')\n",
    "    axs[i, 2].set_ylabel('Count')\n",
    "    axs[i, 2].hist(human_feature, bins = 50, alpha = 0.5, label = 'human')\n",
    "    axs[i, 2].hist(synthetic_feature, bins = 50, alpha = 0.5, label = 'synthetic')\n",
    "    axs[i, 2].legend(loc = 'upper left', fontsize = 'x-small')\n",
    "\n",
    "    # Turn axis tick labels back on for shared x axis\n",
    "    axs[i, 2].tick_params(labelbottom = True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "data_lake.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
