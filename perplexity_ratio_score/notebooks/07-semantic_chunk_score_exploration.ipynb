{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic chunk score exploration\n",
    "\n",
    "Now that we have some of the semantic chunk data scored for perplexity ratio, let's take a look and see what we are working with.\n",
    "\n",
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change working directory to parent so we can import as we would from the perplexity ratio score root directory\n",
    "%cd ..\n",
    "\n",
    "# PyPI imports\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.stats.api as sms\n",
    "from statistics import mean\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Internal imports\n",
    "import configuration as config\n",
    "import notebooks.helper_functions.plotting_functions as plot_funcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file=f'{config.SCORED_DATA_PATH}/test_chunks.19.parquet'\n",
    "data_df=pd.read_parquet(data_file)\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_obj=plot_funcs.data_composition(data_df)\n",
    "plot_obj.savefig(f'{config.PLOT_PATH}/07-2_data_composition.jpg')\n",
    "plot_obj.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['words']=data_df['text'].apply(lambda x: len(x.split(' ')))\n",
    "data_df['words'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Length distributions: all text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_obj=plot_funcs.length_distributions('Semantic chunk length distributions', data_df)\n",
    "plot_obj.savefig(f'{config.PLOT_PATH}/07-3.1_semantic_chunk_length_distributions.jpg')\n",
    "plot_obj.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, let's take a look at the chunk length distributions for human vs machine text. But before we do - let's provisionally get rid of any single word chunks - we can pretty clearly justify that as a splitting failure, if nothing else. Let's also trim off very long chunks.\n",
    "\n",
    "### 3.2. Length distributions: human vs synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length threshold the data\n",
    "working_data_df=data_df[data_df['words'] > 1]\n",
    "working_data_df=working_data_df[working_data_df['words'] < 1024]\n",
    "\n",
    "# Replace int values for 'synthetic' with human readable strings\n",
    "working_data_df['synthetic']=working_data_df['synthetic'].map({0: 'human', 1: 'synthetic'})\n",
    "working_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_obj=plot_funcs.length_distributions(\n",
    "    'Semantic chunk length distributions:\\nhuman vs synthetic text',\n",
    "    working_data_df, \n",
    "    hue_by='synthetic'\n",
    ")\n",
    "\n",
    "plot_obj.savefig(f'{config.PLOT_PATH}/07-3.2_human_vs_synthetic_length_distributions.jpg')\n",
    "plot_obj.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both distributions plots look qualitatively similar to those generated from the complete dataset prior to scoring. Let's move on and look a the perplexity ratio score.\n",
    "\n",
    "## 4. Perplexity ratio score\n",
    "\n",
    "### 4.1. Human vs synthetic mean by dataset source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=sns.boxplot(data=working_data_df, x='source', y='perplexity_ratio_score', hue='synthetic')\n",
    "ax.tick_params(axis='x', labelrotation=45)\n",
    "plt.savefig(f'{config.PLOT_PATH}/07-4.1_perplexity_ratio_score_mean_by_dataset_source.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK - cool! Looks qualitatively similar to the original Hans dataset by itself. Means are pretty clearly different, but there is a lot of overlap between the distributions. \n",
    "\n",
    "It's interesting that the Gaggar dataset seems the least differentiated and scores highest overall. That dataset contains GPT-3.5-turbo output, which is the most powerful adversary model in the dataset. But, the data was also created by prompting the model to rephrase human written text. This is different than purely generated responses.\n",
    "\n",
    "Let's do a t-test and set-up a confidence interval around the difference in means to find out a little more quantitatively how different the two distributions actually are.\n",
    "\n",
    "### 4.2. Human vs synthetic mean by author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=sns.boxplot(data=working_data_df, x='author', y='perplexity_ratio_score', hue='synthetic')\n",
    "ax.tick_params(axis='x', labelrotation=45)\n",
    "plt.savefig(f'{config.PLOT_PATH}/07-4.2_perplexity_ratio_score_mean_by_author.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same observations as above apply. Perplexity score mean is lower for synthetic text on average. The only model the output of which we are not convincingly detecting in aggregate is GPT-3.5-turbo, but again this is a 'rephrasing' dataset, not a de novo machine generated text dataset.\n",
    "\n",
    "The only real solution to that kind of nit-picking is to synthesize our own data using a selection of different models. Problem will be the cost of using proprietary APIs to do so. No one is gonna take us seriously if we can't detect the newest, fanciest chatGPT.\n",
    "\n",
    "For now, we can just exclude the gagger data on the basis of how it was generated if it drags the dataset wide accuracy down too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into human and synthetic\n",
    "human_df=working_data_df[working_data_df['synthetic'] == 'human']\n",
    "synthetic_df=working_data_df[working_data_df['synthetic'] == 'synthetic']\n",
    "\n",
    "# Pull the data for this model\n",
    "human=human_df['perplexity_ratio_score']\n",
    "synthetic=synthetic_df['perplexity_ratio_score']\n",
    "\n",
    "# Get the means in question and their difference\n",
    "human_mean=mean(human)\n",
    "synthetic_mean=mean(synthetic)\n",
    "mean_diff=human_mean - synthetic_mean\n",
    "\n",
    "# Get the confidence interval\n",
    "cm=sms.CompareMeans(sms.DescrStatsW(human), sms.DescrStatsW(synthetic))\n",
    "difference=cm.tconfint_diff(usevar='unequal')\n",
    "low_bound=difference[0]\n",
    "high_bound=difference[1]\n",
    "\n",
    "# Do a t-test with H0 equal means, H1 human greater than synthetic\n",
    "ttest_result = ttest_ind(human, synthetic, alternative='greater')\n",
    "\n",
    "print(f'T-test p-value = {ttest_result.pvalue}')\n",
    "print(f'Difference in means = {mean_diff:.3f}, 95% CI = ({low_bound:.3f}, {high_bound:.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, marginally better even. Just for reference - here is the Falcon-7B reader/writer result for the old Hans only dataset:\n",
    "\n",
    "```\n",
    "Model: Falcon-7B, t-test p-value = 1.468072030319169e-94\n",
    "Model: Falcon-7B, difference in means = 0.116, 95% CI = (0.106, 0.126)\n",
    "```\n",
    "\n",
    "### 4.3. Human vs synthetic distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Perplexity ratio score distribution: Falcon-7B')\n",
    "plt.hist(human_df['perplexity_ratio_score'], density=True, label='Human text', alpha=0.5)\n",
    "plt.hist(synthetic_df['perplexity_ratio_score'], density=True, label='Synthetic text', alpha=0.5)\n",
    "plt.xlabel('Perplexity ratio score')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig(f'{config.PLOT_PATH}/07-4.3_perplexity_ratio_score_distribution.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Perplexity ratio score by text length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Perplexity ratio score by fragment length: Falcon-7B')\n",
    "plt.scatter(human_df['words'], human_df['perplexity_ratio_score'], alpha=0.5, label='Human text')\n",
    "plt.scatter(synthetic_df['words'], synthetic_df['perplexity_ratio_score'], alpha=0.5, label='Synthetic text')\n",
    "plt.xlabel('Fragment length (tokens)')\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig(f'{config.PLOT_PATH}/07-4.4_perplexity_ratio_score_by_text_length.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, on the whole, very similar to the Hans data alone. Even if the fact that the gagger synthetic GPT data seems to score similarly to human is slightly concerning, it's good news that overall our trick seems to work well on data from other sources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
