{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic chunk data exploration\n",
    "\n",
    "Now that we have the text data split into semantic chunks, let's load them up and take a quick look at what we have.\n",
    "\n",
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change working directory to parent so we can import as we would\n",
    "# from the perplexity ratio score root directory\n",
    "%cd ..\n",
    "\n",
    "# PyPI imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Internal imports\n",
    "import configuration as config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file=f'{config.INTERMEDIATE_DATA_PATH}/all_chunkss.json'\n",
    "data_df=pd.read_json(data_file)\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_labels(mylabels, tooclose=0.1, sepfactor=2):\n",
    "    vecs=np.zeros((len(mylabels), len(mylabels), 2))\n",
    "    dists=np.zeros((len(mylabels), len(mylabels)))\n",
    "\n",
    "    for i in range(0, len(mylabels)-1):\n",
    "        for j in range(i+1, len(mylabels)):\n",
    "            a=np.array(mylabels[i].get_position())\n",
    "            b=np.array(mylabels[j].get_position())\n",
    "            dists[i,j]=np.linalg.norm(a-b)\n",
    "            vecs[i,j,:]=a-b\n",
    "            \n",
    "            if dists[i,j] < tooclose:\n",
    "                mylabels[i].set_x(a[0] + sepfactor*vecs[i,j,0])\n",
    "                mylabels[i].set_y(a[1] + sepfactor*vecs[i,j,1])\n",
    "                mylabels[j].set_x(b[0] - sepfactor*vecs[i,j,0])\n",
    "                mylabels[j].set_y(b[1] - sepfactor*vecs[i,j,1])\n",
    "\n",
    "authors=data_df['author'].value_counts()\n",
    "datasets=data_df['source'].value_counts()\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(6, 3))\n",
    "axs[0].set_title('Author')\n",
    "wedges, labels=axs[0].pie(authors, labels=authors.index)\n",
    "fix_labels(labels, sepfactor=10)\n",
    "axs[1].set_title('Data source')\n",
    "axs[1].pie(datasets, labels=datasets.index)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['words']=data_df['text'].apply(lambda x: len(x.split(' ')))\n",
    "data_df['words'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(9, 9), squeeze=True)\n",
    "axs=axs.flatten()\n",
    "\n",
    "for i, dataset in enumerate(datasets.index):\n",
    "    plot_data_df=data_df[data_df['source'] == dataset]\n",
    "    sns.kdeplot(data=plot_data_df, x='words', log_scale=10, legend=False, ax=axs[i])\n",
    "    axs[i].set_title(dataset)\n",
    "\n",
    "sns.kdeplot(data=data_df, x='words', log_scale=10, legend=False, ax=axs[-1])\n",
    "axs[-1].set_title('combined')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, let's take a look at the chunk length distributions for human vs machine text. But before we do - let's provisionally get rid of any single word chunks - we can pretty clearly justify that as a splitting failure, if nothing else. Let's also trim off very long chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_data_df=data_df[data_df['words'] > 1]\n",
    "working_data_df=working_data_df[working_data_df['words'] < 1024]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Text length: human vs machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(9, 9), squeeze=True)\n",
    "axs=axs.flatten()\n",
    "\n",
    "for i, dataset in enumerate(datasets.index):\n",
    "    plot_data_df=working_data_df[working_data_df['source'] == dataset]\n",
    "    sns.kdeplot(data=plot_data_df, x='words', hue='synthetic', common_norm=True, log_scale=10, legend=True, ax=axs[i])\n",
    "    axs[i].set_title(dataset)\n",
    "\n",
    "sns.kdeplot(data=working_data_df, x='words', hue='synthetic', common_norm=True, log_scale=10, legend=True, ax=axs[-1])\n",
    "axs[-1].set_title('combined')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK - I see differences. Let's try plotting the histogram bin fractions against each other for synthetic and human text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins=100\n",
    "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(9, 9), squeeze=True)\n",
    "axs=axs.flatten()\n",
    "\n",
    "# Loop on the datasets from each source\n",
    "for i, dataset in enumerate(datasets.index):\n",
    "    plot_data_df=working_data_df[working_data_df['source'] == dataset]\n",
    "\n",
    "    # Get bins for human and synthetic data together\n",
    "    counts, bins=np.histogram(plot_data_df['words'], bins=num_bins)\n",
    "\n",
    "    # Use the bin edges to get densities for human and synthetic separately\n",
    "    human_density, human_bins=np.histogram(plot_data_df['words'][plot_data_df['synthetic'] == 0], bins=bins, density=True)\n",
    "    synthetic_density, synthetic_bins=np.histogram(plot_data_df['words'][plot_data_df['synthetic'] == 1], bins=bins, density=True)\n",
    "\n",
    "    # Plot the bin densities against each other\n",
    "    this_plot=axs[i].scatter(x=human_density, y=synthetic_density, c=bins[:-1])\n",
    "    fig.colorbar(this_plot)\n",
    "    # axs[i].set_yscale('log')\n",
    "    # axs[i].set_xscale('log')\n",
    "    axs[i].set_xlabel('human bin density')\n",
    "    axs[i].set_ylabel('synthetic bin density')\n",
    "    axs[i].set_title(dataset)\n",
    "\n",
    "# Do the same for the complete dataset\n",
    "counts, bins=np.histogram(working_data_df['words'], bins=num_bins)\n",
    "human_density, human_bins=np.histogram(working_data_df['words'][working_data_df['synthetic'] == 0], bins=bins, density=True)\n",
    "synthetic_density, synthetic_bins=np.histogram(working_data_df['words'][working_data_df['synthetic'] == 1], bins=bins, density=True)\n",
    "\n",
    "last_plot=axs[-1].scatter(x=human_density, y=synthetic_density, c=bins[:-1])\n",
    "# axs[-1].set_yscale('log')\n",
    "# axs[-1].set_xscale('log')\n",
    "axs[-1].set_xlabel('human bin density')\n",
    "axs[-1].set_ylabel('synthetic bin density')\n",
    "axs[-1].set_title('combined')\n",
    "fig.colorbar(last_plot)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
