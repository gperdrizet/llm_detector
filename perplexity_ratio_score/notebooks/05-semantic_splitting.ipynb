{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic splitting\n",
    "\n",
    "Last approach to try for text splitting is semantic splitting with semantic-text-splitter from PyPI. Hopefully, this will combine the best aspects of sentence splitting and word length based 'dumb' splitting. We will have some control over the output chunk size and splits will occur in more rational places that a simple arbitrary word length.\n",
    "\n",
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/arkk/llm_detector/perplexity_ratio_score\n"
     ]
    }
   ],
   "source": [
    "# Change working directory to parent so we can import as we would\n",
    "# from the perplexity ratio score root directory\n",
    "%cd ..\n",
    "\n",
    "# Standard library imports\n",
    "import glob\n",
    "import time\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "\n",
    "# PyPI imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from semantic_text_splitter import TextSplitter\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# Internal imports\n",
    "import configuration as config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load an example data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>synthetic</th>\n",
       "      <th>author</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ad them all. Some of the best were the ones th...</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>yatsenko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ER (emergency) vets are more expensive than re...</td>\n",
       "      <td>1</td>\n",
       "      <td>unknown_model</td>\n",
       "      <td>yatsenko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey, Mrs. Johnson! Here's my essay on how a pe...</td>\n",
       "      <td>1</td>\n",
       "      <td>unknown_model</td>\n",
       "      <td>grinberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This article is about the daemonic name, for t...</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>yatsenko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Budget For Server Desktop Upgrade Essay\\n\\nExe...</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>yatsenko</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  synthetic  \\\n",
       "0  ad them all. Some of the best were the ones th...          0   \n",
       "1  ER (emergency) vets are more expensive than re...          1   \n",
       "2  Hey, Mrs. Johnson! Here's my essay on how a pe...          1   \n",
       "3  This article is about the daemonic name, for t...          0   \n",
       "4  Budget For Server Desktop Upgrade Essay\\n\\nExe...          0   \n",
       "\n",
       "          author    source  \n",
       "0          human  yatsenko  \n",
       "1  unknown_model  yatsenko  \n",
       "2  unknown_model  grinberg  \n",
       "3          human  yatsenko  \n",
       "4          human  yatsenko  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file=f'{config.INTERMEDIATE_DATA_PATH}/texts.0.parquet'\n",
    "data_df=pd.read_parquet(data_file)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test split a small batch of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 999 records in 4.7 seconds\n",
      "Splitting rate: 213.9 records per second\n"
     ]
    }
   ],
   "source": [
    "# Holder for results\n",
    "results={\n",
    "    'text': [],\n",
    "    'synthetic': [],\n",
    "    'author': [],\n",
    "    'source': []\n",
    "}\n",
    "\n",
    "\n",
    "# Tokenizer & splitter\n",
    "tokenizer_name='bert-base-uncased'\n",
    "max_tokens=512\n",
    "\n",
    "tokenizer=Tokenizer.from_pretrained(tokenizer_name)\n",
    "splitter=TextSplitter.from_huggingface_tokenizer(tokenizer, max_tokens)\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "for i in range(1000):\n",
    "    \n",
    "    text=data_df['text'].iloc[i]\n",
    "    chunks=splitter.chunks(text)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        results['text'].append(chunk)\n",
    "        results['synthetic'].append(data_df['synthetic'].iloc[i])\n",
    "        results['author'].append(data_df['author'].iloc[i])\n",
    "        results['source'].append(data_df['source'].iloc[i])\n",
    "\n",
    "dT=time.time() - start_time\n",
    "splitting_rate=(i + 1)/dT\n",
    "print(f'Split {i} records in {dT:.1f} seconds')\n",
    "print(f'Splitting rate: {splitting_rate:.1f} records per second')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slower than the NLTK sentence splitter, which ran at ~600 records per second. But, it's still tractable. At ~200 records per second, it should take about four and a half hours to split all 3.47 million records, or about 30 minutes when parallelized over 16 input files. Let's do it.\n",
    "\n",
    "## 3. Parallel splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the splitting function\n",
    "\n",
    "def split_text(data_file: str=None, target_size: int=512, worker: int=0) -> dict:\n",
    "    '''Function to parallelize semantic splitting of text over input files. \n",
    "    Meant to be called with multiprocessing worker. Take an input file \n",
    "    string, loads the data, splits sentences, collects results in dictionary\n",
    "    and returns dictionary.'''\n",
    "\n",
    "    data_df=pd.read_parquet(data_file)\n",
    "    print(f\"\\nWorker {worker} loaded: {data_file.split('/')[-1]}\", end='')\n",
    "\n",
    "    results={\n",
    "        'text': [],\n",
    "        'synthetic': [],\n",
    "        'author': [],\n",
    "        'source': []\n",
    "    }\n",
    "\n",
    "    # Tokenizer & splitter\n",
    "    tokenizer_name='bert-base-uncased'\n",
    "    tokenizer=Tokenizer.from_pretrained(tokenizer_name)\n",
    "    splitter=TextSplitter.from_huggingface_tokenizer(tokenizer, target_size)\n",
    "\n",
    "    for i in range(len(data_df)):\n",
    "        \n",
    "        text=data_df['text'].iloc[i]\n",
    "        chunks=splitter.chunks(text)\n",
    "\n",
    "        for chunk in chunks:\n",
    "            results['text'].append(chunk)\n",
    "            results['synthetic'].append(data_df['synthetic'].iloc[i])\n",
    "            results['author'].append(data_df['author'].iloc[i])\n",
    "            results['source'].append(data_df['source'].iloc[i])\n",
    "\n",
    "    print(f'\\nWorker {worker} finished, parsed {len(chunks)} chunks', end='')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Worker 10 loaded: texts.10.parquet\n",
      "Worker 13 loaded: texts.13.parquet\n",
      "Worker 5 loaded: texts.5.parquet\n",
      "Worker 15 loaded: texts.15.parquet\n",
      "Worker 9 loaded: texts.9.parquet\n",
      "Worker 4 loaded: texts.4.parquet\n",
      "Worker 0 loaded: texts.0.parquet\n",
      "Worker 7 loaded: texts.7.parquet\n",
      "Worker 14 loaded: texts.14.parquet\n",
      "Worker 6 loaded: texts.6.parquet\n",
      "Worker 1 loaded: texts.1.parquet\n",
      "Worker 3 loaded: texts.3.parquet\n",
      "Worker 11 loaded: texts.11.parquet\n",
      "Worker 2 loaded: texts.2.parquet\n",
      "Worker 8 loaded: texts.8.parquet\n",
      "Worker 12 loaded: texts.12.parquet\n",
      "Worker 15 finished, parsed 1 sentences\n",
      "Worker 13 finished, parsed 1 sentences\n",
      "Worker 5 finished, parsed 1 sentences\n",
      "Worker 11 finished, parsed 1 sentences\n",
      "Worker 3 finished, parsed 1 sentences\n",
      "Worker 14 finished, parsed 1 sentences\n",
      "Worker 2 finished, parsed 1 sentences\n",
      "Worker 4 finished, parsed 1 sentences\n",
      "Worker 6 finished, parsed 1 sentences\n",
      "Worker 9 finished, parsed 2 sentences\n",
      "Worker 10 finished, parsed 1 sentences\n",
      "Worker 8 finished, parsed 2 sentences\n",
      "Worker 0 finished, parsed 1 sentences\n",
      "Worker 7 finished, parsed 2 sentences\n",
      "Worker 12 finished, parsed 1 sentences\n",
      "Worker 1 finished, parsed 1 sentences\n",
      "CPU times: user 17.3 s, sys: 17.6 s, total: 35 s\n",
      "Wall time: 30min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Get list of input files\n",
    "input_files=glob.glob(f'{config.INTERMEDIATE_DATA_PATH}/texts.*.parquet')\n",
    "\n",
    "# Instantiate pool with one worker per input file\n",
    "pool=mp.Pool(\n",
    "    processes=len(input_files),\n",
    "    maxtasksperchild=1\n",
    ")\n",
    "\n",
    "# Holder for returns from workers\n",
    "async_results=[]\n",
    "\n",
    "# Loop input files\n",
    "for i, data_file in enumerate(input_files):\n",
    "\n",
    "    async_results.append(pool.apply_async(split_text,args=(data_file,512,i,)))\n",
    "\n",
    "# Clean up\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "# Get the results\n",
    "results=[async_result.get() for async_result in async_results]\n",
    "\n",
    "# Collect the results\n",
    "chunks={\n",
    "    'text': [],\n",
    "    'synthetic': [],\n",
    "    'author': [],\n",
    "    'source': []\n",
    "}\n",
    "\n",
    "for result in results:\n",
    "    for key, value in result.items():\n",
    "        chunks[key].extend(value)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>synthetic</th>\n",
       "      <th>author</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ad them all. Some of the best were the ones th...</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>yatsenko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ER (emergency) vets are more expensive than re...</td>\n",
       "      <td>1</td>\n",
       "      <td>unknown_model</td>\n",
       "      <td>yatsenko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey, Mrs. Johnson! Here's my essay on how a pe...</td>\n",
       "      <td>1</td>\n",
       "      <td>unknown_model</td>\n",
       "      <td>grinberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This article is about the daemonic name, for t...</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>yatsenko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Budget For Server Desktop Upgrade Essay\\n\\nExe...</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>yatsenko</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  synthetic  \\\n",
       "0  ad them all. Some of the best were the ones th...          0   \n",
       "1  ER (emergency) vets are more expensive than re...          1   \n",
       "2  Hey, Mrs. Johnson! Here's my essay on how a pe...          1   \n",
       "3  This article is about the daemonic name, for t...          0   \n",
       "4  Budget For Server Desktop Upgrade Essay\\n\\nExe...          0   \n",
       "\n",
       "          author    source  \n",
       "0          human  yatsenko  \n",
       "1  unknown_model  yatsenko  \n",
       "2  unknown_model  grinberg  \n",
       "3          human  yatsenko  \n",
       "4          human  yatsenko  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_df=pd.DataFrame(chunks)\n",
    "chunks_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save results\n",
    "\n",
    "### 4.1. Parquet shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give it a shuffle\n",
    "chunks_df=chunks_df.sample(frac=1)\n",
    "\n",
    "# Split the dataframe into 16 shards\n",
    "chunk_shards=np.array_split(chunks_df, 16)\n",
    "\n",
    "# Save each chunk as parquet with a clean index\n",
    "for i, chunk in enumerate(chunk_shards):\n",
    "    output_file=f'{config.INTERMEDIATE_DATA_PATH}/chunks.{i}.parquet'\n",
    "    chunk.reset_index(inplace=True, drop=True)\n",
    "    chunk.to_parquet(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Single JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sentences data to dict\n",
    "chunks_dict=chunks_df.to_dict(orient='list')\n",
    "\n",
    "# Save it as JSON\n",
    "with open(f'{config.INTERMEDIATE_DATA_PATH}/all_chunks.json', 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(chunks_dict, output_file, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
