{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic splitting\n",
    "\n",
    "Last approach to try for text splitting is semantic splitting with semantic-text-splitter from PyPI. Hopefully, this will combine the best aspects of sentence splitting and word length based 'dumb' splitting. We will have some control over the output chunk size and splits will occur in more rational places that a simple arbitrary word length.\n",
    "\n",
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change working directory to parent so we can import as we would\n",
    "# from the perplexity ratio score root directory\n",
    "%cd ..\n",
    "\n",
    "# Standard library imports\n",
    "import glob\n",
    "import time\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "\n",
    "# PyPI imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from semantic_text_splitter import TextSplitter\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# Internal imports\n",
    "import configuration as config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load an example data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file=f'{config.INTERMEDIATE_DATA_PATH}/texts.0.parquet'\n",
    "data_df=pd.read_parquet(data_file)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test split a small batch of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holder for results\n",
    "results={\n",
    "    'text': [],\n",
    "    'synthetic': [],\n",
    "    'author': [],\n",
    "    'source': []\n",
    "}\n",
    "\n",
    "\n",
    "# Tokenizer & splitter\n",
    "tokenizer_name='bert-base-uncased'\n",
    "max_tokens=512\n",
    "\n",
    "tokenizer=Tokenizer.from_pretrained(tokenizer_name)\n",
    "splitter=TextSplitter.from_huggingface_tokenizer(tokenizer, max_tokens)\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "for i in range(1000):\n",
    "    \n",
    "    text=data_df['text'].iloc[i]\n",
    "    chunks=splitter.chunks(text)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        results['text'].append(chunk)\n",
    "        results['synthetic'].append(data_df['synthetic'].iloc[i])\n",
    "        results['author'].append(data_df['author'].iloc[i])\n",
    "        results['source'].append(data_df['source'].iloc[i])\n",
    "\n",
    "dT=time.time() - start_time\n",
    "splitting_rate=(i + 1)/dT\n",
    "print(f'Split {i} records in {dT:.1f} seconds')\n",
    "print(f'Splitting rate: {splitting_rate:.1f} records per second')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slower than the NLTK sentence splitter, which ran at ~600 records per second. But, it's still tractable. At ~200 records per second, it should take about four and a half hours to split all 3.47 million records, or about 30 minutes when parallelized over 16 input files. Let's do it.\n",
    "\n",
    "## 3. Parallel splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the splitting function\n",
    "\n",
    "def split_text(data_file: str=None, target_size: int=512, worker: int=0) -> dict:\n",
    "    '''Function to parallelize semantic splitting of text over input files. \n",
    "    Meant to be called with multiprocessing worker. Take an input file \n",
    "    string, loads the data, splits sentences, collects results in dictionary\n",
    "    and returns dictionary.'''\n",
    "\n",
    "    data_df=pd.read_parquet(data_file)\n",
    "    print(f\"\\nWorker {worker} loaded: {data_file.split('/')[-1]}\", end='')\n",
    "\n",
    "    results={\n",
    "        'text': [],\n",
    "        'synthetic': [],\n",
    "        'author': [],\n",
    "        'source': []\n",
    "    }\n",
    "\n",
    "    # Tokenizer & splitter\n",
    "    tokenizer_name='bert-base-uncased'\n",
    "    tokenizer=Tokenizer.from_pretrained(tokenizer_name)\n",
    "    splitter=TextSplitter.from_huggingface_tokenizer(tokenizer, target_size)\n",
    "\n",
    "    for i in range(len(data_df)):\n",
    "        \n",
    "        text=data_df['text'].iloc[i]\n",
    "        chunks=splitter.chunks(text)\n",
    "\n",
    "        for chunk in chunks:\n",
    "            results['text'].append(chunk)\n",
    "            results['synthetic'].append(data_df['synthetic'].iloc[i])\n",
    "            results['author'].append(data_df['author'].iloc[i])\n",
    "            results['source'].append(data_df['source'].iloc[i])\n",
    "\n",
    "    print(f'\\nWorker {worker} finished, parsed {results[\"text\"]} chunks', end='')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Collect the results\n",
    "chunks={\n",
    "    'text': [],\n",
    "    'synthetic': [],\n",
    "    'author': [],\n",
    "    'source': []\n",
    "}\n",
    "\n",
    "target_lengths=[16,32,64,128,256,512]\n",
    "\n",
    "for target_length in target_lengths:\n",
    "\n",
    "    # Get list of input files\n",
    "    input_files=glob.glob(f'{config.INTERMEDIATE_DATA_PATH}/texts.*.parquet')\n",
    "\n",
    "    # Instantiate pool with one worker per input file\n",
    "    pool=mp.Pool(\n",
    "        processes=len(input_files),\n",
    "        maxtasksperchild=1\n",
    "    )\n",
    "\n",
    "    # Holder for returns from workers\n",
    "    async_results=[]\n",
    "\n",
    "    # Loop input files\n",
    "    for i, data_file in enumerate(input_files):\n",
    "\n",
    "        async_results.append(pool.apply_async(split_text,args=(data_file,target_length,i,)))\n",
    "\n",
    "    # Clean up\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Get the results\n",
    "    results=[async_result.get() for async_result in async_results]\n",
    "\n",
    "    # Collect the results\n",
    "    for result in results:\n",
    "        for key, value in result.items():\n",
    "            chunks[key].extend(value)\n",
    "    \n",
    "    print(f'Finished target length {target_length}\\n')\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_df=pd.DataFrame(chunks)\n",
    "chunks_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save results\n",
    "\n",
    "### 4.1. Parquet shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give it a shuffle\n",
    "chunks_df=chunks_df.sample(frac=1)\n",
    "\n",
    "# Split the dataframe into 16 shards\n",
    "chunk_shards=np.array_split(chunks_df, 16)\n",
    "\n",
    "# Save each chunk as parquet with a clean index\n",
    "for i, chunk in enumerate(chunk_shards):\n",
    "    output_file=f'{config.INTERMEDIATE_DATA_PATH}/chunks.{i}.parquet'\n",
    "    chunk.reset_index(inplace=True, drop=True)\n",
    "    chunk.to_parquet(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Single JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sentences data to dict\n",
    "chunks_dict=chunks_df.to_dict(orient='list')\n",
    "\n",
    "# Save it as JSON\n",
    "with open(f'{config.INTERMEDIATE_DATA_PATH}/all_chunks.json', 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(chunks_dict, output_file, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
