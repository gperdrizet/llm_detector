{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human and synthetic text dataset wrangling\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We have 5 target datasets. The plan it to get them downloaded and saved locally. Read into Python as appropriate and combined into a unified dataset. Here are the target datasets:\n",
    "\n",
    "1. [Hans 2024](https://github.com/ahans30/Binoculars/tree/main), referred to as `hans`. Source: GitHub.\n",
    "2. [AI vs human text](https://www.kaggle.com/datasets/shanegerami/ai-vs-human-text), referred to as `gerami`. Source: Kaggle.\n",
    "3. [Human vs. LLM text corpus](https://www.kaggle.com/datasets/starblasters8/human-vs-llm-text-corpus), referred to as `grinberg`. Source: Kaggle.\n",
    "4. [Human-ChatGPT texts](https://github.com/HarshOza36/Detection-Of-Machine-Generated-Text/tree/master), referred to as `gaggar`. Source: GitHub.\n",
    "5. [ai-text-detection-pile](https://huggingface.co/datasets/artem9k/ai-text-detection-pile), referred to as `yatsenko`. Source: HuggingFace."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
