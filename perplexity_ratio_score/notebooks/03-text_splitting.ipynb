{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text splitting\n",
    "\n",
    "From the text length distributions in the data exploration notebook, it's pretty clear that we need to break the text up. And, we need to do some clean-up, especially in the shorter length regimes. Seems to me the first thing to do is break up the text into shorter fragments. I'd like to try doing this with semantic/tokenization based splitting to get sentences, rather than arbitrary length text fragments that could be broken in the middle of a word or thought.\n",
    "\n",
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/arkk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/siderealyear/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change working directory to parent so we can import as we would\n",
    "# from the perplexity ratio score root directory\n",
    "%cd ..\n",
    "\n",
    "# Standard library imports\n",
    "import glob\n",
    "import time\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "\n",
    "# PyPI imports\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Internal imports\n",
    "import configuration as config\n",
    "\n",
    "# Download NLTK assets\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load an example data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>synthetic</th>\n",
       "      <th>author</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ad them all. Some of the best were the ones th...</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>yatsenko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ER (emergency) vets are more expensive than re...</td>\n",
       "      <td>1</td>\n",
       "      <td>unknown_model</td>\n",
       "      <td>yatsenko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey, Mrs. Johnson! Here's my essay on how a pe...</td>\n",
       "      <td>1</td>\n",
       "      <td>unknown_model</td>\n",
       "      <td>grinberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This article is about the daemonic name, for t...</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>yatsenko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Budget For Server Desktop Upgrade Essay\\n\\nExe...</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>yatsenko</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  synthetic  \\\n",
       "0  ad them all. Some of the best were the ones th...          0   \n",
       "1  ER (emergency) vets are more expensive than re...          1   \n",
       "2  Hey, Mrs. Johnson! Here's my essay on how a pe...          1   \n",
       "3  This article is about the daemonic name, for t...          0   \n",
       "4  Budget For Server Desktop Upgrade Essay\\n\\nExe...          0   \n",
       "\n",
       "          author    source  \n",
       "0          human  yatsenko  \n",
       "1  unknown_model  yatsenko  \n",
       "2  unknown_model  grinberg  \n",
       "3          human  yatsenko  \n",
       "4          human  yatsenko  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file=f'{config.INTERMEDIATE_DATA_PATH}/texts.0.parquet'\n",
    "data_df=pd.read_parquet(data_file)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test split a small batch of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 4999 records in 7.1 seconds\n",
      "Splitting rate: 700.7 records per second\n"
     ]
    }
   ],
   "source": [
    "# Holder for results\n",
    "results={\n",
    "    'text': [],\n",
    "    'synthetic': [],\n",
    "    'author': [],\n",
    "    'source': []\n",
    "}\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "for i in range(5000):\n",
    "    \n",
    "    text=data_df['text'].iloc[i]\n",
    "    sentences=nltk.tokenize.sent_tokenize(text, language='english')\n",
    "\n",
    "    for sentence in sentences:\n",
    "        results['text'].append(sentence)\n",
    "        results['synthetic'].append(data_df['synthetic'].iloc[i])\n",
    "        results['author'].append(data_df['author'].iloc[i])\n",
    "        results['source'].append(data_df['source'].iloc[i])\n",
    "\n",
    "dT=time.time() - start_time\n",
    "splitting_rate=i/dT\n",
    "print(f'Split {i} records in {dT:.1f} seconds')\n",
    "print(f'Splitting rate: {splitting_rate:.1f} records per second')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so ~600 records per second, single threaded, means about an hour and a half to split all 3.47 million records. If we parallelize it over the input files, we should be looking at about 6 minutes, assuming a linear speed-up. I'd like to collect the results back to the main process and then shuffle/split them again, so we end up with more approximately equal numbers of sentences in each file.\n",
    "\n",
    "## 3. Parallel splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the splitting function\n",
    "\n",
    "def split_text(data_file: str=None, worker: int=0) -> dict:\n",
    "    '''Function to parallelize NLTK based sentence splitting of\n",
    "    text over input files. Meant to be called with multiprocessing\n",
    "    worker. Take an input file string, loads the data, splits\n",
    "    sentences, collects results in dictionary and returns dictionary.'''\n",
    "\n",
    "    data_df=pd.read_parquet(data_file)\n",
    "    print(f\"\\nWorker {worker} loaded: {data_file.split('/')[-1]}\", end='')\n",
    "\n",
    "    results={\n",
    "        'text': [],\n",
    "        'synthetic': [],\n",
    "        'author': [],\n",
    "        'source': []\n",
    "    }\n",
    "\n",
    "    for i in range(len(data_df)):\n",
    "        \n",
    "        text=data_df['text'].iloc[i]\n",
    "        sentences=nltk.tokenize.sent_tokenize(text, language='english')\n",
    "\n",
    "        for sentence in sentences:\n",
    "            results['text'].append(sentence)\n",
    "            results['synthetic'].append(data_df['synthetic'].iloc[i])\n",
    "            results['author'].append(data_df['author'].iloc[i])\n",
    "            results['source'].append(data_df['source'].iloc[i])\n",
    "\n",
    "    print(f'\\nWorker {worker} finished, parsed {len(sentences)} sentences', end='')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Worker 13 loaded: texts.13.parquet\n",
      "Worker 10 loaded: texts.10.parquet\n",
      "Worker 5 loaded: texts.5.parquet\n",
      "Worker 15 loaded: texts.15.parquet\n",
      "Worker 9 loaded: texts.9.parquet\n",
      "Worker 6 loaded: texts.6.parquet\n",
      "Worker 1 loaded: texts.1.parquet\n",
      "Worker 0 loaded: texts.0.parquet\n",
      "Worker 7 loaded: texts.7.parquet\n",
      "Worker 8 loaded: texts.8.parquet\n",
      "Worker 11 loaded: texts.11.parquet\n",
      "Worker 4 loaded: texts.4.parquet\n",
      "Worker 12 loaded: texts.12.parquet\n",
      "Worker 14 loaded: texts.14.parquet\n",
      "Worker 3 loaded: texts.3.parquet\n",
      "Worker 2 loaded: texts.2.parquet\n",
      "Worker 12 finished, parsed 17 sentences\n",
      "Worker 5 finished, parsed 1 sentences\n",
      "Worker 10 finished, parsed 23 sentences\n",
      "Worker 3 finished, parsed 7 sentences\n",
      "Worker 0 finished, parsed 5 sentences\n",
      "Worker 15 finished, parsed 13 sentences\n",
      "Worker 2 finished, parsed 1 sentences\n",
      "Worker 4 finished, parsed 5 sentences\n",
      "Worker 13 finished, parsed 25 sentences\n",
      "Worker 9 finished, parsed 29 sentences\n",
      "Worker 7 finished, parsed 40 sentences\n",
      "Worker 6 finished, parsed 1 sentences\n",
      "Worker 14 finished, parsed 9 sentences\n",
      "Worker 8 finished, parsed 131 sentences\n",
      "Worker 11 finished, parsed 13 sentences\n",
      "Worker 1 finished, parsed 3 sentences\n",
      "CPU times: user 55.8 s, sys: 32.2 s, total: 1min 27s\n",
      "Wall time: 9min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Get list of input files\n",
    "input_files=glob.glob(f'{config.INTERMEDIATE_DATA_PATH}/texts.*.parquet')\n",
    "\n",
    "# Instantiate pool with one worker per input file\n",
    "pool=mp.Pool(\n",
    "    processes=len(input_files),\n",
    "    maxtasksperchild=1\n",
    ")\n",
    "\n",
    "# Holder for returns from workers\n",
    "async_results=[]\n",
    "\n",
    "# Loop input files\n",
    "for i, data_file in enumerate(input_files):\n",
    "\n",
    "    async_results.append(pool.apply_async(split_text,args=(data_file,i,)))\n",
    "\n",
    "# Clean up\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "# Get the results\n",
    "results=[async_result.get() for async_result in async_results]\n",
    "\n",
    "# Collect the results\n",
    "sentences={\n",
    "    'text': [],\n",
    "    'synthetic': [],\n",
    "    'author': [],\n",
    "    'source': []\n",
    "}\n",
    "\n",
    "for result in results:\n",
    "    for key, value in result.items():\n",
    "        sentences[key].extend(value)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>synthetic</th>\n",
       "      <th>author</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ad them all.</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>yatsenko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Some of the best were the ones that weren't su...</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>yatsenko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The young ones.</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>yatsenko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>They knew what they wanted but their fear held...</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>yatsenko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They were my favorite.</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>yatsenko</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  synthetic author  \\\n",
       "0                                       ad them all.          0  human   \n",
       "1  Some of the best were the ones that weren't su...          0  human   \n",
       "2                                    The young ones.          0  human   \n",
       "3  They knew what they wanted but their fear held...          0  human   \n",
       "4                             They were my favorite.          0  human   \n",
       "\n",
       "     source  \n",
       "0  yatsenko  \n",
       "1  yatsenko  \n",
       "2  yatsenko  \n",
       "3  yatsenko  \n",
       "4  yatsenko  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df=pd.DataFrame(sentences)\n",
    "sentences_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save results\n",
    "\n",
    "### 4.1. Parquet shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give it a shuffle\n",
    "sentences_df=sentences_df.sample(frac=1)\n",
    "\n",
    "# Split the dataframe into 16 chunks\n",
    "chunks=np.array_split(sentences_df, 16)\n",
    "\n",
    "# Save each chunk as parquet with a clean index\n",
    "for i, chunk in enumerate(chunks):\n",
    "    output_file=f'{config.INTERMEDIATE_DATA_PATH}/sentences.{i}.parquet'\n",
    "    chunk.reset_index(inplace=True, drop=True)\n",
    "    chunk.to_parquet(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Single JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sentences data to dict\n",
    "sentences_dict=sentences_df.to_dict(orient='list')\n",
    "\n",
    "# Save it as JSON\n",
    "with open(f'{config.INTERMEDIATE_DATA_PATH}/all_sentences.json', 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(sentences_dict, output_file, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
