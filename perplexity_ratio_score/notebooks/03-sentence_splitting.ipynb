{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence splitting\n",
    "\n",
    "From the text length distributions in the data exploration notebook, it's pretty clear that we need to break the text up. And, we need to do some clean-up, especially in the shorter length regimes. Seems to me the first thing to do is break up the text into shorter fragments. I'd like to try doing this with semantic/tokenization based splitting to get sentences, rather than arbitrary length text fragments that could be broken in the middle of a word or thought.\n",
    "\n",
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/siderealyear/projects/llm_detector/perplexity_ratio_score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/siderealyear/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/siderealyear/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change working directory to parent so we can import as we would\n",
    "# from the perplexity ratio score root directory\n",
    "%cd ..\n",
    "\n",
    "# Standard library imports\n",
    "import glob\n",
    "import time\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "\n",
    "# PyPI imports\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Internal imports\n",
    "import configuration as config\n",
    "\n",
    "# Download NLTK assets\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load an example data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>synthetic</th>\n",
       "      <th>author</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Throughout the Middle Ages, Newcastle was Engl...</td>\n",
       "      <td>1</td>\n",
       "      <td>unknown_model</td>\n",
       "      <td>grinberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SOUTH FLORIDA F-3E Mustang II #92496 N9E5M was...</td>\n",
       "      <td>1</td>\n",
       "      <td>unknown_model</td>\n",
       "      <td>yatsenko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Romeo: What did it feel like to finally have t...</td>\n",
       "      <td>1</td>\n",
       "      <td>unknown_model</td>\n",
       "      <td>yatsenko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We love our pets and want to keep track of the...</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>yatsenko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>To achieve success.\\n\\nGoal setting is a cruci...</td>\n",
       "      <td>1</td>\n",
       "      <td>unknown_model</td>\n",
       "      <td>gerami</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  synthetic  \\\n",
       "0  Throughout the Middle Ages, Newcastle was Engl...          1   \n",
       "1  SOUTH FLORIDA F-3E Mustang II #92496 N9E5M was...          1   \n",
       "2  Romeo: What did it feel like to finally have t...          1   \n",
       "3  We love our pets and want to keep track of the...          0   \n",
       "4  To achieve success.\\n\\nGoal setting is a cruci...          1   \n",
       "\n",
       "          author    source  \n",
       "0  unknown_model  grinberg  \n",
       "1  unknown_model  yatsenko  \n",
       "2  unknown_model  yatsenko  \n",
       "3          human  yatsenko  \n",
       "4  unknown_model    gerami  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file=f'{config.INTERMEDIATE_DATA_PATH}/texts.0.parquet'\n",
    "data_df=pd.read_parquet(data_file)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test split a small batch of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 5000 records in 5.4 seconds\n",
      "Splitting rate: 917.6 records per second\n"
     ]
    }
   ],
   "source": [
    "# Holder for results\n",
    "results={\n",
    "    'text': [],\n",
    "    'synthetic': [],\n",
    "    'author': [],\n",
    "    'source': []\n",
    "}\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "for i in range(5000):\n",
    "    \n",
    "    text=data_df['text'].iloc[i]\n",
    "    sentences=nltk.tokenize.sent_tokenize(text, language='english')\n",
    "\n",
    "    for sentence in sentences:\n",
    "        results['text'].append(sentence)\n",
    "        results['synthetic'].append(data_df['synthetic'].iloc[i])\n",
    "        results['author'].append(data_df['author'].iloc[i])\n",
    "        results['source'].append(data_df['source'].iloc[i])\n",
    "\n",
    "dT=time.time() - start_time\n",
    "splitting_rate=(i + 1)/dT\n",
    "print(f'Split {i + 1} records in {dT:.1f} seconds')\n",
    "print(f'Splitting rate: {splitting_rate:.1f} records per second')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so ~920 records per second, single threaded, means about an hour to split all 3.47 million records. If we parallelize it over ~30 input files, we should be looking at under 5 minutes, assuming a linear speed-up. I'd like to collect the results back to the main process and then shuffle/split them again, so we end up with more approximately equal numbers of sentences in each file.\n",
    "\n",
    "## 3. Parallel splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the splitting function\n",
    "\n",
    "def split_text(data_file: str=None, worker: int=0) -> dict:\n",
    "    '''Function to parallelize NLTK based sentence splitting of\n",
    "    text over input files. Meant to be called with multiprocessing\n",
    "    worker. Take an input file string, loads the data, splits\n",
    "    sentences, collects results in dictionary and returns dictionary.'''\n",
    "\n",
    "    data_df=pd.read_parquet(data_file)\n",
    "    print(f\"\\nWorker {worker} loaded: {data_file.split('/')[-1]}\", end='')\n",
    "\n",
    "    results={\n",
    "        'text': [],\n",
    "        'synthetic': [],\n",
    "        'author': [],\n",
    "        'source': []\n",
    "    }\n",
    "\n",
    "    for i in range(len(data_df)):\n",
    "        \n",
    "        text=data_df['text'].iloc[i]\n",
    "        sentences=nltk.tokenize.sent_tokenize(text, language='english')\n",
    "\n",
    "        for sentence in sentences:\n",
    "            results['text'].append(sentence)\n",
    "            results['synthetic'].append(data_df['synthetic'].iloc[i])\n",
    "            results['author'].append(data_df['author'].iloc[i])\n",
    "            results['source'].append(data_df['source'].iloc[i])\n",
    "\n",
    "    print(f'\\nWorker {worker} finished, parsed {len(sentences)} sentences', end='')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Worker 16 loaded: texts.0.parquet\n",
      "Worker 28 loaded: texts.28.parquet\n",
      "Worker 3 loaded: texts.9.parquet\n",
      "Worker 4 loaded: texts.13.parquet\n",
      "Worker 7 loaded: texts.14.parquet\n",
      "Worker 19 loaded: texts.2.parquet\n",
      "Worker 20 loaded: texts.11.parquet\n",
      "Worker 8 loaded: texts.30.parquet\n",
      "Worker 0 loaded: texts.23.parquet\n",
      "Worker 18 loaded: texts.17.parquet\n",
      "Worker 6 loaded: texts.29.parquet\n",
      "Worker 29 loaded: texts.15.parquet\n",
      "Worker 1 loaded: texts.27.parquet\n",
      "Worker 10 loaded: texts.1.parquet\n",
      "Worker 17 loaded: texts.26.parquet\n",
      "Worker 15 loaded: texts.4.parquet\n",
      "Worker 27 loaded: texts.8.parquet\n",
      "Worker 30 loaded: texts.25.parquet\n",
      "Worker 14 loaded: texts.16.parquet\n",
      "Worker 2 loaded: texts.6.parquet\n",
      "Worker 11 loaded: texts.20.parquet\n",
      "Worker 12 loaded: texts.18.parquet\n",
      "Worker 13 loaded: texts.7.parquet\n",
      "Worker 23 loaded: texts.12.parquet\n",
      "Worker 24 loaded: texts.5.parquet\n",
      "Worker 21 loaded: texts.21.parquet\n",
      "Worker 22 loaded: texts.22.parquet\n",
      "Worker 5 loaded: texts.24.parquet\n",
      "Worker 26 loaded: texts.19.parquet\n",
      "Worker 25 loaded: texts.3.parquet\n",
      "Worker 9 loaded: texts.10.parquet\n",
      "Worker 16 finished, parsed 17 sentences\n",
      "Worker 3 finished, parsed 14 sentences\n",
      "Worker 4 finished, parsed 31 sentences\n",
      "Worker 27 finished, parsed 40 sentences\n",
      "Worker 18 finished, parsed 1 sentences\n",
      "Worker 19 finished, parsed 30 sentences\n",
      "Worker 13 finished, parsed 21 sentences\n",
      "Worker 17 finished, parsed 2 sentences\n",
      "Worker 14 finished, parsed 23 sentences\n",
      "Worker 24 finished, parsed 1 sentences\n",
      "Worker 7 finished, parsed 89 sentences\n",
      "Worker 9 finished, parsed 7 sentences\n",
      "Worker 28 finished, parsed 4 sentences\n",
      "Worker 10 finished, parsed 23 sentences\n",
      "Worker 26 finished, parsed 342 sentences\n",
      "Worker 8 finished, parsed 1 sentences\n",
      "Worker 25 finished, parsed 3 sentences\n",
      "Worker 29 finished, parsed 13 sentences\n",
      "Worker 6 finished, parsed 54 sentences\n",
      "Worker 20 finished, parsed 1 sentences\n",
      "Worker 11 finished, parsed 9 sentences\n",
      "Worker 21 finished, parsed 27 sentences\n",
      "Worker 30 finished, parsed 21 sentences\n",
      "Worker 15 finished, parsed 22 sentences\n",
      "Worker 12 finished, parsed 71 sentences\n",
      "Worker 2 finished, parsed 14 sentences\n",
      "Worker 23 finished, parsed 2 sentences\n",
      "Worker 0 finished, parsed 49 sentences\n",
      "Worker 22 finished, parsed 3 sentences\n",
      "Worker 1 finished, parsed 23 sentences\n",
      "Worker 5 finished, parsed 153 sentences\n",
      "CPU times: user 58.6 s, sys: 1min 3s, total: 2min 2s\n",
      "Wall time: 6min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Get list of input files\n",
    "input_files=glob.glob(f'{config.INTERMEDIATE_DATA_PATH}/texts.*.parquet')\n",
    "\n",
    "# Instantiate pool with one worker per input file\n",
    "pool=mp.Pool(\n",
    "    processes=len(input_files),\n",
    "    maxtasksperchild=1\n",
    ")\n",
    "\n",
    "# Holder for returns from workers\n",
    "async_results=[]\n",
    "\n",
    "# Loop input files\n",
    "for i, data_file in enumerate(input_files):\n",
    "\n",
    "    async_results.append(pool.apply_async(split_text,args=(data_file,i,)))\n",
    "\n",
    "# Clean up\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "# Get the results\n",
    "results=[async_result.get() for async_result in async_results]\n",
    "\n",
    "# Collect the results\n",
    "sentences={\n",
    "    'text': [],\n",
    "    'synthetic': [],\n",
    "    'author': [],\n",
    "    'source': []\n",
    "}\n",
    "\n",
    "for result in results:\n",
    "    for key, value in result.items():\n",
    "        sentences[key].extend(value)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>synthetic</th>\n",
       "      <th>author</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bruce Banner, after coming out of his Green Ra...</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>yatsenko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A sickly moon hangs in the air, as the walls a...</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>yatsenko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As the adrenaline wears off, Bruce begins to r...</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>yatsenko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>He was trying to eliminate his other half, so ...</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>yatsenko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The forty-seventh attempt.</td>\n",
       "      <td>0</td>\n",
       "      <td>human</td>\n",
       "      <td>yatsenko</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  synthetic author  \\\n",
       "0  Bruce Banner, after coming out of his Green Ra...          0  human   \n",
       "1  A sickly moon hangs in the air, as the walls a...          0  human   \n",
       "2  As the adrenaline wears off, Bruce begins to r...          0  human   \n",
       "3  He was trying to eliminate his other half, so ...          0  human   \n",
       "4                         The forty-seventh attempt.          0  human   \n",
       "\n",
       "     source  \n",
       "0  yatsenko  \n",
       "1  yatsenko  \n",
       "2  yatsenko  \n",
       "3  yatsenko  \n",
       "4  yatsenko  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df=pd.DataFrame(sentences)\n",
    "sentences_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save results\n",
    "\n",
    "### 4.1. Parquet shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give it a shuffle\n",
    "sentences_df=sentences_df.sample(frac=1)\n",
    "\n",
    "# Split the dataframe into 16 chunks\n",
    "chunks=np.array_split(sentences_df, mp.cpu_count() - 2)\n",
    "\n",
    "# Save each chunk as parquet with a clean index\n",
    "for i, chunk in enumerate(chunks):\n",
    "    output_file=f'{config.INTERMEDIATE_DATA_PATH}/sentences.{i}.parquet'\n",
    "    chunk.reset_index(inplace=True, drop=True)\n",
    "    chunk.to_parquet(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Single JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sentences data to dict\n",
    "sentences_dict=sentences_df.to_dict(orient='list')\n",
    "\n",
    "# Save it as JSON\n",
    "with open(f'{config.INTERMEDIATE_DATA_PATH}/all_sentences.json', 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(sentences_dict, output_file, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
